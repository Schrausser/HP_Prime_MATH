<html> 
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="Author" content="Dietmar Gerald Schrausser">

<title>
HP_Prime_Math/manual
</title>

</head> 
<body style="font-family:calibri;font-size:85%;background-color:#eceff1;color:#37474f">

<style>
a {color: #78909c; }
a:visited {color: #78909c;}
table
 {
text-align: center;
border-collapse: collapse;
margin: 1 auto;
height: 12vh;
width: 35vh;
}
th 
{
border-bottom: 1px solid black;
border-top: 1px solid black;
}
</style>

<h1 id="hp_prime_math-manual">HP_Prime_MATH: Manual</h1>
<p>Dietmar G. Schrausser</p>
<br>
<br>
<h2 id="introduction">Introduction</h2>
<br>
<br>
<p>Mathematical and statistical applications for <em>HP Prime</em> (s. HP Inc., <a href="https://www.hpcalc.org/details/7445">2017</a>; Schrausser, <a href="https://doi.org/10.5281/zenodo.14721085">2025</a>). <em>Algorithms</em> are presented in context with the corresponding scope of application (s. Functions). <em>CAS</em> programs (1), HP Prime <em>User</em> functions (2) and functions for HP Prime <em>Applications</em> (3) are listed in alphabetical order (s. Source Codes), for a <em>comparison</em> to corresponding <em>Schrausser-MAT</em> functions (Schrausser, <a href="https://www.academia.edu/81395688">2022a</a>) see Table 1. In addition to the source codes of the functions, <em>raw</em> data sets are provided for <em>correlation</em>- as well as <em>resampling</em>-methods (s. Data).</p>
<p>On <em>mathematical statistical</em> methods in general see e.g. Cox and Hinkley (<a href="https://doi.org/10.1201/b14832">1974</a>), Bortz and Weber (<a href="https://doi.org/10.1007/b137571">2005</a>), Lehmann and Romano (<a href="https://books.google.com/books?id=IlJE_9_e8UEC">2008</a>) or Bortz and Schuster (<a href="https://doi.org/10.1007/978-3-642-12770-0">2010</a>), Schrausser (<a href="https://doi.org/10.5281/zenodo.10969144">2024a</a>) provides a comprehensive overview of the most important <em>distribution functions</em> and corresponding <em>algorithms</em>. Introducing works on <em>resampling</em> methods are given by e.g. Good (<a href="https://www.amazon.com/Resampling-Methods-Practical-Guide-Analysis/dp/0817643869">2006</a>) or Beasley and Rodgers (<a href="https://psycnet.apa.org/doi/10.4135/9780857020994.n16">2009</a>), for <em>calculus</em> and <em>theory of functions</em> see e.g. Meyberg and Vachenauer (<a href="https://doi.org/10.1007/978-3-642-56654-7">2001a</a>, <a href="https://doi.org/10.1007/978-3-642-56655-4">b</a>) or Remmert and Schumacher (<a href="https://doi.org/10.1007/978-3-642-56281-5">2002</a>), on <em>complex numbers</em> in the <em>complex plane</em> see e.g. Burckel (<a href="https://doi.org/10.1007/978-1-0716-1965-0">2021</a>) and Vince (<a href="https://doi.org/10.1007/978-1-4471-7509-4_4">2021</a>).</p>
<p>For the history of <em>statistical inference</em> in general see e.g. Stigler (<a href="https://www.scirp.org/(S(351jmbntvnsjt1aadkposzje))/reference/ReferencesPapers.aspx?ReferenceID=1973131">1986</a>) and Hald (<a href="https://onlinelibrary.wiley.com/doi/book/10.1002/0471725161">1990</a>, <a href="https://www.abebooks.com/History-Mathematical-Statistics-1750-1930-Wiley/31042381048/bd">1998</a>, <a href="https://www.wiley.com/en-us/A%2BHistory%2Bof%2BProbability%2Band%2BStatistics%2Band%2BTheir%2BApplications%2Bbefore%2B1750-p-9780471725176">2003</a>, <a href="https://link.springer.com/book/10.1007/978-0-387-46409-1#bibliographic-information">2007</a>), <em>historical foundations</em> of mathematics are thematized and discussed in e.g. Suter (<a href="https://doi.org/10.3931/e-rara-65095">1887</a>), Heath (<a href="https://archive.org/details/cu31924008704219">1921a</a>, <a href="[https://archive.org/details/historyofgreekma029268mbp/page/n5/mode/1up">b</a>), Boyer (<a href="https://archive.org/details/ahistoryofmathematicscarlbboyer1968_315_t">1968</a>), Neugebauer (<a href="https://books.google.com/books?id=JVhTtVA2zr8C">1969</a>), Ewald (<a href="https://philpapers.org/rec/BRAFKT">1996a</a>, <a href="https://philpapers.org/rec/EWAFKT-4">b</a>), Katz (<a href="https://www.gettextbooks.com/isbn/9780321387004/">2009</a>) or Merzbach and Boyer (<a href="https://books.google.com/books/about/A_History_of_Mathematics.html?id=bR9HAAAAQBAJ">2011</a>).</p>
<br>
<br>
<h2 id="functions">Functions</h2>
<br>
<br>
<h3 id="correlation">Correlation</h3>

<p>To measure the degree of a linear <em>relation</em> between variables,
Karl Pearson (<a href="https://openlibrary.org/books/OL24168960M">1904</a>) was developing statistical procedures for biometry including the <em>correlation</em> and <em>regression</em> coefficients based on the works of Bravais (<a href="https://books.google.com/books?id=7g_hAQAACAAJ">1844</a>) and
Galton (<a href="https://doi.org/10.1038/015492a0">1877</a>) who introduced the symbol <math display="inline">
<mi>r</mi>
</math>, on the then designation of the term <em>reversion</em>.</p>

<table>
<caption style="text-align:left; font-size:80%">
<strong>Table 1. </strong>Appropriate <em>correlation</em> coefficients; <em>product-moment</em> or Pearson <em>correlation</em>  <math display="inline">
<msub><mi>r</mi><mrow data-mjx-texclass="ORD"><mi>x</mi><mi>y</mi></mrow></msub>
</math>, Spearman's <em>rank correlation</em> coefficient <em>rho</em> <math display="inline">
<mi>&rho;</mi>
</math>, biserial (or <em>biseral</em>) coefficients <math display="inline">
<msub><mi>r</mi><mrow data-mjx-texclass="ORD"><mi>b</mi><mi>i</mi><mi>s</mi></mrow></msub>
</math>,  <math display="inline">
<msub><mi>r</mi><mrow data-mjx-texclass="ORD"><mi>p</mi><mi>b</mi></mrow></msub>
</math>, <math display="inline">
<msub><mi>r</mi><mrow data-mjx-texclass="ORD"><mi>b</mi><mi>i</mi><mi>s</mi><mi>R</mi></mrow></msub>
</math> and <em>phi</em> coefficient <math display="inline">
<mi>&Phi;</mi>
</math> at the corresponding <em>scale</em> levels, <em>interval i</em>, <em>ordinal o</em>, and <em>nominal n</em>.</caption>
<tr>
<th style="width:25%; font-size: 80%"></th>
<th style="width:25%; font-size: 80%"><em>i</em></th>
<th style="width:25%; font-size: 80%"><em>o</em></th>
<th style="width:25%; font-size: 80%"><em>n</em></th>
</tr>
<td style="font-size: 80%"><strong><em>i</em></strong></td>
<td style="font-size: 80%"><math display="inline">
<msub><mi>r</mi><mrow data-mjx-texclass="ORD"><mi>x</mi><mi>y</mi></mrow></msub>
</math></td>
<td></td>
<td></td>
 </tr>
 <tr>
 <td style="font-size: 80%"><strong><em>o</em></strong></td>
<td></td>
<td style="font-size: 80%"><math display="inline">
<mi>&rho;</mi>
</math>¹</td>
<td></td>
</tr>
<tr>
<td style="font-size: 80%"><strong><em>n</em></strong></td>
<td style="font-size: 80%"><math display="inline">
<msub><mi>r</mi><mrow data-mjx-texclass="ORD"><mi>b</mi><mi>i</mi><mi>s</mi></mrow></msub>
</math>,  <math display="inline">
<msub><mi>r</mi><mrow data-mjx-texclass="ORD"><mi>p</mi><mi>b</mi></mrow></msub>
</math></td>
<td style="font-size: 80%"><math display="inline">
<msub><mi>r</mi><mrow data-mjx-texclass="ORD"><mi>b</mi><mi>i</mi><mi>s</mi><mi>R</mi></mrow></msub>
</math></td>
<td style="font-size: 80%"><math display="inline">
<mi>&Phi;</mi></math>²</td>
</tr>
<tfoot style="text-align:left; font-size:70%; border-top: 1px solid black;">
<tr>
<td colspan="4"><p>¹) also Kendall's tau <math display="inline"><mi>&tau;</mi></math> (<a href="http://www.jstor.org/stable/2332226">1938</a>) or Somers' <math display="inline"><mi>D</mi></math> (<a href="http://www.jstor.org/stable/2090408">1962</a>).<br />
²) also <em>tetrachoric correlation</em> <math display="inline"><msub><mi>r</mi><mrow data-mjx-texclass="ORD"><mi>t</mi><mi>e</mi><mi>t</mi></mrow></msub></math> (Pearson, <a href="https://royalsocietypublishing.org/doi/abs/10.1098/rsta.1900.0022">1900</a>).</p></td>
</tr>
</tfoot>
</table>

<p>The methodological apparatus of <em>factor analysis</em> as a further and broader concept, based on <em>multiple regression</em> and <em>matrix</em> calculation was <em>first discussed</em> by
Charles Edward Spearman (<a href="http://www.jstor.org/stable/1412159">1904</a>), later the <em>initial</em> developed took place by Louis Leon Thurstone (<a href="https://doi.org/10.1037/h0069792">1931</a>, <a href="https://doi.org/10.1037/h0075959">1934</a>, <a href="https://archive.org/details/vectorsofmindmul010122mbp/page/n7/mode/1up">1935</a>; s. also Cattell, <a href="https://doi.org/10.1207/s15327906mbr0102_10">1966</a>).</p>

<br>
<h4 id="koric_m-rxyredtrtrwprwprwx-e01-pearson-product-moment-correlation-coefficient-r_xy">[<code>KOR</code>|<code>IC_M</code>] [<code>rxy</code>|<code>RED</code>|<code>tr</code>|<code>TRW</code>|<code>pRW</code>|<code>pRWx</code>] [<code>E01</code>] Pearson product-moment correlation coefficient <math display="inline">
<msub><mi>r</mi><mrow data-mjx-texclass="ORD"><mi>x</mi><mi>y</mi></mrow></msub>
</math></h4>
<hr />
<p>See Pearson (<a href="https://openlibrary.org/books/OL24168960M">1904</a>, <a href="https://openlibrary.org/books/OL6555066M">1905</a>).</p>
<p>$$r_{xy}=\frac{\sigma_{xy}^2}{\sigma_x\cdot \sigma_y},$$</p>
<p>$$\sigma^2_{xy}=\frac{\sum_{i=1}^n{(x_i-\overline{x})\cdot(y_i-\overline{y})}}{n}$$</p>
<p>with</p>
<p>$$t_{(df)}=\frac{r\cdot\sqrt{n-2}}{\sqrt{1-r^2}}$$</p>
<p>where</p>
<p><math display="inline"><msup><mi>r</mi><mn>2</mn></msup>
</math> = coefficient of determination, redundancy <math display="inline"><mi>d</mi><mi>e</mi><mi>t</mi><mi mathvariant="normal">%</mi><mo>=</mo><msup><mi>r</mi><mn>2</mn></msup><mo>⋅</mo><mn>100</mn>
</math><br />
<math display="inline"><msubsup><mi>&sigma;</mi><mrow data-mjx-texclass="ORD"><mi>x</mi><mi>y</mi></mrow><mn>2</mn></msubsup>
</math> = covariance of <math display=„inline“> <mi>x</mi> </math> and <math display=„inline“> <mi>y</mi> </math><br />
<math display=„inline“>
<mi>d</mi><mi>f</mi>
</math> = <math display="inline">
<mi>n</mi><mo>&minus; 2</mo>
</math></p>
<br>
<h4 id="rho-spearmans-rho">[<code>RHO</code>] Spearman's <math display="inline">
<mi>&rho;</mi>
</math></h4>
<hr />
<p>Equivalent to the <em>product moment correlation</em> when <em>rank</em> values are present (s. Spearman, <a href="http://www.jstor.org/stable/1412159">1904</a>).</p>
<p>$$r_s=\rho=1-\frac{6\cdot\sum_{i=1}^n{d_i^2}}{n\cdot(n^2-2)}$$</p>
<p>with</p>
<p>$$t_{(df)}=\frac{\rho\cdot\sqrt{n-2}}{\sqrt{1-\rho^2}};n\ge30$$</p>

<p>where</p>
<p><math display="inline"><msub><mi>d</mi><mi>i</mi></msub></math> = rank difference of <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> and <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math><br />
<math display=„inline“>
<mi>d</mi><mi>f</mi>
</math> = <math display="inline">
<mi>n</mi><mo>&minus; 2</mo>
</math></p>
<br>

<h4 id="tau-kendalls-tau-tau_a">[<code>TAU</code>] Kendall's tau <math display="inline">
<msub><mi>&tau;</mi><mi>a</mi></msub>
</math></h4>
<hr />
<p>Without adjustment for <em>ties</em> (s. Kendall, <a href="http://www.jstor.org/stable/2332226">1938</a>).</p>
<p>$$\tau_a=1-\frac{2\cdot n_d}{0.5\cdot n\cdot(n-1)},$$</p>

<p>with</p>
<p>$$z=3\cdot\tau_a\cdot\frac{\sqrt{n\cdot(n-1)}}{\sqrt{2\cdot(2\cdot n+5)}};n\gt 10$$</p>
<p>alternatively</p>
<p>$$z=\frac{n_c-n_d}{\sqrt{\frac{1}{18}\cdot n\cdot(n-1)\cdot(2\cdot n+5)}}$$</p>
<p>where</p>
<p><math display=„inline“> <mi>n</mi> </math> = total number of pairs<br />
<math display="inline"><msub><mi>n</mi><mi>d</mi></msub></math> = number of discordant pairs<br />
<math display="inline"><msub><mi>n</mi><mi>c</mi></msub></math> = number of concordant pairs, with <math display="inline"><msub><mi>n</mi><mi>c</mi></msub><mo>=</mo><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="OPEN"><mo minsize="2.047em" maxsize="2.047em">(</mo></mrow><mfrac linethickness="0"><mi>n</mi><mn>2</mn></mfrac><mrow data-mjx-texclass="CLOSE"><mo minsize="2.047em" maxsize="2.047em">)</mo></mrow></mrow><mo>-</mo><msub><mi>n</mi><mi>d</mi></msub>
</math></p>
<br>

<h4 id="delta2-somers-d">[<code>DELTA2</code>] Somers' <math display="inline">
<mi>D</mi>
</math></h4>
<hr />
<p>For <em>binary</em> data [0,1] (s. Somers, <a href="http://www.jstor.org/stable/2090408">1962</a>).</p>
<p>$$D_{YX}=\frac{n_{1,1}}{n}-\frac{n_{1,0}}{n}$$</p>
<p>where</p>
<p><math display=„inline“> <mi>n</mi> </math> = total number of pairs<br />
<math display="inline"><msub><mi>n</mi><mi>1,1</mi></msub></math> = number of pairs with <math display="inline"><mi>Y</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>X</mi><mo>=</mo><mn>1</mn></math>
<br />
<math display="inline"><msub><mi>n</mi><mi>1,0</mi></msub></math> = number of pairs with <math display="inline"><mi>Y</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>X</mi><mo>=</mo><mn>0</mn>
</math>
</p>
<br>
<h4 id="rpbis-point-biserial-correlation-coefficient-r_pb">[<code>rpbis</code>] Point biserial correlation coefficient <math display="inline">
<msub><mi>r</mi><mrow data-mjx-texclass="ORD"><mi>p</mi><mi>b</mi></mrow></msub>
</math></h4>
<hr />
<p>Also point <em>biseral</em>.</p>
<p>$$r_{pb}=\frac{\overline{x}_1-\overline{x}_0}{\sigma_x}\cdot\sqrt{\frac{n_1\cdot n_2}{n^2}}$$</p>
<p>with</p>
<p>$$t_{(df)}=\frac{r_{pb}\cdot\sqrt{n-2}}{\sqrt{1-r_{pb}^2}}$$</p>
<p>where</p>
<p><math display=„inline“>
<mi>d</mi><mi>f</mi>
</math> = <math display="inline">
<mi>n</mi><mo>&minus; 2</mo>
</math></p>
<br>
<h4 id="rbissrbiszrbisprbis-biserial-correlation-coefficient-r_bis">[<code>rbis</code>|<code>srbis</code>|<code>zrbis</code>|<code>prbis</code>] Biserial correlation coefficient <math display="inline">
<msub><mi>r</mi><mrow data-mjx-texclass="ORD"><mi>b</mi><mi>i</mi><mi>s</mi></mrow></msub>
</math></h4>
<hr />
<p>Pearson (<a href="http://www.jstor.org/stable/2345365">1909</a>), see e.g. Tate (<a href="http://www.jstor.org/stable/2333437">1955</a>),
also called <em>biseral</em>.</p>
<p>$$r_{bis}=\frac{\overline{x}_1-\overline{x}_0}{\sigma_x}\cdot\frac{n_1\cdot n_2}{\vartheta\cdot n^2}$$</p>
<p>with</p>
<p>$$z=\frac{r_{bis}}{\sigma_{r_{bis}}},$$</p>
<p>$$\sigma_{r_{bis}}=\frac{\sqrt{n_1\cdot n_2}}{\vartheta\cdot n\cdot\sqrt{n}}$$</p>
<p>where</p>
<p><math display="inline"><mi>&vartheta;</mi><mo>=</mo><mfrac><mn>1</mn><msqrt><mn>2</mn><mo>&middot;</mo><mo>&pi;</mo></msqrt></mfrac><mo>&middot;</mo><msup><mo>e</mo><mrow data-mjx-texclass="ORD"><mo>-</mo><mfrac><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>p</mi><mo>=</mo><mfrac><msub><mi>n</mi><mn>0</mn></msub><mi>n</mi></mfrac><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><mn>2</mn></mfrac></mrow></msup>
</math></p>
<br>
<h4 id="rbisru_1u_2zrbisrprbisr-rank-biserial-correlation-coefficient-r_bisr">[<code>rbisR</code>|<code>U_1</code>|<code>U_2</code>|<code>zrbisR</code>|<code>prbisR</code>] Rank biserial correlation coefficient <math display="inline">
<msub><mi>r</mi><mrow data-mjx-texclass="ORD"><mi>b</mi><mi>i</mi><mi>s</mi><mi>R</mi></mrow></msub>
</math></h4>
<hr />
<p>Also rank <em>biseral</em> correlation, corresponds to the <em>effect size</em> for the <em>Mann–Whitney</em> <math display="inline">
<mi>U</mi>
</math> <em>test</em> (Mann & Whitney, <a href="https://doi.org/10.1214/aoms/1177730491">1947</a>).</p>
<p>$$r_{bisR}=\frac{2}{n}\cdot(\overline{i}_1-\overline{i}_2)$$</p>
<p>with</p>
<p>$$z=\frac{U-n_1\cdot\frac{n_2}{2}}{\sqrt{\frac{n_1\cdot n_2\cdot(n+1)}{12}}},$$</p>
<p>where</p>
<p><math display="inline"><mi>U</mi><mo>=</mo><msub><mi>n</mi><mn>1</mn></msub><mo>&middot;</mo><msub><mi>n</mi><mn>2</mn></msub><mo>+</mo><mfrac><mrow><msubsup><mi>n</mi><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><msub><mi>n</mi><mn>1</mn></msub></mrow><mn>2</mn></mfrac><mrow data-mjx-texclass="ORD"><mo>-</mo></mrow><munderover><mo data-mjx-texclass="OP">&Sigma;</mo><mrow data-mjx-texclass="ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow data-mjx-texclass="ORD"><msub><mi>n</mi><mn>1</mn></msub></mrow></munderover><msub><mi>x</mi><mi>i</mi></msub>
</math></p>
<br>
<h4 id="phc-phixphipphi-phi-coefficient-phi">[<code>PHC</code>] [<code>PHI</code>|<code>xPHI</code>|<code>pPHI</code>] Phi coefficient <math display="inline">
<mi>&Phi;</mi>
</math></h4>
<hr />
<p>Yule (<a href="http://www.jstor.org/stable/2340126">1912</a>).</p>
<p>$$\Phi=\frac{a\cdot d-b\cdot c}{\sqrt{(a+c)\cdot(b+d)\cdot(a+b)\cdot(c+d)}}$$</p>
<p>with</p>
<p>$$\chi^2_{(df)}=n\cdot\Phi^2$$</p>
<p>where</p>
<p><math display="inline">
<mi>d</mi><mi>f</mi>
</math>=1</p>
<br>
<h4 id="phc-rtetsrtetprtet-tetrachoric-correlation-r_tet">[<code>PHC</code>] [<code>rtet</code>|<code>srtet</code>|<code>prtet</code>] Tetrachoric correlation <math display="inline">
<msub><mi>r</mi><mrow data-mjx-texclass="ORD"><mi>t</mi><mi>e</mi><mi>t</mi></mrow></msub>
</math></h4>
<hr />
<p>Pearson (<a href="https://royalsocietypublishing.org/doi/abs/10.1098/rsta.1900.0022">1900a</a>), Everitt (<a href="https://doi.org/10.1093/biomet/7.4.437">1910</a>, <a href="http://www.jstor.org/stable/2331587">1912</a>), s. e.g. Brown (<a href="http://www.jstor.org/stable/2346985">1977</a>), Digby (<a href="http://www.jstor.org/stable/2531104">1983</a>), also Bonett and Price (<a href="http://www.jstor.org/stable/3701350">2005</a>) or Long et al. (<a href="https://doi.org/10.1177/0013164408324463">2009</a>), <em>proposed</em> approximate algorithm.</p>
<p>$$r_{tet}=\cos\frac{\pi}{1+\sqrt{\frac{b\cdot c}{a\cdot d}}}$$</p>
<p>with</p>
<p>$$z=\frac{r_{tet}}{\sigma_{r_{tet}}},$$</p>
<p>$$\sigma_{r_{tet}}=\sqrt{\frac{\frac{a+b}{n}\cdot\frac{a+c}{n}\cdot\frac{c+d}{n}\cdot\frac{b+d}{n}}{n}}\cdot\frac{1}{\vartheta_x\cdot \vartheta_y}$$</p>
<p>where</p>
<p><math display="inline"><msub><mi>&vartheta;</mi><mi>x</mi></msub><mo>=</mo><mfrac><mn>1</mn><msqrt><mn>2</mn><mo>&middot;</mo><mo>&pi;</mo></msqrt></mfrac><mo>&middot;</mo><msup><mo>e</mo><mrow data-mjx-texclass="ORD"><mo>-</mo><mfrac><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>p</mi><mo>=</mo><mfrac><mrow><mi>c</mi><mo>+</mo><mi>d</mi></mrow><mi>n</mi></mfrac><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><mn>2</mn></mfrac></mrow></msup>
</math><br />
<math display="inline"><msub><mi>&vartheta;</mi><mi>y</mi></msub><mo>=</mo><mfrac><mn>1</mn><msqrt><mn>2</mn><mo>&middot;</mo><mo>&pi;</mo></msqrt></mfrac><mo>&middot;</mo><msup><mo>e</mo><mrow data-mjx-texclass="ORD"><mo>-</mo><mfrac><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>p</mi><mo>=</mo><mfrac><mrow><mi>b</mi><mo>+</mo><mi>d</mi></mrow><mi>n</mi></mfrac><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><mn>2</mn></mfrac></mrow></msup>
</math></p>
<br>
<h4 id="pkr-rxy_zzrxy_zprxy_zry_xz--partial-correlation-r_xycdot-z">[<code>PKR</code>] [<code>rxy_z</code>|<code>zrxy_z</code>|<code>prxy_z</code>|<code>ry_xz</code>]  Partial correlation <math display="inline">
<msub><mi>r</mi><mrow data-mjx-texclass="ORD"><mi>x</mi><mi>y</mi><mo>&middot;</mo><mi>z</mi></mrow></msub>
</math></h4>
<hr />
<p>$$r_{xy \cdot z}=\frac{r_{xy}-r_{xz}\cdot r_{yz}}{\sqrt{1-r_{xz}^2}\cdot \sqrt{1-r_{yz}^2}}$$</p>
<p>with</p>
<p>$$z=Z_{r_{xy\cdot z}}\cdot\sqrt{n-2}$$</p>
<p>and <em>semi</em> partial correlation</p>
<p>$$r_{y(x \cdot z)}=\frac{r_{xy}-r_{xz}\cdot r_{yz}}{\sqrt{1-r_{xz}^2}}$$.</p>
<br>
<h4 id="zcor-zrrz-fisher-z-transformation">[<code>ZCor</code>] [<code>Zr</code>|<code>rZ</code>] Fisher <math display="inline">
<mi>Z</mi>
</math>-transformation</h4>
<hr />
<p>Fisher (<a href="https://doi.org/10.2307/2331838">1915</a>).</p>
<p>$$Z=\frac{1}{2}\cdot \ln\frac{1+r}{1-r}$$</p>
<p>with</p>
<p>$$z=\frac{Z}{\sqrt{\frac{1}{n-3}}}$$</p>
<p>and</p>
<p>$$r_Z=\frac{e^{2\cdot Z}-1}{e^{2\cdot Z}+1}$$</p>
<br>
<h4 id="zrrprr-fisher-z-difference-cohens-q">[<code>Zrr</code>|<code>prr</code>] Fisher <math display="inline">
<mi>Z</mi>
</math> difference, Cohen's <math display=„inline“> <mi>q</mi> </math></h4>
<hr />
<p>Cohen (<a href="https://www.scirp.org/(S(lz5mqp453edsnp55rrgjct55))/reference/ReferencesPapers.aspx?ReferenceID=2041144">1988</a>, p. 110).</p>
<p>$$\theta=d_Z=Z_{r_1}-Z_{r_2}$$</p>
<p>with</p>
<p>$$z=\frac{d_Z}{\sqrt{\frac{1}{n_1-3}+\frac{1}{n_2-3}}}$$</p>
<br>
<h4 id="mzmr-averaged-fisher-z">[<code>mZ</code>|<code>mr</code>] Averaged Fisher <math display="inline">
<mi>Z</mi>
</math></h4>
<hr />
<p>$$\overline Z=\frac{\sum_{i=1}^k{(n_i-3)}\cdot Z_i}{\sum_{i=1}^k{(n_i-3)}}$$</p>
<br>
<h4 id="mcorr2-mcorrscrcf2fmcorrpmcorr-coefficient-of-multiple-correlation-r_c12-cohens-f2">[<code>MCORR2</code>] [<code>MCORR</code>|<code>SCR</code>|<code>Cf2</code>|<code>FMCORR</code>|<code>pMCORR</code>] Coefficient of multiple correlation <math display="inline"><msub><mi>R</mi><mrow data-mjx-texclass="ORD"><mi>c</mi><mo>,</mo><mn>12</mn></mrow></msub>
</math>, Cohen's <math display="inline"><msup><mi>f</mi><mn>2</mn></msup>
</math></h4>
<hr />
<p>For <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><msubsup><mrow data-mjx-texclass="ORD"><mover><mi>R</mi><mo stretchy="false">^</mo></mover></mrow><mrow data-mjx-texclass="ORD"><mi>c</mi><mo>,</mo><mn>12</mn></mrow><mn>2</mn></msubsup></math> see Olkin and Pratt (<a href="https://doi.org/10.1214/aoms/1177706717">1958</a>).</p>
<p>$$R_{c,12}=\sqrt{\frac{r_{1c}^2+r_{2c}^2-2\cdot r_{12}\cdot r_{1c}\cdot r_{2c}}{1-r_{12}^2}},$$</p>
<p>$$\hat R_{c,12}^2=1-\frac{n-3}{n-k-2}\cdot \biggl[(1-R_{c,12}^2)+\frac{2}{n-k}\cdot(1-R_{c,12}^2)^2\biggr];k=2$$</p>
<p>with</p>
<p>$$f^2=\frac{R_{c,12}^2}{1-R_{c,12}^2},$$</p>
<p>$$F_{(3,df_2)}=\frac{R_{c,12}^2\cdot(n-4)}{(1-R_{c,12}^2)\cdot 3}$$</p>
<p>where</p>

<p><math display="inline"><msup><mi>f</mi><mn>2</mn></msup>
</math> = effect size for multiple regression (Cohen, <a href="https://www.scirp.org/(S(lz5mqp453edsnp55rrgjct55))/reference/ReferencesPapers.aspx?ReferenceID=2041144">1988</a>, p. 410)<br />
<math display="inline"><mi>d</mi><msub><mi>f</mi><mn>2</mn></msub><mo>=</mo><mi>n</mi><mo>−</mo><mn>4</mn>
</math></p>
<br>
<br>
<h3 id="exposure-functions">Exposure functions</h3>
<p>To calculate the appropriate <em>time-aperture-speed</em> combination for given <em>light values</em> on a logarithmic scale, see e.g. Allbright (<a href="https://doi.org/10.1080/00223638.1991.11737126">1991</a>), Marsden and Weinstein (<a href="https://doi.org/10.1007/978-1-4612-5024-1_9">1985</a>), Howie (<a href="https://doi.org/10.1007/978-1-4471-0341-7_6">2001</a>) and Sobot (<a href="https://doi.org/10.1007/978-3-030-79545-0_4">2021</a>).</p>
<br>
<h4 id="evtevaev-e02e03-exposure-value-ev">[<code>Ev</code>|<code>TEv</code>|<code>AEv</code>] [<code>E02</code>|<code>E03</code>] Exposure value <math display=„inline“>
<mi>E</mi><mi>v</mi>
</math></h4>
<hr />
<p>$$Ev=\log2\frac{{Av}^2}{Tv^{-1}}=\frac{\log(Tv\cdot Av^2)}{\log(2)}$$</p>
<p>hence</p>
<p>$$Tv=\frac{2^{Ev}}{Av^2},$$</p>
<p>$$Av=\frac{\sqrt{2^{Ev}\cdot Tv}}{Tv}$$</p>
<p>with</p>
<p><math display=„inline“>
<mi>T</mi><mi>v</mi>
</math> = time value with <math display="inline"><mi>T</mi><mi>v</mi><mo>=</mo><msup><mi>s</mi><mrow data-mjx-texclass="ORD"><mo>-</mo><mn>1</mn></mrow></msup>
</math><br />
<math display=„inline“>
<mi>A</mi><mi>v</mi>
</math> = aperture value <math display=„inline“> <mi>f</mi> </math></p>
<br>
<h4 id="avtv-aperture-av-for-time-tv-with-given-ev">[<code>AvTv</code>] Aperture <math display=„inline“>
<mi>A</mi><mi>v</mi>
</math> for time <math display=„inline“>
<mi>T</mi><mi>v</mi>
</math> with given <math display=„inline“>
<mi>E</mi><mi>v</mi>
</math></h4>
<hr />
<p>$$Av_{Tv}=Av_{Tv_0}\cdot a_{Tv}$$</p>
<p>with</p>
<p>$$a_{Tv}=2^{\frac{1}{2}\cdot\log2\frac{Tv_0}{Tv}}$$</p>
<p>$$=e^{\frac{1}{2}\cdot\log\frac{Tv_0}{Tv}}$$</p>
<p>where</p>
<p><math display=„inline“>
<mi>T</mi><mi>v</mi>
</math> = time value with <math display="inline"><mi>T</mi><mi>v</mi><mo>=</mo><msup><mi>s</mi><mrow data-mjx-texclass="ORD"><mo>-</mo><mn>1</mn></mrow></msup>
</math><br />
<math display="inline"><mi>T</mi><msub><mi>v</mi><mn>0</mn></msub>
</math> = initial time value with <math display="inline"><mi>T</mi><msub><mi>v</mi><mn>0</mn></msub><mo>=</mo><msup><mi>s</mi><mrow data-mjx-texclass="ORD"><mo>-</mo><mn>1</mn></mrow></msup>
</math><br />
<math display=„inline“>
<mi>A</mi><mi>v</mi>
</math> = aperture value <math display=„inline“> <mi>f</mi> </math></p>
<br>
<h4 id="avs-e03-aperture-av-for-speed-s-with-given-ev">[<code>AvS</code>] [<code>E03</code>] Aperture <math display=„inline“>
<mi>A</mi><mi>v</mi>
</math> for speed <math display=„inline“> <mi>S</mi> </math> with given <math display=„inline“>
<mi>E</mi><mi>v</mi>
</math></h4>
<hr />
<p>$$Av_S=Av_{S_0}\cdot a_{S}$$</p>
<p>with</p>
<p>$$a_S=2^{\frac{1}{2}\cdot\log2\frac{S}{S_0}}$$</p>
<p>$$=e^{\frac{1}{2}\cdot\log\frac{S}{S_0}}$$</p>
<p>where</p>
<p><math display=„inline“> <mi>S</mi> </math> = arithmetic speed <math display="inline">
<mi>I</mi><mi>S</mi><mi>O</mi>
</math><br />
<math display="inline"><msub><mi>S</mi><mo>0</mo></msub></math> = initial arithmetic speed <math display="inline">
<mi>I</mi><mi>S</mi><mi>O</mi>
</math><br />
<math display=„inline“>
<mi>A</mi><mi>v</mi>
</math> = aperture value <math display=„inline“> <mi>f</mi> </math></p>
<h4 id="avtvk-aperture-av-shift-from-time-tv-in-steps-k">[<code>AvTvk</code>] Aperture <math display=„inline“>
<mi>A</mi><mi>v</mi>
</math> shift from time <math display=„inline“>
<mi>T</mi><mi>v</mi>
</math> in steps <math display=„inline“> <mi>k</mi> </math></h4>
<hr />
<p>$$Tv_{n-k}=Tv_n\cdot 2^k,Tv_{n+k}=\frac{Tv_n}{2^k}$$</p>
<p>with</p>
<p>$$Av=Av_0\cdot\sqrt2^k$$</p>
<p>where</p>
<p><math display="inline"><mi>A</mi><msub><mi>v</mi><mn>0</mn></msub>
</math> = initial aperture value</p>
<br>
<h4 id="avsk-aperture-av-shift-from-speed-s-in-steps-k">[<code>AvSk</code>] Aperture <math display=„inline“>
<mi>A</mi><mi>v</mi>
</math> shift from speed <math display=„inline“> <mi>S</mi> </math> in steps <math display=„inline“> <mi>k</mi> </math></h4>
<hr />
<p>$$S_{n+k}=S_n\cdot \sqrt{2}^k,S_{n-k}=\frac{S_n}{\sqrt{2}^k}$$</p>
<p>with</p>
<p>$$Av=Av_0\cdot\sqrt[]2^k$$</p>
<p>where</p>
<p><math display="inline"><mi>A</mi><msub><mi>v</mi><mn>0</mn></msub>
</math> = initial aperture value</p>
<br>
<h4 id="isolisoa-speed-s-in--logarithmic--iso-or-arithmetic-iso-conversion">[<code>ISOL</code>|<code>ISOA</code>] Speed <math display=„inline“> <mi>S</mi> </math> in  logarithmic  <math display="inline">
<mi>I</mi><mi>S</mi><mi>O</mi><mi>°</mi>
</math> or arithmetic <math display="inline">
<mi>I</mi><mi>S</mi><mi>O</mi>
</math> conversion</h4>
<hr />
<p>$$S°=10\cdot\log10(S)+1=\frac{10\cdot\log{(S)}}{\log{(10)}}+1,$$</p>
<p>$$S=10^{\frac{S°-1}{10}}$$</p>
<br>
<br>
<h3 id="functions-of-integration-pi-and-gamma">Functions of integration, <math display="inline">
<mo>&pi;</mo>
</math> and <math display=„inline“> <mi>&Gamma;</mi> </math></h3>
<p>Gottfried Wilhelm Leibniz (<a href="https://gdz.sub.uni-goettingen.de/id/PPN788262599">1684</a>, <a href="https://gdz.sub.uni-goettingen.de/id/PPN788262947">1686</a>, <a href="https://archive.org/details/s1id13206590">1693</a>) along with Sir Isaac Newton (<a href="https://books.google.com/books?id=XJwx0lnKvOgC">1687</a>, <a href="https://digital.onb.ac.com/OnbViewer/viewer.faces?doc=ABO_%2BZ180810706&amp;order=7&amp;view=SINGLE">1713</a>, <a href="https://gdz.sub.uni-goettingen.de/id/PPN512261393">1726</a>) are considered the discoverers of <em>differential</em> and <em>integral calculus</em>. According to current consensus, both developed the methods independently of each other, see the so-called <em>Leibniz-Newton calculus controversy</em> (c.f. Cajori, <a href="http://www.jstor.org/stable/2974042">1919</a>; Cassirer, <a href="http://www.jstor.org/stable/2180670">1943</a>; Rosenthal, <a href="http://www.jstor.org/stable/2308368">1951</a>; Schrader, <a href="http://www.jstor.org/stable/27956626">1962</a>; Kossovsky, <a href="https://doi.org/10.1007/978-3-030-51744-1_33">2020</a>).</p>

<p>Newton <em>began</em> working on a <em>geometric</em> form of <em>calculus</em> (the method of <em>fluxions and fluents</em>) in 1666, published in <a href="https://books.google.com/books?id=XJwx0lnKvOgC">1687</a> (c.f. Roero,  <a href="https://doi.org/https://doi.org/10.1016/B978-044450871-3/50085-1">2005</a>), yet, it was Leibniz who <em>introduced</em> the symbols <math display="inline">
<mi>&int;</mi>
</math> and <math display="inline">
<mi>&part;</mi>
</math>.</p>
<p>Here, the functions are primarily intended to display and calculate <math display="inline">
<mo>&pi;</mo>
</math> and <math display="inline">
<mi>&Gamma;</mi>
</math> within the coordinate system.</p>

<br>
<h4 id="f01f05-circular-function-pi">[<code>F01</code>|<code>F05</code>] Circular function, <math display="inline">
<mo>&pi;</mo>
</math></h4>
<hr />
<p>Weierstraß (<a href="https://quod.lib.umich.edu/u/umhistmath/AAN8481.0001.001">1894</a>, p. 53) describes <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mfrac><mi>π</mi><mn>2</mn></mfrac><mo>=</mo><msubsup><mo data-mjx-texclass="OP">∫</mo><mn>0</mn><mrow data-mjx-texclass="ORD"><mi mathvariant="normal">∞</mi></mrow></msubsup><mfrac><mn>1</mn><mrow><mn>1</mn><mo>−</mo><msup><mi>x</mi><mn>2</mn></msup></mrow></mfrac><mi>d</mi><mi>x</mi></math>, which <em>may</em> be less heuristic.</p>
<p>$$f(x)=\sqrt{1-(\frac{x-b}{a})^2}\cdot a+c$$</p>
<p>with</p>
<p>$$F(x)=\frac{\pi}{2}=\int_{-1}^1f(x)dx;a=1,b=c=0$$</p>
<br>
<h4 id="f01z-spherical-functions-pi">[<code>F01Z</code>] Spherical functions, <math display="inline">
<mo>&pi;</mo>
</math></h4>
<hr />
<p>For source codes to <em>volume integrals</em> of the <em>sphere</em> see Schrausser (<a href="https://doi.org/10.5281/zenodo.14280500">2024</a>).</p>
<p>$$f_1(x,y)=\sqrt{(1-x^2)+(1-y^2)}$$</p>
<p>with</p>
<p>$$F_1(x,y)=\frac{\sqrt{2}\cdot8\cdot\pi}{3}=2\cdot\iint_{-\sqrt{2}}^{\sqrt{2}}f_1(x,y)dxdy$$</p>
<p>where</p>
<p><math display="inline"><mi>d</mi><mo>=</mo><mn>2</mn><mo>&middot;</mo><msqrt><mn>2</mn></msqrt>
</math><br />
<math display="inline"><mo>&pi;</mo><mo>=</mo><mfrac><mrow><mn>3</mn><mo>&middot;</mo><msub><mi>F</mi><mn>1</mn></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mrow><mn>8</mn><mo>&middot;</mo><msqrt><mn>2</mn></msqrt></mrow></mfrac>
</math><br />
<math display="inline"><mi>O</mi><mo>=</mo><mn>8</mn><mo>&middot;</mo><mo>&pi;</mo>
</math></p>
<p>$$f_2(x,y)=\sqrt{1-x^2-y^2}$$</p>
<p>with</p>
<p>$$F_2(x,y)=\frac{4\cdot\pi}{3}=2\cdot\iint_{-1}^{1}f_2(x,y)dxdy$$</p>
<p>where</p>

<p><math display="inline"><mi>d</mi><mo>=</mo><mn>2</mn></math><br />

<math display="inline"><mo>&pi;</mo><mo>=</mo><mfrac><mrow><mn>3</mn><mo>&middot;</mo><msub><mi>F</mi><mn>2</mn></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mn>4</mn></mfrac>
</math></p>
<math display="inline"><mi>O</mi><mo>=</mo><mn>4</mn><mo>&middot;</mo><mo>&pi;</mo>
</math>

<p>$$f_3(x,y)=\sqrt{(x-x^2)+(y-y^2)}$$</p>
<p>with</p>
<p>$$F_3(x,y)=\frac{\sqrt{2}\cdot\pi}{3}=2\cdot\iint_{1-\sqrt{\frac{1}{2}}-\frac{1}{2}}^{1+\sqrt{\frac{1}{2}}-\frac{1}{2}}f_3(x,y)dxdy$$</p>
<p>where</p>

<p><math display="inline"><mi>d</mi><mo>=</mo><msqrt><mn>2</mn></msqrt>
</math><br />
<math display="inline"><mo>&pi;</mo><mo>=</mo><mfrac><mrow><mn>3</mn><mo>&middot;</mo><msub><mi>F</mi><mn>3</mn></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><msqrt><mn>2</mn></msqrt></mfrac></math><br />
<math display="inline"><mi>O</mi><mo>=</mo><mo>&pi;</mo>
</math></p>
<br>
<h4 id="f04f01z-gamma-function-gamma">[<code>F04</code>|<code>F01Z</code>] Gamma function, <math display=„inline“> <mi>&Gamma;</mi> </math></h4>
<hr />
<p>To extend the <em>factorial</em> to <em>non-integer</em> arguments, first considered by Daniel Bernoulli and Christian Goldbach
(Bernoulli, <a href="https://commons.m.wikimedia.org/wiki/File:DanielBernoulliLetterToGoldbach-1729-10-06.jpg">1729</a>), later Leonhard Euler (<a href="https://scholarlycommons.pacific.edu/euler-works/19/">1738</a>) and Johann Carl Friedrich Gauss (s. Remmert, <a href="https://doi.org/10.1007/978-1-4757-2956-6_2">1998</a>), first tables were given by Jahnke and Emde (<a href="https://books.google.com/books?id=BVRzvgAACAAJ">1909</a>).</p>
<p>$$f(x,t)=\vartheta_\Gamma=t^{x-1}\cdot e^t$$</p>
<p>with</p>
<p>$$F(x,t)=\Gamma_x=\int_{0}^{\infty}f(x,t) dt+c,$$
$$F'(x)=\frac{\partial \Gamma}{\partial x}$$</p>
<br>
<br>
<h3 id="distribution-functions">Distribution functions</h3>
<p>The discovery of the <em>normal distribution</em> is attributed to Abraham de Moivre (<a href="https://books.google.com/books?id=PII_AAAAcAAJ">1738</a>), later Gauss (<a href="https://archive.org/details/theoriamotuscor00gausgoog/page/n1/mode/1up">1809</a>) described the <em>arithmetic mean</em> as an estimator in context with the <em>normal law of errors</em>. Beneath the <em>normal distribution</em>, Gauss (<a href="https://doi.org/10.3931/e-rara-2857">1823</a>) also introduces several important statistical concepts, such as the methods of <em>least squares</em> and of <em>maximum likelihood</em>.</p>

<p>The <math display="inline">
<mi>t</mi>
</math><em>-distribution</em> first derived as a posterior distribution by Lüroth (<a href="https://doi.org/10.1002/asna.18760871402">1876</a>), appearing later as <em>Pearson Type IV</em> (Pearson, <a href="https://doi.org/10.1098/rsta.1895.0010">1895</a>),  however gets its name as <em>Student's</em> <math display="inline">
<mi>t</mi>
</math><em>-distribution</em> from William Sealy Gosset (<a href="https://doi.org/10.2307/2331554">1908</a>), who published it using the pseudonym <em>Student</em>, though it was actually through the <em>extensive</em> works of Sir Ronald Aylmer Fisher that the distribution became well known.</p>
<p>The <math display="inline"><msup><mi>&chi;</mi><mn>2</mn></msup>
</math><em>-distribution</em> was first described by Friedrich Robert Helmert (<a href="https://gdz.sub.uni-goettingen.de/id/PPN599415665_0021">1876</a>) and independently <em>rediscovered</em> by Pearson (<a href="https://doi.org/10.1080/14786440009463897">1900b</a>) in context with the <em>goodness of fit</em> paradigm, where he developed the <math display="inline"><msup><mi>&chi;</mi><mn>2</mn></msup>
</math>-test with computed <em>table</em> of values, published by Elderton (<a href="https://doi.org/10.1093/biomet/1.2.155">1902</a>), s. further Pearson (<a href="https://doi.org/10.1093/biomet/10.1.85">1914</a>) or Plackett (<a href="https://doi.org/10.2307/1402731">1983</a>).</p>
<p>Fisher (<a href="https://hdl.handle.net/2440/15097">1918</a>, <a href="https://hdl.handle.net/2440/15169">1921</a>, <a href="https://www.scirp.org/reference/referencespapers?referenceid=2056938">1925</a>) introduced the term <em>variance</em> and proposed its formal analysis, as well as the <math display="inline">
<mi>F</mi><em>-distribution</em> (Fisher, <a href="https://repository.rothamsted.ac.uk/item/8w2q9/on-a-distribution-yielding-the-error-functions-of-several-well-known-statistics">1924</a>; s. also Snedecor, <a href="https://doi.org/10.1037/13308-000">1934</a> and Scheffé, <a href="https://psycnet.apa.org/record/1961-00074-000">1959</a>). The methods became widely known from <em>Methods for Research Workers</em> (Fisher, <a href="https://www.scirp.org/reference/referencespapers?referenceid=2056938">1925</a>, <a href="https://www.worldcat.org/de/title/statistical-methods-for-research-workers/oclc/312138">1954</a>, <a href="https://www.amazon.com/Statistical-methods-research-workers-Fourteenth/dp/0050021702">1973</a>, <a href="https://www.amazon.com/Statistical-Methods-Research-Workers-Fisher/dp/9351286584">2017</a>).</p>

<br></p>
<h4 id="q01_-zwertzvalzvalp-standardizing-z-values">[<code>Q01_</code>] [<code>ZWERT</code>|<code>zVAL</code>|<code>zVALp</code>] Standardizing, <math display="inline">
<mi>z</mi>
</math>-values</h4>
<hr />
<p>$$z=\frac{a-\overline x_x}{\sigma_x},$$</p>
<p>$$\zeta=z'=\frac{a-\overline x_x}{\hat\sigma_x}$$</p>
<p>where</p>
<p><math display="inline">
<mi>z</mi><mi>′</mi>
</math> = estimated population <math display="inline">
<mi>z</mi>
</math>-values</p>
<br>
<h4 id="npz-quantity-proportion-of-a-at-n-n_ge-p">[<code>npz</code>] Quantity proportion of <math display="inline">
<mi>a</mi>
</math> at <math display="inline">
<mi>N</mi>
</math>, <math display="inline"><msub><mi>n</mi><mrow data-mjx-texclass="ORD"><msup><mi></mi><mo>&ge;</mo></msup><mi>p</mi></mrow></msub>
</math></h4>
<hr />
<p>$$n_{^\ge p}=(1-p_{{\frac{a-\overline x}{\sigma}}})\cdot N$$</p>
<p>where</p>
<p><math display="inline">
<mi>a</mi>
</math> = test value<br />
<math display="inline">
<mi>N</mi>
</math> = number of cases, population</p>
<br>
<h4 id="amg-weighted-arithmetic-mean-ddotoverline-x">[<code>AMG</code>] Weighted arithmetic mean</h4>
<hr />
<p>$$\ddot{\overline x}=\frac{\sum_{i=1}^k{\overline x_i\cdot n_i}}{n}$$</p>
<br>
<h4 id="q01_amggm_-geometric-mean-dot-x">[<code>Q01_</code>|<code>AMG</code>|<code>GM_</code>] Geometric mean <math display="inline"><mrow data-mjx-texclass="ORD"><mover><mi>x</mi><mo>&dot;</mo></mover></mrow></math></h4>
<hr />
<p>For the <em>weighted geometric</em> mean <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow data-mjx-texclass="ORD"><mover><mrow data-mjx-texclass="ORD"><mover><mi>x</mi><mo>˙</mo></mover></mrow><mo>¨</mo></mover></mrow></math> s. Siegel (<a href="https://doi.org/10.1080/01621459.1942.10500636">1942</a>).</p>
<p>$$\dot x=\sqrt[n]{\prod_{i=1}^n{x_i}},$$</p>
<p>$$\ddot{\dot x}=\sqrt[\sum_{i=1}^k{n_i}]{\prod_{i=1}^k{\dot x_i}^{n_i}}$$</p>
<br>
<h4 id="q01_amghm_-harmonic-mean-overlineoverline-x">[<code>Q01_</code>|<code>AMG</code>|<code>HM_</code>] Harmonic mean <math display="inline"><mrow data-mjx-texclass="ORD"><mover><mi>x</mi><mo>=</mo></mover></mrow></math></h4>
<hr />
<p>$$\overline{\overline x}=H=\frac{n}{\sum_{i=1}^n{x_i^{-1}}},$$</p>
<p>$$\ddot{\overline{\overline x}}=\frac{\sum_{i=1}^k{n_i}}{\sum_{i=1}^k\frac{n_i}{\overline{\overline x}_i}}$$</p>
<br>
<h4 id="q01_-coefficient-of-variation-omega">[<code>Q01_</code>] Coefficient of variation <math display=„inline“> <mi>&omega;</mi> </math></h4>
<hr />
<p>$$\omega=\frac{\sigma}{\overline x};\overline x\gt0$$</p>
<br>
<h4 id="mdn-mean-dispersion-overline-d">[<code>MDN</code>] Mean dispersion <math display="inline"><mover><mi>d</mi><mo accent="true">&minus;</mo></mover>
</math></h4>
<hr />
<p>Schrausser (<a href="https://www.academia.edu/81395688">2022a</a>, p. 33).</p>
<p>$$\overline d=\frac{\sum_{i=1}^n{|x_i-\overline x|}}{\sum_{i=1}^n1};x_i\ne \overline x,$$</p>
<p>$$\hat{\overline \delta}=\sigma\cdot\zeta\cdot\sqrt{\frac{n}{n-1}}=\hat\sigma\cdot\zeta$$</p>
<p>with</p>
<p>$$\hat\sigma_{\overline d}=\sigma\cdot\frac{1}{2\cdot\zeta\cdot\sqrt{n}}$$</p>
<p>where</p>
<p><math display="inline"><msub><mi>&zeta;</mi><mrow data-mjx-texclass="ORD"><mover><mi>d</mi><mo accent="true">&minus;</mo></mover></mrow></msub><mo>=</mo><mover><mi>&delta;</mi><mo accent="true">&minus;</mo></mover><mo>=</mo><mfrac><mn>4</mn><msup><mn>5</mn></msup></mfrac>
</math></p>
<br>
<h4 id="nvtlg-e01f02f03-standard-normal-distribution-fxz">[<code>NVTLG</code>] [<code>E01</code>|<code>F02</code>|<code>F03</code>] Standard normal distribution <math display=„inline“><mi>f</mi><mo stretchy=„false“>(</mo><mi>x</mi><mo>=</mo><mi>z</mi><mo stretchy=„false“>)</mo>
</math></h4>
<hr />
<p>De Moivre  (<a href="https://books.google.com/books?id=PII_AAAAcAAJ">1738</a>), Gauss (<a href="https://archive.org/details/theoriamotuscor00gausgoog/page/n1/mode/1up">1809</a>, <a href="https://doi.org/10.3931/e-rara-2857">1823</a>).</p>
<p>$$f(z)=\vartheta=\frac{1}{\sqrt{2\cdot\pi}}\cdot e^{-\frac{z^2}{2}}$$</p>
<p>with</p>
<p>$$F(z)=p=\int_{-\infty}^z{f(z)dz},$$
$$f'(z)=\frac{\partial f(z)}{\partial z}$$</p>
<br>
<h4 id="nvxy-f01z-bivariate-normal-distribution-fz_1z_2">[<code>NVXY</code>] [<code>F01Z</code>] Bivariate normal distribution <math display="inline"><mi>f</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo stretchy="false">)</mo>
</math></h4>
<hr />
<p>$$f(z_1,z_2)=\vartheta=\frac{1}{2\cdot\pi\cdot\sqrt{1-r^2}}\cdot e^{\frac{-1}{2\cdot(1-r^2)}\cdot(z_1^2-2\cdot r\cdot z_1\cdot z_2+z_2^2)}$$</p>
<p>$$F(z_1,z_2)=1=\iint_{-\infty}^\infty f(z_1,z_2)dz_1dz_2$$</p>
<p>where</p>
<p><math display=„inline“> <mi>r</mi> </math> = correlation <math display="inline"><msub><mi>r</mi><mrow data-mjx-texclass="ORD"><mo stretchy="false">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></msub>
</math></p>
<br>
<h4 id="tvtlgf06_-f02f06f03z-students-t-distribution-fxt">[<code>tVTLG</code>|<code>F06_</code>] [<code>F02</code>|<code>F06</code>|<code>F03Z</code>] Student's <math display=„inline“> <mi>t</mi> </math>-distribution <math display=„inline“><mi>f</mi><mo stretchy=„false“>(</mo><mi>x</mi><mo>=</mo><mi>t</mi><mo stretchy=„false“>)</mo>
</math></h4>
<hr />
<p>Lüroth (<a href="https://doi.org/10.1002/asna.18760871402">1876</a>), Pearson (<a href="https://doi.org/10.1098/rsta.1895.0010">1895</a>), Gosset (<a href="https://doi.org/10.2307/2331554">1908</a>).</p>
<p>$$f(t)=\vartheta=\frac{\Gamma_\frac{df+1}{2}}{\Gamma_{\frac{df}{2}}}\cdot(df\cdot \pi)^{-\frac{1}{2}}\cdot(1+\frac{t^2}{df})^{-\frac{df+1}{2}}$$</p>
<p>with</p>
<p>$$F(t,df)=p=\int_{-\infty}^t{f(t)dt}$$</p>
<p>where</p>
<p><math display="inline"><msub><mi mathvariant="normal">&Gamma;</mi><mi>x</mi></msub><mo>=</mo><msubsup><mo data-mjx-texclass="OP">&int;</mo><mrow data-mjx-texclass="ORD"><mn>0</mn></mrow><mi mathvariant="normal">&infin;</mi></msubsup><mrow data-mjx-texclass="ORD"><msup><mi>y</mi><mrow data-mjx-texclass="ORD"><mi>x</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>&middot;</mo><msup><mo>e</mo><mrow data-mjx-texclass="ORD"><mo>-</mo><mi>y</mi></mrow></msup><mi>d</mi><mi>y</mi><mo>+</mo><mi>c</mi></mrow>
</math></p>
<br>
<h4 id="ch2vtlgf07_-f02f07f03z-chi2-distribution-fxchi2">[<code>ch2VTLG</code>|<code>F07_</code>] [<code>F02</code>|<code>F07</code>|<code>F03Z</code>] <math display="inline"><msup><mi>&chi;</mi><mn>2</mn></msup>
</math>-distribution <math display="inline"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>=</mo><msup><mi>&chi;</mi><mn>2</mn></msup><mo stretchy="false">)</mo>
</math></h4>
<hr />
<p>Helmert (<a href="https://gdz.sub.uni-goettingen.de/id/PPN599415665_0021">1876</a>), Pearson (<a href="https://doi.org/10.1080/14786440009463897">1900b</a>, <a href="https://doi.org/10.1093/biomet/10.1.85">1914</a>), Elderton (<a href="https://doi.org/10.1093/biomet/1.2.155">1902</a>), Plackett (<a href="https://doi.org/10.2307/1402731">1983</a>).</p>
<p>$$f(\chi^2)=\vartheta=\frac{1}{2^\frac{df}{2}\cdot\Gamma_\frac{df}{2}}\cdot{\chi^2}^{\frac{df}{2}-1}\cdot e^{-\frac{\chi^2}{2}}$$</p>
<p>with</p>
<p>$$F(\chi^2,df)=1-p^{\alpha2}=\int_{0}^{\chi^2}{f(\chi^2)d\chi^2}$$</p>
<p>where</p>
<p><math display="inline"><msub><mi mathvariant="normal">&Gamma;</mi><mi>x</mi></msub><mo>=</mo><msubsup><mo data-mjx-texclass="OP">&int;</mo><mrow data-mjx-texclass="ORD"><mn>0</mn></mrow><mi mathvariant="normal">&infin;</mi></msubsup><mrow data-mjx-texclass="ORD"><msup><mi>y</mi><mrow data-mjx-texclass="ORD"><mi>x</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>&middot;</mo><msup><mo>e</mo><mrow data-mjx-texclass="ORD"><mo>-</mo><mi>y</mi></mrow></msup><mi>d</mi><mi>y</mi><mo>+</mo><mi>c</mi></mrow>
</math></p>
<br>
<h4 id="fvtlg-f02f03z-f-distribution-fxf">[<code>FVTLG</code>] [<code>F02</code>|<code>F03Z</code>] <math display=„inline“> <mi>F</mi> </math>-distribution <math display=„inline“><mi>f</mi><mo stretchy=„false“>(</mo><mi>x</mi><mo>=</mo><mi>F</mi><mo stretchy=„false“>)</mo>
</math></h4>
<hr />
<p>Fisher (<a href="https://repository.rothamsted.ac.uk/item/8w2q9/on-a-distribution-yielding-the-error-functions-of-several-well-known-statistics">1924</a>), Snedecor (<a href="https://doi.org/10.1037/13308-000">1934</a>), Scheffé (<a href="https://psycnet.apa.org/record/1961-00074-000">1959</a>).</p>
<p>$$f(F)=\vartheta=\frac{\Gamma_{\frac{df_1+df_2}{2}}}{\Gamma_{\frac{df_1}{2}}\cdot\Gamma_{\frac{df_2}{2}}}\cdot(\frac{df_1}{df_2})^{\frac{df_1}{2}}\cdot F^{\frac{df_1}{2}-1}\cdot(1+\frac{df_1}{df_2}\cdot F)^{-\frac{df_1+df_2}{2}}$$</p>
<p>with</p>
<p>$$F(F,df_1,df_2)=1-p^{\alpha2}=\int_{0}^{F}{f(F)dF}$$</p>
<p>where</p>
<p><math display="inline"><msub><mi mathvariant="normal">&Gamma;</mi><mi>x</mi></msub><mo>=</mo><msubsup><mo data-mjx-texclass="OP">&int;</mo><mrow data-mjx-texclass="ORD"><mn>0</mn></mrow><mi mathvariant="normal">&infin;</mi></msubsup><mrow data-mjx-texclass="ORD"><msup><mi>y</mi><mrow data-mjx-texclass="ORD"><mi>x</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>&middot;</mo><msup><mo>e</mo><mrow data-mjx-texclass="ORD"><mo>-</mo><mi>y</mi></mrow></msup><mi>d</mi><mi>y</mi><mo>+</mo><mi>c</mi></mrow>
</math></p>
<br>
<h4 id="q02_-third-standardized-moment-skewness--alpha_3">[<code>Q02_</code>] Third standardized moment, skewness  <math display=„inline“><msub><mi>&alpha;</mi><mn>3</mn></msub>
</math></h4>
<hr />
<p>$$\alpha_3=\frac{\sum_{i=1}^n{z_i^3}}{n},$$</p>
<p>$$\hat\alpha_3=\frac{n\cdot\sum_{i=1}^n{(x_i-\overline x)^3}}{(n-1)\cdot(n-2)\cdot\hat\sigma^3}$$</p>
<p>with</p>
<p>$$z=\frac{\alpha_3}{\sqrt{\frac{6}{n}}}$$</p>
<br>
<h4 id="q02_-fourth-standardized-moment-excess-kurtosis-alpha_4">[<code>Q02_</code>] Fourth standardized moment, excess kurtosis <math display=„inline“><msub><mi>&alpha;</mi><mn>4</mn></msub>
</math></h4>
<hr />
<p>$$\alpha_4=\frac{\sum_{i=1}^n{z_i^4}}{n}-3,$$</p>
<p>$$\hat\alpha_4=\frac{n\cdot(n+1)\cdot\sum_{i=1}^n{(x_i-\overline x)^4}-3\cdot\sum_{i=1}^n{(x_i-\overline x)^2}\cdot\sum_{i=1}^n{(x_i-\overline x)^2\cdot(n-1)}}{(n-1)\cdot(n-2)\cdot(n-3)\cdot\hat\sigma^4}$$</p>
<p>with</p>
<p>$$z=\frac{\alpha_4}{2\cdot\sqrt{\frac{6}{n}}}$$</p>
<br>
<h4 id="smg-cix-estimated-standard-error-of-mean-hatsigma_overline-x-confidence-interval-ci_p">[<code>SMG</code>] [<code>CIx</code>] Estimated standard error of mean <math display="inline"><msub><mrow data-mjx-texclass="ORD"><mover><mi>&sigma;</mi><mo stretchy="false">^</mo></mover></mrow><mrow data-mjx-texclass="ORD"><mover><mi>x</mi><mo accent="true">&minus;</mo></mover></mrow></msub>
</math>, confidence interval <math display=„inline“><mi>C</mi><msub><mi>I</mi><mi>p</mi></msub>
</math></h4>
<hr />
<p>Neyman (<a href="https://doi.org/10.1098/rsta.1937.0005">1937</a>) introduced the <em>confidence interval</em> into statistical hypothesis testing vs. Fisher's <em>null hypothesis</em> testing, the <em>Neyman–Pearson lemma</em> (Neyman & Pearson, <a href="https://doi.org/10.1098/rsta.1933.0009">1933</a>; Lehmann, <a href="https://doi.org/10.1080/01621459.1993.10476404">1993</a>).</p>
<p>$$\hat\sigma_{\overline x}=\sqrt{\frac{\sigma\cdot\frac{n}{n-1}}{n}}$$</p>
<p>with</p>
<p>$$CI_p=\overline x\pm t_{(1-\frac{1-p}{2},n-1)}\cdot\hat\sigma_{\overline x},$$</p>
<p>$$CI_p=\theta\pm t_{(1-\frac{1-p}{2},df)}\cdot\hat\sigma_{\theta}$$</p>
<p>where</p>
<p><math display=„inline“> <mi>p</mi> </math> = probability<br />
<math display=„inline“> <mi>n</mi> </math> = number of cases</p>
<br>
<h4 id="cixy-cir-e01-standard-error-of-prediction-sigma_hat-yx-confidence-interval-ci_p">[<code>CIXY</code>] [<code>CIr</code>] [<code>E01</code>] Standard error of prediction <math display="inline"><msub><mi>&sigma;</mi><mrow data-mjx-texclass="ORD"><mrow data-mjx-texclass="ORD"><mover><mi>y</mi><mo stretchy="false">^</mo></mover></mrow><mi>x</mi></mrow></msub>
</math>, confidence interval <math display=„inline“><mi>C</mi><msub><mi>I</mi><mi>p</mi></msub>
</math></h4>
<hr />
<p>$$\sigma_{\hat yx}=\sigma_y\cdot\sqrt{1-r^2}$$</p>
<p>with</p>
<p>$$CI_p=\hat y_x\pm z_{(1-\frac{1-p}{2})}\cdot\sigma_{\hat yx}$$</p>
<p>where</p>
<p><math display=„inline“> <mi>p</mi> </math> = probability<br />
<math display=„inline“> <mi>r</mi> </math> = correlation<br />
<math display="inline"><mrow data-mjx-texclass="ORD"><mover><mi>y</mi><mo stretchy="false">^</mo></mover></mrow>
</math> = predicted value <math display=„inline“> <mi>y</mi> </math></p>
<br>
<h4 id="epsilon-efgefr-e01-effect-size-epsilon-cohens-d">[<code>EPSILON</code>] [<code>EFG</code>|<code>EFR</code>] [<code>E01</code>] Effect size <math display=„inline“> <mi>&epsilon;</mi> </math>, Cohen's <math display=„inline“> <mi>d</mi> </math></h4>
<hr />
<p>Cohen (<a href="https://doi.org/10.1016/C2013-0-10517-X">1977</a>,<a href="https://www.scirp.org/(S(lz5mqp453edsnp55rrgjct55))/reference/ReferencesPapers.aspx?ReferenceID=2041144">1988</a>, p. 20, p. 49, <a href="https://doi.org/10.1037/0033-2909.112.1.15">1992</a>), Borenstein et al.  (<a href="https://www.semanticscholar.org/paper/Power-and-precision-%3A-a-computer-program-for-power-Borenstein-Rothstein/f379f13a460b01488c35aea408e355436dbae839">1997</a>), Borenstein et al. (<a href="https://books.google.com/books?id=tYg02XZBeNAC&amp;printsec=frontcover&amp;hl=de#v=onepage&amp;q&amp;f=false">2001</a>).</p>
<p>$$\epsilon=d=\frac{\mu_1-\mu_0}{\hat\sigma},$$</p>
<p>$$d_v=\frac{d}{\sqrt{1-r}}$$</p>
<p>with</p>
<p>$$\overline x_{crit}^\beta=\mu_1±t_{(p_{crit},df)}\cdot\hat\sigma_{\overline x},$$</p>
<p>$$t_{(df)}^{\alpha}=\frac{\overline x_{0_1}-\mu_0}{\hat\sigma_{\overline x}},$$</p>
<p>$$t_{(df)}^{\beta}=\frac{\overline x_{0_1}-\mu_1}{\hat\sigma_{\overline x}}$$</p>
<p>where</p>
<p><math display="inline">
<msub><mi>d</mi><mi>v</mi></msub>
</math> = <math display=„inline“> <mi>d</mi> </math> for paired samples<br />
<math display=„inline“> <mi>r</mi> </math> = correlation<br />
Power = <math display="inline"><msup><mi>p</mi><mrow data-mjx-texclass="ORD"><mn>1</mn><mo>&minus;</mo><mi>&beta;</mi></mrow></msup><mo>=</mo><mn>1</mn><mo>&minus;</mo><msup><mi>p</mi><mrow data-mjx-texclass="ORD"><mi>&beta;</mi></mrow></msup>
</math></p>
<br>
<h4 id="epsilon2-optimal-effect-size-epsilon_p">[<code>EPSILON2</code>] Optimal effect size <math display="inline"><msub><mi>&epsilon;</mi><mi>p</mi></msub>
</math></h4>
<hr />
<p>$$\epsilon_p=\sqrt{\frac{{(2\cdot t_{(p_{crit},df)}})^2}{n}},$$</p>
<br>
<h4 id="epsilon2-optimal-alpha-level">[<code>EPSILON2</code>] Optimal alpha level</h4>
<hr />
<p>$$t^\alpha _{(opt,df)}=\frac{\sqrt{\epsilon^2\cdot n}}{2}$$</p>
<br>
<h4 id="tkv-ttkvptkv-variance-difference-t-test">[<code>TKV</code>] [<code>tTKV</code>|<code>pTKV</code>] Variance difference <math display=„inline“> <mi>t</mi> </math>-test</h4>
<hr />
<p>For <em>paired</em> samples <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mrow data-mjx-texclass="ORD"><mo stretchy="false">|</mo></mrow><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false">)</mo></math>.</p>
<p>$$\theta=d_{\sigma ^2}=\sigma_1^2-\sigma_2^2$$</p>
<p>with</p>
<p>$$t_{(df)}=d_{\sigma^2}\cdot\frac{\sqrt{n-2}}{2\cdot\sqrt{\sigma_1^2\cdot\sigma_2^2\cdot(1-r^2)}}$$</p>
<p>where</p>
<p><math display=„inline“>
<mi>d</mi><mi>f</mi>
</math> = <math display="inline">
<mi>n</mi><mo>&minus; 2</mo>
</math></p>
<br>
<h4 id="tv_-ttvptv-paired-2-sample-t-test">[<code>TV_</code>] [<code>tTV</code>|<code>pTV</code>] Paired 2-sample <math display=„inline“> <mi>t</mi> </math>-test</h4>
<hr />
<p>$$\theta=\overline x_d=\frac{\sum_{i=1}^n{x_{(i,1)}-x_{(i,2)}}}{n}$$</p>
<p>with</p>
<p>$$t_{(df)}=\frac{\overline x_d}{\hat\sigma_{\overline x_d}},$$</p>
<p>$$\hat\sigma_{\overline x_d}=\sqrt{{\frac{\sum_{i=1}^n{(x_{(i,1)}-x_{(i,2)})^2}-\frac{(\sum_{i=1}^n{x_{(i,1)}-x_{(i,2)}})^2}{n}}{n-1}}}\cdot \frac{1}{\sqrt{n}}$$</p>
<p>where</p>
<p><math display="inline"><msub><mover><mi>x</mi><mo accent="true">&minus;</mo></mover><mi>d</mi></msub>
</math> = mean of the differences of <math display="inline"><msub><mi>x</mi><mi>1</mi></msub></math> and <math display="inline"><msub><mi>x</mi><mi>2</mi></msub></math> values<br />
<math display=„inline“>
<mi>d</mi><mi>f</mi>
</math> = <math display="inline">
<mi>n</mi><mo>&minus; 1</mo>
</math></p>
<br>
<h4 id="tu_-ttu_ptu_ttuxptux-unpaired-2-sample-t-test">[<code>TU_</code>] [<code>tTU_</code>|<code>pTU_</code>|<code>tTUx</code>|<code>pTUx</code>] Unpaired 2-sample <math display=„inline“> <mi>t</mi> </math>-test</h4>
<hr />
<p>$$\theta=d_\overline{x}=\overline x_1-\overline x_2$$</p>
<p>with</p>
<p>$$t_{(df)}=\frac{d_\overline{x}}{\hat\sigma_{d_{\overline x}}},$$</p>
<p>$$\hat\sigma_{d_{\overline x}}=\sqrt{\frac{\sum_{i=1}^{n_1}{(x_{(i,1)}-\overline{x}_1)^2}+\sum_{i=1}^{n_2}{(x_{(i,2)}-\overline{x}_2)^2}}{n-2}}\cdot\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}$$</p>
<p>where</p>
<p><math display="inline"><msub><mi>d</mi><mrow data-mjx-texclass="ORD"><mover><mi>x</mi><mo accent="true">&minus;</mo></mover></mrow></msub>
</math> = difference of the means <math display="inline"><msub><mover><mi>x</mi><mo accent="true">&minus;</mo></mover><mn>1</mn></msub>
</math> and <math display="inline"><msub><mover><mi>x</mi><mo accent="true">&minus;</mo></mover><mn>2</mn></msub>
</math><br />
<math display=„inline“>
<mi>d</mi><mi>f</mi>
</math> = <math display="inline"><mi>n</mi><mo>&minus;</mo><mn>2</mn>
</math></p>
<br>
<h4 id="tt_-ttt_ptt_-one-sample-t-test">[<code>TT_</code>] [<code>tTT_</code>|<code>pTT_</code>] One-sample <math display=„inline“> <mi>t</mi> </math>-test</h4>
<hr />
<p>$$\theta=d_{\overline x y}=\overline x-y$$</p>
<p>with</p>
<p>$$t_{(df)}=\frac{d_{\overline x y}}{\sqrt{\frac{\sigma^2}{n-1}}}$$</p>
<p>where</p>
<p><math display="inline"><msub><mi>d</mi><mrow data-mjx-texclass="ORD"><mover><mi>x</mi><mo accent="true">&minus;</mo></mover><mi>y</mi></mrow></msub>
</math> = difference between sample mean <math display="inline"><mover><mi>x</mi><mo accent="true">&minus;</mo></mover>
</math> and test value <math display=„inline“> <mi>y</mi> </math><br />
<math display=„inline“>
<mi>d</mi><mi>f</mi>
</math> = <math display="inline">
<mi>n</mi><mo>&minus; 1</mo>
</math></p>
<br>
<h4 id="abt1-x2fp2fzbnpzbn-chi2-test-for-independence">[<code>ABT1</code>] [<code>x2F</code>|<code>p2F</code>|<code>zBN</code>|<code>pzBN</code>] <math display="inline"><msup><mi>&chi;</mi><mn>2</mn></msup>
</math>-test for independence</h4>
<hr />
<p>$$\chi^2=\sum_{i=1}^n{\frac{(f_{e_i}-f_{b_i})^2}{f_{b_i}}}$$</p>
<p>with</p>
<p>$$z=\frac{b-\frac{b+c}{2}}{\sqrt{\frac{b+c}{4}}}$$</p>
<br>
<h4 id="vfch-x4fp4fx4fyp4fyz4fpz4f-2--2-chi2-test-for-independence">[<code>VFCH</code>] [<code>x4F</code>|<code>p4F</code>|<code>x4FY</code>|<code>p4FY</code>|<code>z4F</code>|<code>pz4F</code>] 2 × 2 <math display="inline"><msup><mi>&chi;</mi><mn>2</mn></msup>
</math>-test for independence</h4>
<hr />
<p>For Yates's correction for <em>continuity</em> see Yates (<a href="http://www.jstor.org/stable/2983604">1934</a>).</p>
<p>$$\chi^2=\frac{N\cdot (a\cdot d-b\cdot c)^2}{(a+b)\cdot(c+d)\cdot(a+c)\cdot(b+d)},$$</p>
<p>$$\chi^2_{Yates}=\frac{N\cdot (|a\cdot d-b\cdot c|\cdot\frac{N}{2})^2}{(a+b)\cdot(c+d)\cdot(a+c)\cdot(b+d)};4\lt f_e\lt7$$</p>
<p>with</p>
<p>$$z=\frac{d-N\cdot P_d}{\sqrt{N\cdot P_d\cdot(1-P_d)-N\cdot (N-1)\cdot P_d\cdot(P_d-\hat P_d)}}$$</p>
<p>where</p>
<p><math display="inline"><msub><mi>f</mi><mn>e</mn></msub>
</math> = expected frequency<br />
<math display="inline"><msub><mi>P</mi><mi>d</mi></msub><mo>=</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>d</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo>&middot;</mo><mo stretchy="false">(</mo><mi>c</mi><mo>+</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><msup><mi>N</mi><mn>2</mn></msup></mfrac>
</math><br />
<math display="inline"><msub><mrow data-mjx-texclass="ORD"><mover><mi>P</mi><mo stretchy="false">^</mo></mover></mrow><mi>d</mi></msub><mo>=</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>d</mi><mo>+</mo><mi>b</mi><mo>&minus;</mo><mn>1</mn><mo stretchy="false">)</mo><mo>&middot;</mo><mo stretchy="false">(</mo><mi>c</mi><mo>+</mo><mi>d</mi><mo>&minus;</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>N</mi><mo>&minus;</mo><mn>1</mn><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow></mfrac>
</math><br />
<math display=„inline“>
<mi>d</mi><mi>f</mi>
</math>=1</p>
<br>
<h4 id="vfch-xmnpmnxmnypmny-mcnemars-chi2-test-for-paired--2--2-contingency-tables-with-dichotomous-trait">[<code>VFCH</code>] [<code>xMN</code>|<code>pMN</code>|<code>xMNY</code>|<code>pMNY</code>] McNemar's <math display="inline"><msup><mi>&chi;</mi><mn>2</mn></msup>
</math>-test for paired  2 × 2 contingency tables with dichotomous trait</h4>
<hr />
<p>McNemar (<a href="https://doi.org/10.1007/BF02295996">1947</a>).</p>
<p>$$\chi^2=\frac{(b-c)^2}{b+c},$$</p>
<p>$$\chi^2=\frac{(|b-c|-\frac{1}{2})^2}{b+c};20\lt (b+c)\lt 30$$</p>
<br>
<br>
<h3 id="probability">Probability</h3>
<p>Since until the Renaissance a <em>probable</em> opinion was merely <em>confirmed</em> by an authority and hence there was no further concept of <em>inductive</em> evidence (see Hacking, <a href="https://philpapers.org/rec/HACTEO-8">1975</a>; Hald, <a href="https://www.wiley.com/en-us/A%2BHistory%2Bof%2BProbability%2Band%2BStatistics%2Band%2BTheir%2BApplications%2Bbefore%2B1750-p-9780471725176">2003</a>, p. 31), an <em>objective</em> representation of <em>probability</em> as such was first discussed by Antoine Arnauld and Pierre Nicole, (<a href="https://gallica.bnf.fr/ark:/12148/bpt6k574432.image">1662</a>, <a href="https://books.google.com/books?id=XQVaAAAAcAAJ">1682</a>, <a href="https://archive.org/details/logicorartofthin00arnaiala">1693</a>; c.f. also Arnauld et al., <a href="https://philpapers.org/rec/ARNLLO-8">1970</a>; van Evra, <a href="https://philpapers.org/rec/VANAAA-13">1997</a>; Dessì & Albury, <a href="https://doi.org/10.1080/01445349708837281">1997</a> or Finocchiaro, <a href="https://doi.org/10.1023/A:1007756105432">1997</a>).</p>
<p>The <em>binomial distribution</em> is primarily attributable to de Moivre (<a href="https://doi.org/10.1098/rstl.1710.0018">1711</a>, <a href="https://books.google.com/books?id=3EPac6QpbuMC">1718</a>, <a href="https://books.google.com/books?id=PII_AAAAcAAJ">1738</a>) and Jacob Bernoulli (<a href="https://www.e-rara.ch/zut/doi/10.3931/e-rara-9001">1713</a>), see also Schneider (<a href="https://doi.org/10.1016/B978-044450871-3/50087-5">2005a</a>, <a href="https://doi.org/10.1016/B978-044450871-3/50087-5">b</a>). Although not included as function, due to its <em>considerability</em> in this context, the <em>configuration frequency analysis, CFA</em> should be mentioned particularly (c.f. Krauth, <a href="https://d-nb.info/740097938">1973</a>; Krauth & Lienert, <a href="https://books.google.com/books?id=4oeIAAAACAAJ">1993</a>).</p>
<p>An account of the <em>systematics and logic</em> of <em>dependent probabilities</em> within the framework of <em>Bayes' theorem</em> (Bayes & Price, <a href="http://www.jstor.org/stable/105741">1763</a>; c.f. Stigler, <a href="https://www.jstor.org/stable/26770983">2018</a>) can be found in Schrausser (<a href="https://doi.org/10.5281/zenodo.14183565">2024c</a>).</p>
<p>The arguably <em>most</em> important methods regarding the calculation of <em>probability parameters</em> are implemente.</p>
<br>
<h4 id="chchachb-arcsine-transformation-cohens-h">[<code>Ch</code>|<code>ChA</code>|<code>ChB</code>] Arcsine transformation, Cohen's <math display=„inline“> <mi>h</mi> </math></h4>
<hr />
<p>Cohen (<a href="https://www.scirp.org/(S(lz5mqp453edsnp55rrgjct55))/reference/ReferencesPapers.aspx?ReferenceID=2041144">1988</a>, p. 181).</p>
<p>$$\theta=p_1-p_2,$$</p>
<p>$$h=2\cdot\arcsin{\sqrt{p_1}}-2\cdot\arcsin{\sqrt{p_2}}$$</p>
<p>with</p>
<p>$$p_1=\sin({\frac{2\cdot\arcsin\sqrt{p_2}+h}{2}})^2,$$</p>
<p>$$p_2=-\sin({\frac{-2\cdot\arcsin\sqrt{p_1}+h}{2}})^2$$</p>
<p>where</p>
<p>probabilities = <math display="inline">
<msub><mi>p</mi><mi>1</mi></msub>
</math>, <math display="inline">
<msub><mi>p</mi><mi>2</mi></msub>
</math></p>
<br>
<h4 id="abt1-addp-e01-additive-probability-for-independent-events-u-pcup_n-a">[<code>ABT1</code>] [<code>ADDP</code>] [<code>E01</code>] Additive probability for independent events <math display="inline"><msup><mi></mi><mi>u</mi></msup><mi>p</mi><mo stretchy="false">(</mo><msub><mo>&cup;</mo><mi>n</mi></msub><mi>A</mi><mo stretchy="false">)</mo>
</math></h4>
<hr />
<p>Corresponds to the <em>geometric distribution</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>f</mi><mo stretchy="false">(</mo><mi>X</mi><mo>≤</mo><mi>r</mi><mrow data-mjx-texclass="ORD"><mo stretchy="false">|</mo></mrow><mi>p</mi><mo stretchy="false">)</mo></math>.</p>
<p>$$^u p(\cup_n A)=1-(1-pA)^n$$</p>
<p>where</p>
<p><math display=„inline“> <mi>n</mi> </math> = number of events A<br />
<math display=„inline“> <mi>p</mi><mi>A</mi> </math> = probability of event A</p>
<br>
<h4 id="gmvtlg-geometric-distribution-fxle-rp">[<code>GMVTLG</code>] Geometric distribution <math display=„inline“><mi>f</mi><mo stretchy=„false“>(</mo><mi>X</mi><mo>&le;</mo><mi>r</mi><mrow data-mjx-texclass=„ORD“><mo stretchy=„false“>|</mo></mrow><mi>p</mi><mo stretchy=„false“>)</mo>
</math></h4>
<hr />
<p>Corresponds to the <em>additive probability</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><msup><mi></mi><mi>u</mi></msup><mi>p</mi><mo stretchy="false">(</mo><msub><mo>∪</mo><mi>n</mi></msub><mi>A</mi><mo stretchy="false">)</mo></math>.</p>
<p>$$f(X=r|p)=P_n=p\cdot q^r$$</p>
<p>with</p>
<p>$$f(X\le r|p)=p_n=\sum_{i=0}^r{p\cdot q^i}$$</p>
<p>where</p>
<p><math display=„inline“> <mi>p</mi> </math> = probability of event<br />
<math display="inline"><mi>r</mi><mo>+</mo><mn>1</mn><mo>=</mo><mi>n</mi>
</math> = number of events</p>
<br>
<h4 id="nbnmvtlg-nbinom-e01-negative-binomial-distribution-fxle-rrp">[<code>NBNMVTLG</code>] [<code>NBINOM</code>] [<code>E01</code>] Negative binomial distribution <math display=„inline“><mi>f</mi><mo stretchy=„false“>(</mo><mi>X</mi><mo>&le;</mo><mi>r</mi><mrow data-mjx-texclass=„ORD“><mo stretchy=„false“>|</mo></mrow><mi>r</mi><mo>,</mo><mi>p</mi><mo stretchy=„false“>)</mo>
</math></h4>
<hr />
<p>With <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>k</mi><mo>=</mo><mn>1</mn></math> it corresponds to the <em>geometric distribution</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>f</mi><mo stretchy="false">(</mo><mi>X</mi><mo>≤</mo><mi>r</mi><mrow data-mjx-texclass="ORD"><mo stretchy="false">|</mo></mrow><mi>p</mi><mo stretchy="false">)</mo></math> and the <em>additive probability</em> <math xmlns="http://www.w3.org/1998/Math/MathML" display=inline"><msup><mi></mi><mi>u</mi></msup><mi>p</mi><mo stretchy="false">(</mo><msub><mo>∪</mo><mi>n</mi></msub><mi>A</mi><mo stretchy="false">)</mo></math>.</p>
<p>$$f(X=r|r,p)=P_n=\frac{(k+r-1)!}{r!\cdot(k-1)!} \cdot p^k\cdot q^r$$</p>
<p>with</p>
<p>$$f(X\le r|r,p)=p_n=\sum_{i=0}^r{\frac{(k+i-1)!}{i!\cdot(k-1)!} \cdot p^k\cdot q^i}$$</p>
<p>where</p>
<p><math display="inline"><mi>r</mi><mo>+</mo><mn>k</mn><mo>=</mo><mi>n</mi>
</math> = number of events<br />
<math display=„inline“> <mi>k</mi> </math> = number of successes</p
<br>
<h4 id="abt1-binomzbnpzbn-e01-exact-binomial-test">[<code>ABT1</code>] [<code>BINOM</code>|<code>zBN</code>|<code>pzBN</code>] [<code>E01</code>] Exact binomial test</h4>
<hr />
<p>$$f(X=b|b,c)=P0=\frac{(b+c)!}{b!\cdot c!}\cdot2^{-b}\cdot2^{-c}$$</p>
<p>with</p>
<p>$$f(X\le b|b,c)=p=p^{exact1}=\sum_{i=0}^b{\frac{(b+c)!}{i!\cdot(b+c-i)!}}\cdot2^{-i}\cdot2^{-(b+c-i)};p\le\frac{1}{2},$$</p>
<p>$$p^{exact1}=(1-p)+P0;p\gt\frac{1}{2}$$</p>
<p>also</p>
<p>$$z=\frac{b-\frac{b+c}{2}}{\sqrt{\frac{b+c}{4}}}$$</p>
<br>
<h4 id="fx_-z4fpz4f-exact-hypergeometric-2--2-test">[<code>FX_</code>] [<code>z4F</code>|<code>pz4F</code>] Exact hypergeometric 2 × 2 test</h4>
<hr />
<p><em>Fisher Exact</em> test (Fisher, <a href="https://doi.org/10.2307/2340521">1922</a>; Agresti, <a href="https://doi.org/10.1214/ss/1177011454">1992</a>).</p>
<p>$$f(X=a|a,b,c,d)=P0=\frac{(a+b)!\cdot(c+d)!\cdot(a+c)!\cdot(b+d)!}{N!\cdot a!\cdot b!\cdot c!\cdot d!}$$</p>
<p>with</p>
<p>$$f(X\le a|a,b,c,d)=p^{exact1}=\sum_{i=1}^aPi;p\le\frac{1}{2},$$
$$f(X\ge a|a,b,c,d)=p^{exact1}=\sum_{i=a}^nPi;p\gt\frac{1}{2}$$</p>
<p>where</p>
<p><math display="inline"><mi>P</mi><mi>i</mi><mo>=</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>a</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo>!</mo><mo>&middot;</mo><mo stretchy="false">(</mo><mi>c</mi><mo>+</mo><mi>d</mi><mo stretchy="false">)</mo><mo>!</mo><mo>&middot;</mo><mo stretchy="false">(</mo><mi>a</mi><mo>+</mo><mi>c</mi><mo stretchy="false">)</mo><mo>!</mo><mo>&middot;</mo><mo stretchy="false">(</mo><mi>b</mi><mo>+</mo><mi>d</mi><mo stretchy="false">)</mo><mo>!</mo></mrow><mrow><mi>N</mi><mo>!</mo><mo>&middot;</mo><mi>i</mi><mo>!</mo><mo>&middot;</mo><mo stretchy="false">(</mo><mi>a</mi><mo>+</mo><mi>b</mi><mo>&minus;</mo><mi>i</mi><mo stretchy="false">)</mo><mo>!</mo><mo>&middot;</mo><mo stretchy="false">(</mo><mi>a</mi><mo>+</mo><mi>c</mi><mo>&minus;</mo><mi>i</mi><mo stretchy="false">)</mo><mo>!</mo><mo>&middot;</mo><mo stretchy="false">(</mo><mn>2</mn><mo>&middot;</mo><mi>c</mi><mo>+</mo><mi>d</mi><mo>&minus;</mo><mi>a</mi><mo>&minus;</mo><mi>i</mi><mo stretchy="false">)</mo><mo>!</mo></mrow></mfrac>
</math></p>
<p>also</p>
<p>$$z=\frac{d-N\cdot P_d}{\sqrt{N\cdot P_d\cdot(1-P_d)-N\cdot (N-1)\cdot P_d\cdot(P_d-\hat P_d)}}$$</p>
<p>where</p>
<p><math display="inline"><msub><mi>P</mi><mi>d</mi></msub><mo>=</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>d</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo>&middot;</mo><mo stretchy="false">(</mo><mi>c</mi><mo>+</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><msup><mi>N</mi><mn>2</mn></msup></mfrac>
</math><br />
<math display="inline"><msub><mrow data-mjx-texclass="ORD"><mover><mi>P</mi><mo stretchy="false">^</mo></mover></mrow><mi>d</mi></msub><mo>=</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>d</mi><mo>+</mo><mi>b</mi><mo>&minus;</mo><mn>1</mn><mo stretchy="false">)</mo><mo>&middot;</mo><mo stretchy="false">(</mo><mi>c</mi><mo>+</mo><mi>d</mi><mo>&minus;</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>N</mi><mo>&minus;</mo><mn>1</mn><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow></mfrac>
</math><br />
<math display=„inline“>
<mi>d</mi><mi>f</mi>
</math>=1</p>
<br>
<br>
<h3 id="combinatorics">Combinatorics</h3>
<p>After Gersonides' <em>pioneering</em> work from 1321 dealing with <em>arithmetical</em> operations and <em>combinatorics</em> (s. Abraham Bar Hiyya Savasorda, <a href="https://www.loc.gov/item/2021667539/">1450</a>; Rabinovitch, <a href="http://www.jstor.org/stable/41133303">1970</a>), the methods, being a fundamental part for <em>probability calculations</em>, are <em>mainly</em> based on Blaise Pascal (<a href="https://gallica.bnf.fr/ark:/12148/btv1b86262012.image#">1665</a>), Bernoulli (<a href="https://www.e-rara.ch/zut/doi/10.3931/e-rara-9001">1713</a>) and Euler (<a href="https://scholarlycommons.pacific.edu/euler-works/201/">1753</a>), c.f. Ettingshausen (<a href="https://archive.org/details/diecombinatoris00ettigoog/page/n70/mode/1up?view=theater">1826</a>).</p>
<p>See further Sylvester (<a href="https://archive.org/details/collectedmathem01sylvrich/page/n7/mode/1up">1904</a>, <a href="https://archive.org/details/SylvesterCollected2/page/n3/mode/1up">1908</a>, <a href="https://archive.org/details/TheCollectedMathematicalPapersOfJamesJosephSylvesterVolumeIii/page/n3/mode/1up">1909</a>, <a href="https://archive.org/details/collectedmathema04sylvuoft/page/n8/mode/1up">1912</a>) and MacMahon (<a href="https://openlibrary.org/works/OL1109964W/Combinatory_analysis">1915</a>, <a href="https://books.google.com/books/about/Combinatory_Analysis.html?id=A_PuAAAAMAAJ&amp;redir_esc=y">1916</a>), giving fundamental contributions to <em>matrix-theory</em> and <em>combinatorics</em>.</p>
<p>The functions generate <em>permutation</em> and <em>variation</em> matrices <em>primarily</em> to support the <em>resampling</em> procedures described below (s. Resampling).</p>
<br>
<h4 id="prm2-permutation-matrix-mathbfp_n">[<code>PRM2</code>] Permutation matrix <math display="inline"><mrow data-mjx-texclass="ORD"><msub><mi mathvariant="bold">P</mi><mi mathvariant="bold">n</mi></msub></mrow>
</math></h4>
<hr />
<p><math display=„inline“> <mi>n</mi> </math> elements to <math display="inline"><mi>k</mi><mo>=</mo><mo>1</mo>
</math> class.</p>
<p>$$\mathbf{P_n}=\begin{bmatrix}p_1(x_1)&amp;\cdots&amp;p_1(x_n)\\\vdots&amp;\ddots&amp;\vdots\\p_P(x_1)&amp;\cdots&amp;p_P(x_n)\end{bmatrix}$$</p>
<p>where</p>
<p><math display="inline"><msub><mi>P</mi><mi>n</mi></msub><mo>=</mo><mi>n</mi><mo>!</mo>
</math></p>
<br>
<h4 id="prm5-variation-matrix-mathbf-w-v_2m">[<code>PRM5</code>] Variation matrix <math display="inline"><mrow data-mjx-texclass="ORD"><msup><mi></mi><mi mathvariant="bold">w</mi></msup><msubsup><mi mathvariant="bold">V</mi><mn mathvariant="bold">2</mn><mi mathvariant="bold">m</mi></msubsup></mrow></math></h4>
<hr />
<p>For the <em>dependent</em> 2 sample design, <math display="inline">
<mi>n</mi><mo>= 2</mo>
</math> elements to class <math display=„inline“> <mi>m</mi> </math>.</p>
<p>$$\mathbf{ ^w V_2^m}=\begin{bmatrix}v_1(x_1)&amp;\cdots&amp;v_1(x_m)\\\vdots&amp;\ddots&amp;\vdots\\v_{^wV}(x_1)&amp;\cdots&amp;v_{^wV}(x_m)\end{bmatrix}$$</p>
<p>where</p>
<p><math display="inline"><msup><mi></mi><mi>w</mi></msup><msubsup><mi>V</mi><mn>2</mn><mi>m</mi></msubsup><mo>=</mo><msup><mn>2</mn><mi>m</mi></msup>
</math></p>
<br>
<h4 id="prm4-variation-matrix-mathbf-w-v_nm">[<code>PRM4</code>] Variation matrix <math display="inline"><mrow data-mjx-texclass="ORD"><msup><mi></mi><mi mathvariant="bold">w</mi></msup><msubsup><mi mathvariant="bold">V</mi><mi mathvariant="bold">n</mi><mi mathvariant="bold">m</mi></msubsup></mrow></math></h4>
<hr />
<p><math display=„inline“> <mi>n</mi> </math> elements to class <math display=„inline“> <mi>m</mi> </math>.</p>
<p>$$\mathbf{ ^w V_n^m}=\begin{bmatrix}v_1(x_1)&amp;\cdots&amp;v_1(x_m)\\\vdots&amp;\ddots&amp;\vdots\\v_{^wV}(x_1)&amp;\cdots&amp;v_{^wV}(x_m)\end{bmatrix}$$</p>
<p>where</p>
<p><math display="inline"><msup><mi></mi><mi>w</mi></msup><msubsup><mi>V</mi><mi>n</mi><mi>m</mi></msubsup><mo>=</mo><msup><mi>n</mi><mi>m</mi></msup><mo>;</mo><mi>n</mi><mo>&gt;</mo><mi>m</mi>
</math></p>
<br>
<h4 id="prm3-nk-permutation-matrix-mathbfw-p_nk_mk_n-m">[<code>PRM3</code>] [<code>nk</code>] Permutation matrix <math display="inline"><mrow data-mjx-texclass="ORD"><msup><mi></mi><mi mathvariant="bold">w</mi></msup><msubsup><mi mathvariant="bold">P</mi><mi mathvariant="bold">n</mi><mrow data-mjx-texclass="ORD"><mo mathvariant="bold" stretchy="false">(</mo><msub><mi mathvariant="bold">k</mi><mi mathvariant="bold">m</mi></msub><mo mathvariant="bold">,</mo><msub><mi mathvariant="bold">k</mi><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">n</mi><mo mathvariant="bold">-</mo><mi mathvariant="bold">m</mi></mrow></msub><mo mathvariant="bold" stretchy="false">)</mo></mrow></h4>
<hr />
<p><math display=„inline“> <mi>n</mi> </math> elements to class <math display=„inline“> <mi>m</mi> </math>.</p>
<p>$$\mathbf{^w P_n^{(k_m,k_{m-n})}}=\begin{bmatrix}\begin{array}{ccc|ccc} p_1(x_{11})&amp;\cdots&amp;p_1(x_{k_1})&amp;p_1(x_{12})&amp;\cdots&amp;p_1(x_{k_2})\\\vdots&amp;\ddots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\p_{^wP}(x_{11})&amp;\cdots&amp;p_{^wP}(x_{k_1})&amp;p_{^wP}(x_{12})&amp;\cdots&amp;p_{^wP}(x_{k_2}) \end{array}\end{bmatrix}$$</p>
<p>where</p>
<p><math display="inline"><msup><mi></mi><mi>w</mi></msup><msubsup><mi>P</mi><mi>n</mi><mrow data-mjx-texclass="ORD"><mo stretchy="false">(</mo><msub><mi>k</mi><mi>m</mi></msub><mo>,</mo><msub><mi>k</mi><mrow data-mjx-texclass="ORD"><mi>n</mi><mo>&minus;</mo><mi>m</mi></mrow></msub><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>n</mi><mo>!</mo></mrow><mrow><munderover><mo data-mjx-texclass="OP">&Pi;</mo><mrow data-mjx-texclass="ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>2</mn></munderover><msub><mi>k</mi><mi>i</mi></msub><mo>!</mo></mrow></mfrac><mo>;</mo><mi>n</mi><mo>&ge;</mo><mi>m</mi>
</math></p>
<br>
<br>
<h3 id="resampling">Resampling</h3>
<p><em>Permutation</em> or <em>randomization tests</em> were first mentioned by Fisher (<a href="https://psycnet.apa.org/record/1939-04964-000">1935</a>), based on experiments in agriculture (Fisher, <a href="https://doi.org/10.23637/rothamsted.8v61q">1926</a>; Neyman, <a href="https://link.springer.com/chapter/10.1007/978-94-015-8816-4_10">1923</a>). In this context see Pitman (<a href="http://www.jstor.org/stable/2984124">1937a</a>, <a href="http://www.jstor.org/stable/2983647">b</a>, <a href="http://www.jstor.org/stable/2332008">1938</a>), Fisher (<a href="https://scirp.org/reference/referencespapers.aspx?referenceid=895747">1966</a>, <a href="https://home.iitk.ac.in/~shalab/anova/DOE-RAF.pdf">1971</a>, res.), especially Eugene Sinclair Edgington (<a href="https://doi.org/10.1080/00223980.1964.9916711">1964</a>, <a href="https://doi.org/10.2307/1164966">1980</a>, <a href="https://doi.org/10.1037/0022-0167.34.4.437">1987</a>, <a href="https://doi.org/10.1007/978-3-642-04898-2_56">2011</a>) or Edgington and Onghena (<a href="https://doi.org/10.1201/9781420011814">2007</a>).</p>
<p>The <em>bootstrap</em> method was introduced by Bradley Efron (<a href="https://doi.org/10.1214/aos/1176344552">1979</a>, <a href="https://doi.org/10.1093/biomet/68.3.589">1981</a>, <a href="https://doi.org/10.1137/1.9781611970319">1982</a>) as a further development (Quenouille, <a href="https://doi.org/10.1111/j.2517-6161.1949.tb00023.x">1949</a>; Metropolis & Ulam, <a href="https://doi.org/10.1080/01621459.1949.10483310">1949</a>), for <em>software</em> solutions see e.g. Solomon (<a href="https://doi.org/10.5555/1035853.1035900">1982</a>), Dallal (<a href="http://www.jstor.org/stable/2684555">1986</a>, <a href="https://doi.org/https://doi.org/10.1016/0010-4809(88)90037-7">1988</a>), Peladeau (<a href="https://doi.org/10.3758/BF03204533">1993</a>), Wooff and Peladeau (<a href="https://doi.org/10.2307/2986032">1994</a>), Mehta et al. (<a href="https://doi.org/10.1002/9781118445112.stat04892">2014</a>),  also Schrausser (<a href="https://doi.org/10.5281/zenodo.14280500">2024d</a>).</p>
<br>
<h4 id="pv_-permutation-test-p-for-2-paired-samples-x_1x_2">[<code>PV_</code>] Permutation test <em>P</em> for 2 paired samples <math display=„inline“><mo stretchy=„false“>(</mo><msub><mi>x</mi><mn>1</mn></msub><mrow data-mjx-texclass=„ORD“><mo stretchy=„false“>|</mo></mrow><msub><mi>x</mi><mn>2</mn></msub><mo stretchy=„false“>)</mo>
</math></h4>
<hr />
<p>Random sampling model, <em>systematic</em> permutation, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>p</mi></math>-value not randomized, variation matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow data-mjx-texclass="ORD"><msup><mi></mi><mi mathvariant="bold">w</mi></msup><msubsup><mi mathvariant="bold">V</mi><mn mathvariant="bold">2</mn><mi mathvariant="bold">m</mi></msubsup></mrow></math> <em>required</em>, s. Scambor (<a href="https://doi.org/10.13140/RG.2.2.28632.06405">1997</a>), Scambor and Schrausser (<a href="https://www.academia.edu/94993376">2022</a>, p. 7), respectively.</p>
<p>$$\Theta^1_1=\sum_{i=1}^n{x_{1_i}},\Theta^1_2=\sum_{i=1}^n{x_{2_i}},$$</p>
<p>$$\Theta^2=(\sum_{i=1}^n{x_{1_i}})^2+(\sum_{i=1}^n{x_{2_i}})^2$$</p>
<p>with</p>
<p>$$p^{exact}=\frac{\sum_{i=1}^{2^n}{1}}{2^n};\theta_i\ge \Theta$$</p>
<p>where</p>
<p><math display="inline"><msubsup><mi mathvariant="normal">&Theta;</mi><mn>1</mn><mn>1</mn></msubsup><mo>,</mo><msubsup><mi mathvariant="normal">&Theta;</mi><mn>2</mn><mn>1</mn></msubsup>
</math> = one-tailed test values<br />
<math display="inline"><msup><mi mathvariant="normal">&Theta;</mi><mn>2</mn></msup>
</math> = two-tailed test value</p>
<br>
<h4 id="mpv_-randomized-permutation-test-mp-for-2-paired-samples-x_1x_2">[<code>mPV_</code>] Randomized permutation test <em>mP</em> for 2 paired samples <math display=„inline“><mo stretchy=„false“>(</mo><msub><mi>x</mi><mn>1</mn></msub><mrow data-mjx-texclass=„ORD“><mo stretchy=„false“>|</mo></mrow><msub><mi>x</mi><mn>2</mn></msub><mo stretchy=„false“>)</mo>
</math></h4>
<hr />
<p>Random sampling model, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>p</mi></math>-value not randomized.</p>
<p>$$\Theta^1_1=\sum_{i=1}^n{x_{1_i}},\Theta^1_2=\sum_{i=1}^n{x_{2_i}},$$</p>
<p>$$\Theta^2=(\sum_{i=1}^n{x_{1_i}})^2+(\sum_{i=1}^n{x_{2_i}})^2$$</p>
<p>with</p>
<p>$$p=\frac{\sum_{i=1}^{M}{1}}{M};\theta_i\ge \Theta$$</p>
<p>where</p>
<p><math display="inline"><msubsup><mi mathvariant="normal">&Theta;</mi><mn>1</mn><mn>1</mn></msubsup><mo>,</mo><msubsup><mi mathvariant="normal">&Theta;</mi><mn>2</mn><mn>1</mn></msubsup>
</math> = one-tailed test values<br />
<math display="inline"><msup><mi mathvariant="normal">&Theta;</mi><mn>2</mn></msup>
</math> = two-tailed test value<br />
<math display=„inline“> <mi>M</mi> </math> = simulation cycles over variations <math display="inline"><msup><mi></mi><mi>w</mi></msup><msubsup><mi>V</mi><mn>2</mn><mi>m</mi></msubsup><mo>=</mo><msup><mn>2</mn><mi>n</mi></msup>
</math></p>
<br>
<h4 id="pu_-permutation-test-p-for-2-independent-samples-xg">[<code>PU_</code>] Permutation test <em>P</em> for 2 independent samples <math display=„inline“><mo stretchy=„false“>(</mo><mi>x</mi><mrow data-mjx-texclass=„ORD“><mo stretchy=„false“>|</mo></mrow><mi>g</mi><mo stretchy=„false“>)</mo>
</math></h4>
<hr />
<p>Random sampling model, <em>systematic</em> permutation, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>p</mi></math>-value not randomized, permutation matrix <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow data-mjx-texclass="ORD"><msup><mi></mi><mi mathvariant="bold">w</mi></msup><msubsup><mi mathvariant="bold">P</mi><mi mathvariant="bold">n</mi><mrow data-mjx-texclass="ORD"><mo mathvariant="bold" stretchy="false">(</mo><msub><mi mathvariant="bold">k</mi><mi mathvariant="bold">m</mi></msub><mo mathvariant="bold">,</mo><msub><mi mathvariant="bold">k</mi><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">n</mi><mo mathvariant="bold">−</mo><mi mathvariant="bold">m</mi></mrow></msub><mo mathvariant="bold" stretchy="false">)</mo></mrow></msubsup></mrow></math> <em>required</em>, see Schrausser (<a href="https://doi.org/10.13140/RG.2.2.24500.32640/1">1996</a>, <a href="https://doi.org/10.5281/zenodo.11673333">1998b</a>, <a href="https://www.academia.edu/82224369">2022b</a>, p. 2).</p>
<p>$$\Theta^1_1=\sum_{i=1}^{n_1}{x_{g1_i}},\Theta^1_2=\sum_{i=1}^{n_2}{x_{g2_i}},$$</p>
<p>$$\Theta^2=|\overline x_{g1}-\overline x_{g2}|$$</p>
<p>with</p>
<p>$$p^{exact}=\frac{\sum_{i=1}^{\frac{n!}{n_1!\cdot n_2!}}{1}}{\frac{n!}{n_1!\cdot n_2!}};\theta_i\ge \Theta$$</p>
<p>where</p>
<p><math display="inline"><msubsup><mi mathvariant="normal">&Theta;</mi><mn>1</mn><mn>1</mn></msubsup><mo>,</mo><msubsup><mi mathvariant="normal">&Theta;</mi><mn>2</mn><mn>1</mn></msubsup>
</math> = one-tailed test values<br />
<math display="inline"><msup><mi mathvariant="normal">&Theta;</mi><mn>2</mn></msup>
</math> = two-tailed test value<br />
<math display="inline"><mi>n</mi><mo>=</mo><msub><mi>n</mi><mn>1</mn></msub><mo>+</mo><msub><mi>n</mi><mn>2</mn></msub>
</math></p>
<br>
<h4 id="mpu_-randomized-permutation-test-mp-for-2-independent-samples-xg">[<code>mPU_</code>] Randomized permutation test <em>mP</em> for 2 independent samples <math display=„inline“><mo stretchy=„false“>(</mo><mi>x</mi><mrow data-mjx-texclass=„ORD“><mo stretchy=„false“>|</mo></mrow><mi>g</mi><mo stretchy=„false“>)</mo>
</math></h4>
<hr />
<p>Random sampling model, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>p</mi></math>-value not randomized.</p>
<p>$$\Theta^1_1=\sum_{i=1}^{n_1}{x_{g1_i}},\Theta^1_2=\sum_{i=1}^{n_2}{x_{g2_i}},$$</p>
<p>$$\Theta^2=|\overline x_{g1}-\overline x_{g2}|$$</p>
<p>with</p>
<p>$$p=\frac{\sum_{i=1}^M{1}}{M};\theta_i\ge \Theta$$</p>
<p>where</p>
<p><math display="inline"><msubsup><mi mathvariant="normal">&Theta;</mi><mn>1</mn><mn>1</mn></msubsup><mo>,</mo><msubsup><mi mathvariant="normal">&Theta;</mi><mn>2</mn><mn>1</mn></msubsup>
</math> = one-tailed test values<br />
<math display="inline"><msup><mi mathvariant="normal">&Theta;</mi><mn>2</mn></msup>
</math> = two-tailed test value<br />
<math display="inline"><mi>n</mi><mo>=</mo><msub><mi>n</mi><mn>1</mn></msub><mo>+</mo><msub><mi>n</mi><mn>2</mn></msub>
</math><br />
<math display=„inline“> <mi>M</mi> </math> = simulation cycles over permutations   <math display="inline"><msup><mi></mi><mi>w</mi></msup><msubsup><mi>P</mi><mi>n</mi><mrow data-mjx-texclass="ORD"><mo stretchy="false">(</mo><msub><mi>k</mi><mi>m</mi></msub><mo>,</mo><msub><mi>k</mi><mrow data-mjx-texclass="ORD"><mi>n</mi><mo>&minus;</mo><mi>m</mi></mrow></msub><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mfrac><mrow><mi>n</mi><mo>!</mo></mrow><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>!</mo><mo>&middot;</mo><msub><mi>n</mi><mn>2</mn></msub><mo>!</mo></mrow></mfrac>
</math></p>
<br>
<h4 id="btu_-bootstrap-test-bt-for-2-independent-samples-xg">[<code>BtU_</code>] Bootstrap test <em>Bt</em> for 2 independent samples <math display=„inline“><mo stretchy=„false“>(</mo><mi>x</mi><mrow data-mjx-texclass=„ORD“><mo stretchy=„false“>|</mo></mrow><mi>g</mi><mo stretchy=„false“>)</mo>
</math></h4>
<hr />
<p>Quenouille (<a href="https://doi.org/10.1111/j.2517-6161.1949.tb00023.x">1949</a>), Efron (<a href="https://doi.org/10.1214/aos/1176344552">1979</a>, <a href="https://doi.org/10.1093/biomet/68.3.589">1981</a>, <a href="https://doi.org/10.1137/1.9781611970319">1982</a>).</p>
<p>$$\Theta^1_1=\sum_{i=1}^{n_1}{x_{g1_i}},\Theta^1_2=\sum_{i=1}^{n_2}{x_{g2_i}},$$</p>
<p>$$\Theta^2=|\overline x_{g1}-\overline x_{g2}|$$</p>
<p>with</p>
<p>$$p=\frac{\sum_{i=1}^B{1}}{B};\theta_i\ge \Theta$$</p>
<p><math display="inline"><msubsup><mi mathvariant="normal">&Theta;</mi><mn>1</mn><mn>1</mn></msubsup><mo>,</mo><msubsup><mi mathvariant="normal">&Theta;</mi><mn>2</mn><mn>1</mn></msubsup>
</math> = one-tailed test values<br />
<math display="inline"><msup><mi mathvariant="normal">&Theta;</mi><mn>2</mn></msup>
</math> = two-tailed test value<br />
<math display="inline"><mi>n</mi><mo>=</mo><msub><mi>n</mi><mn>1</mn></msub><mo>+</mo><msub><mi>n</mi><mn>2</mn></msub>
</math><br />
<math display=„inline“> <mi>B</mi> </math> = simulation cycles over variations <math display="inline"><msup><mi></mi><mi>w</mi></msup><msubsup><mi>V</mi><mi>n</mi><mi>m</mi></msubsup><mo>=</mo><msup><mi>n</mi><mi>n</mi></msup>
</math></p>
<br>
<br>
<h3 id="complex-plane">Complex plane</h3>
<p>It was the Italian mathematician Gerolamo Cardano (<a href="https://archive.org/details/arsmagnaorruleso0000card">1545a</a>, <a href="https://web.archive.org/web/20220201093634/http://www.filosofia.unimi.it/cardano/testi/operaomnia/vol_4_s_4.pdf">b</a>) who <em>first</em> conceived the term <em>imaginary</em>, for the further historical development of <em>complex numbers</em> s. René Descartes (<a href="https://books.google.com/books?id=VtFcAAAAcAAJ">1664</a>, <a href="https://books.google.com/books?id=MB7F32p0y5MC">2012</a>, res.), Gauss (<a href="https://doi.org/10.3931/e-rara-61066">1828</a>, <a href="https://doi.org/10.3931/e-rara-61067">1832</a>) also Wirtinger (<a href="https://doi.org/10.1007/BF01447872">1927</a>).</p>
<p>At this point, one should recall the <em>definitional</em> importance of <em>geometry</em> and <em>trigonometry</em> in context with the calculation of <em>complex numbers</em> itself, where <math display="inline"><mo stretchy="false">|</mo><mi>z</mi><mo stretchy="false">|</mo>
</math> is <em>calculated</em> according to Pythagoras (c.f. Ratdolt, <a href="https://catalog.lindahall.org/discovery/delivery/01LINDAHALL_INST:LHL/1286816310005961">1482</a>, propositio 46) by <math display="inline"><mo stretchy="false">|</mo><mi>z</mi><mrow data-mjx-texclass="ORD"><mo stretchy="false">|</mo></mrow><mo>=</mo><mi>r</mi><mo>=</mo><msqrt><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><msup><mi>y</mi><mn>2</mn></msup></msqrt></math>.</p>
<p>After the fundamental change in mathematics from <em>geometric</em> to <em>algebraic</em> representation took place in the 16th century (c.f. Heath, <a href="https://archive.org/details/thirteenbookseu02heibgoog">1908a</a>, <a href="https://archive.org/details/thirteenbookseu00heibgoog">b</a>, <a href="https://archive.org/details/thirteenbookseu03heibgoog">c</a>; Bochner, <a href="https://hdl.handle.net/1911/63315">1978</a>; Anglin & Lambek, <a href="https://doi.org/10.1007/978-1-4612-0803-7_25">1995</a>; Malet, <a href="https://doi.org/10.1016/j.hm.2004.11.011">2006</a> or Alten et al., <a href="https://doi.org/10.1007/978-3-642-38239-0_4">2014</a>), the origins of <em>trigonometric series</em> of <em>tangents</em> and <em>sine</em> can be seen following early attempts (s. Jyesthadeva, <a href="https://archive.org/details/raswhishNA-124">1530</a>; Whish, <a href="https://doi.org/10.1017/S0950473700001221">1834</a>; Gupta, <a href="https://doi.org/10.1016/0315-0860(74)90067-6">1974</a> or Divakaran, <a href="http://www.jstor.org/stable/23497280">2007</a>) during the European <em>reinvention</em> in the works of Gregory (<a href="https://archivesearch.lib.cam.ac.uk/repositories/2/archival_objects/566767">1671</a>, <a href="https://books.google.com/books?id=ZtRYqgyD5YsC">1668a</a>, <a href="https://archive.org/details/gregory_universalis">b</a>), Leibniz (<a href="https://books.google.com/books/about/Acta_eruditorum.html?id=E7MasYIsMKQC">1682</a>, <a href="https://doi.org/10.26015/adwdocs-1924">2012</a>), Newton (<a href="https://www.newtonproject.ox.ac.uk/view/texts/normalized/NATP00204">1669</a>, <a href="https://doi.org/10.3931/e-rara-8934">1711</a>) and Brook Taylor (<a href="https://books.google.com/books?id=iXN1xgEACAAJ">1715</a>, <a href="https://books.google.com/books?id=r-Gq9YyZYXYC">1717</a>) with the definition of the <em>Taylor series</em> of <em>sine</em>, where <math display="inline"><mo>sin</mo><mi>x</mi><mo>=</mo><munderover><mo data-mjx-texclass="OP">&Sigma;</mo><mrow data-mjx-texclass="ORD"><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant="normal">&infin;</mi></munderover><mfrac><mrow><mo stretchy="false">(</mo><mo>-</mo><mn>1</mn><msup><mo stretchy="false">)</mo><mi>n</mi></msup></mrow><mrow><mo stretchy="false">(</mo><mn>2</mn><mo>&middot;</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>!</mo></mrow></mfrac><mo>&middot;</mo><msup><mi>x</mi><mrow data-mjx-texclass="ORD"><mn>2</mn><mo>&middot;</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msup></math> (c.f. Gregory & Collins, <a href="https://books.google.com/books?id=_eruAAAAMAAJ">1939</a>; Boyer, <a href="https://archive.org/details/ahistoryofmathematicscarlbboyer1968_315_t">1968</a>, p. 422 ff.; Feigenbaum, <a href="http://www.jstor.org/stable/41133765">1985</a>).</p>
<p>Finally, Euler (<a href="https://scholarlycommons.pacific.edu/euler-works/101/">1748a</a>, <a href="https://scholarlycommons.pacific.edu/euler-works/102/">b</a>) established the <em>analytic</em> treatment of <em>trigonometric</em> functions, defining them in relation with <em>complex exponential</em> functions by <math display="inline"><msup><mi>e</mi><mrow data-mjx-texclass="ORD"><mi>i</mi><mi>x</mi></mrow></msup><mo>=</mo><mo>cos</mo><mi>x</mi><mo>+</mo><mi>i</mi><mo>sin</mo><mi>x</mi></math>, where <math display="inline"><mo>e</mo><mo>=</mo><munderover><mo data-mjx-texclass="OP">&Sigma;</mo><mrow data-mjx-texclass="ORD"><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant="normal">&infin;</mi></munderover><mfrac><mn>1</mn><mrow><mi>n</mi><mo>!</mo></mrow></mfrac></math>
and thus laid the foundation of <em>modern</em> mathematical analysis (c.f. Finkel, <a href="http://www.jstor.org/stable/2968971">1897</a>; Walter, <a href="http://www.jstor.org/stable/2320218">1982</a>; Koyama & Kurokawa, <a href="http://www.jstor.org/stable/4097775">2005</a>; Calinger, <a href="http://www.jstor.org/stable/j.ctv7h0smb">2016</a> and Schrausser, <a href="https://doi.org/10.5281/zenodo.11356370">2024b</a>).</p>
<br>
<h4 id="cplxcplx2-f02z-geometric-representation-of-complex-numbers-z-in-the-complex-plane">[<code>CPLX</code>|<code>CPLX2</code>] [<code>F02Z</code>] Geometric representation of complex numbers <math display="inline">
<mi>z</mi>
</math> in the complex plane</h4>
<hr />
<p>Argand diagram (s. Argand, <a href="https://fr.wikisource.org/wiki/Annales_de_math%C3%A9matiques_pures_et_appliqu%C3%A9es/Tome_04/Philosophie_math%C3%A9matique,_article_4">1813</a>, <a href="http://catalogue.bnf.fr/ark:/12148/cb300261909">1874</a>, res.).</p>
<p>$$z=\Re+\Im=x+iy,$$</p>
<p>$$|z|=r=\sqrt{x^2+y^2}$$</p>
<p>with</p>
<p>$$\varphi=\pi;x\lt0\wedge y=0,$$</p>
<p>$$\varphi=2\cdot\arctan{\frac{y}{\sqrt{x^2+y^2}+x}};y\ne0,$$</p>
<p>$$\varphi=0;x\gt0\wedge y=0$$</p>
<p>where</p>
<p><math display=„inline“> <mi>r</mi> </math> = amplitude<br />
<math display="inline">
<mi>&phi;</mi>
</math> = phase, argument</p>
<br>
<h4 id="cplhx-graph-of-the-complex-function-zreim">[<code>CPLHX</code>] Graph of the complex function <math display=„inline“><mi>z</mi><mo>=</mo><mi mathvariant=„normal“>&Re;</mi><mo>+</mo><mi mathvariant=„normal“>&Im;</mi></math></h4>
<hr />
<p>$$y_1=\Re \cdot x,$$</p>
<p>$$y_2=\Im,$$</p>
<p>$$y_3=\sqrt{(\Re\cdot x)^2+\Im^2},$$</p>
<p>$$y_4=\frac{\Im}{|\Im|}\cdot\frac{\pi}{2}-\arctan({\frac{\Re}{\Im}\cdot x})$$</p>
<br>
<br>
<p><strong>Table 2.</strong> Methods and the corresponding HP Prime (<a href="https://www.hpcalc.org/details/7445">2017</a>) <em>CAS</em>, <em>User</em> and <em>Application</em> functions (Schrausser, <a href="https://doi.org/10.5281/zenodo.14721085">2025</a>) compared to equivalent Schrausser-MAT functions <em>MAT</em> (Schrausser, <a href="https://www.academia.edu/81395688">2022a</a>).</p>
<pre><code>method		HP-Prime function			
		CAS	User	App.	MAT

Correlation	IC_M	pRW	E01	DET
		KOR	pRWx		KOR
			RED		KOX
			rxy		RED
			tr		TRW
		TRW		
		RHO			
		TAU			
		DELTA2			
			rpbis		KPB
			prbis		KBS
			rbis		SBS
			srbis		BSK
			zrbis		ZBS
			prbisR		KBR
			rbisR		ZBR
			U_1		
			U_2		
			zrbisR		
		PHC	PHI		KPH
			pPHI		KPM
			xPHI		XKP
		PHC	prtet		KTET
			rtet		STET
			srtet		ZTET
		PKR	prxy_z		PKR
			rxy_z		ZKR
			ry_xz		
			zrxy_z		
		ZCor	rZ		FZR
			Zr		RFZ
					SFZ
			prr		ZRR
			Zrr		
		mZ	mr		GFZ
			mZ		
		MCORR2	Cf2		MKR
			FMCORR		SKM
			MCORR		BMK
			pMCORR		MBC
			SCR		FMK
Exposure		AEv	E03	
			Ev	E02	
			TEv		
			AvTv		
			AvS	E03	
			AvTvk		
			AvSk		
Integration			F01	
				F05	
				F01Z	
				F01Z	GAMMA
				F04	
Distribution	Q01_	zVAL		Z__
			zVALp		Z_P
			ZWERT		Z_W
			npz		
		AMG			AMG
		AMG			GM_
		GM_			
		Q01_			
		AMG			HM_
		HM_			
		Q01_			
		Q01_			SDV
		MDN			D__
		NVTLG		E01	DZW
				F02	PZD
				F03
		NVXY		F01Z	
		F06_		F02	DTW
		tVTLG		F03Z	PTD
				F06	
		ch2VTLG		F02	DXW
		F07_		F03Z	PXD
				F07	
		FVTLG		F02	DFW
				F03Z	PFD
		Q02_			SCH
					SHP
					SSH
					ZSH
		Q02_			EXZ
					EZP
					SEX
					ZEX
		SMG	CIx		SMG
					SMX
		CIXY	CIr	E01	
		EPSILON	EFG	E01	EFG
			EFR		
		EPSILON2		EFS
		EPSILON2		OPP
		TKV	pTKV		TKV
			tTKV		
		TV_	pTV		TV_
			tTV		
		TU_	pTU_		TU_
			pTUx		TUX
			tTU_		
			tTUx		
		TT_	pTT_		TT_
			tTT_		
		ABT1	p2F		EDX
			pzBN		ZBN
			x2F		
			zBN		
		VFCH	p4F		VFX
			p4FY		VFYX
			pz4F		ZFX
			x4F		
			x4FY		
			z4F		
		VFCH	pMN		MNX
			pMNY		MNYX
			xMN		
			xMNY		
Probability		ch		
			chA		
			chB		
		ABT1	ADDP	E01	AWN
		GMVTLG			GMP
					GMW
		NBNMVTLGNBINOM	E01	NBP
					NBW
		ABT1	BINOM	E01	BN0
			pzBN		BN1
			zBN		BN2
		FX_	pz4F		FX0
			z4F		FX1
					FX2
Combinatorics	PRM2			P_M
					PM_
		PRM3	nk		;
					P2M
					PMM
					PMW
		PRM4			VRW
					VWM
		PRM5			
Resampling	PV_			
		mPV_			
		PU_			
		mPU_			
		BtU_			
Complex plane	CPLX		F02Z	
		CPLX2			
		CPLHX			
</code></pre>
<br>
<br>
<h2 id="source-codes">Source codes</h2>
<br>
<h3 id="cas-functions">CAS functions</h3>
<br>
<h4 id="a">A</h4>
<h5 id="abt1pas">ABT1.pas</h5>
<pre><code>//ABT1()/D.G.SCHRAUSSER/2022
//Binomialp[A1:bA2:c]/Addp[C1:pC2:nC3:k]
#cas
ABT1():=
BEGIN
STARTAPP(&quot;Arbeitsblatt&quot;);
STARTVIEW(1)
5▶A1;
5▶A2;
=A1+A2▶A3;
0.5▶B1;
=BINOMIAL_CDF(A3,B1,A1)▶B4
0.5▶C1;
8▶C2;
1▶C5;
=1-(1-C1)^C2▶D4;
=Σ((C5+I-1)!/(I!*(C5-1)!)*C1^C5*(1-C1)^I,I,0,C2-C5)▶D6;
END;
#end 
//
</code></pre>
<h5 id="abt2pas">ABT2.pas</h5>
<pre><code>//ABT2(cell count a,b,c,d)/D.G.SCHRAUSSER/2025
//2×2 chi-squared test for independence
//Observed frequencies abcd fb
//Expected frequencies fe
//Probabilities p(A^B), p(B|A), p(A|B)
//Chi-squared with 2-tailed p
//e.g.ABT2(17,12,14,24)[Spreadsheet]
#cas
ABT2(a,b,c,d):=
BEGIN
STARTAPP(&quot;Arbeitsblatt&quot;);
STARTVIEW(1)
&quot;A1&quot;▶B1
&quot;A2&quot;▶C1
&quot;A1&quot;▶E1
&quot;A2&quot;▶F1
&quot;B1&quot;▶A2
&quot;B2&quot;▶A3
&quot;B1&quot;▶A5
&quot;B2&quot;▶A6
&quot;B1&quot;▶A8
&quot;B2&quot;▶A9
&quot;B1&quot;▶A11
&quot;B2&quot;▶A12
a▶B2
b▶C2
c▶B3
d▶C3
D2:==B2+C2
D3:==B3+C3
B4:==B2+B3
C4:==C2+C3
D4:==B2+C2+B3+C3
E5:==B2/D4
F5:==C2/D4
g5:==D2/D4
E6:==B3/D4
F6:==C3/D4
g6:==D3/D4
E7:==B4/D4
F7:==C4/D4
g7:==g5+g6
E2:==B2/D2
F2:==C2/D2
g2:==E2+F2
E3:==B3/D3
F3:==C3/D3
g3:==E3+F3
B5:==B2/B4
B6:==B3/B4
B7:==B5+B6
C5:==C2/C4
C6:==C3/C4
C7:==C5+C6
B8:==E7*D2
B9:==E7*D3
C8:==F7*D2
C9:==F7*D3
D13:==D4*(B2*C3-C2*B3)^2/((B2+C2)*(B3+C3)*(B2+B3)*(C2+C3))
B11:==(B2-B8)^2/B8
B12:==(B3-B9)^2/B9
C11:==(C2-C8)^2/C8
C12:==(C3-C9)^2/C9
D11:==((B2-B8)^2/B8)+(C2-C8)^2/C8
D12:==((B3-B9)^2/B9)+(C3-C9)^2/C9
B13:==B11+B12
C13:==C11+C12
g11:==1-(CHISQUARE_CDF(1,D11))
g12:==1-(CHISQUARE_CDF(1,D12))
g13:==1-(CHISQUARE_CDF(1,D13))
E11:==1-(CHISQUARE_CDF(1,B11))
E12:==1-(CHISQUARE_CDF(1,B12))
E13:==1-(CHISQUARE_CDF(1,B13))
F11:==1-(CHISQUARE_CDF(1,C11))
F12:==1-(CHISQUARE_CDF(1,C12))
F13:==1-(CHISQUARE_CDF(1,C13))
END;
#end 
//
</code></pre>
<h5 id="amgpas">AMG.pas</h5>
<pre><code>//AMG()/D.G.SCHRAUSSER/2025
//Weighted arithmetic, geometric and harmonic mean
#cas
AMG():=
BEGIN
//L1()(2) provided
size(L1)▶L3(1)
mean(L1)(1)▶L3(2)
MAKELIST(L1(x)(1)*L1(x)(2),x,1,L3(1))▶L2
ΣLIST(L1)(2)▶L3(3)
Σ(L2)/L3(3)▶L3(4)
L3(3) NTHROOT (product((L1(x)(1))^L1(x)(2),x,1,L3(1)))▶L3(5)
L3(3)/(Σ(L1(x)(2)/L1(x)(1),x,1,L3(1)))▶L3(6)
//n,AM,sumni,GAM,GGM,GHM
L3
END;
#end
//
</code></pre>
<h4 id="b">B</h4>
<h5 id="bnmvtlgpas">BNMVTLG.pas</h5>
<pre><code>//BNMVTLG(p[e],a=k,n)/D.G.SCHRAUSSER/2025
//e.g.BNMVTLG(0.5,5,10)
#cas
BNMVTLG(P,K,N):=
BEGIN
B=0;
FOR I FROM 0 TO K DO
 BINOMIAL(N,P,I)▶L4(I)
 B=B+L4(I)
END;
D5=L4;L4={}
FOR I FROM 0 TO N DO
 BINOMIAL(N,P,I)▶L5(I);
END;
D6=L5;L5={};
STARTAPP(&quot;Statistiken_1_Var&quot;);
STARTVIEW(1);
&quot;D5&quot;▶H1(1);5▶H1(3);
&quot;D6&quot;▶H2(1);5▶H2(3);
//p
RETURN(B);
END;
#end
//
</code></pre>
<h5 id="btu_pas">BtU_.pas</h5>
<pre><code>//BtU_(simulation cycles B)/D.G.SCHRAUSSER/2025
//Bootstrap method, Bt
//2 independent samples (x|g)
//e.g.BtU_(1000)
#cas
BtU_(B):=
BEGIN
//L1L2 provided
{}▶L3;{}▶L4
{}▶L5;{}▶L6
0▶M11
0▶M12
0▶M2
SIZE(L1)▶N1
SIZE(L2)▶N2
N=N1+N2
ABS(mean(L1)-mean(L2))▶Q02
ΣLIST(L1)▶Q011
ΣLIST(L2)▶Q012
//
CONCAT(L1,L2)▶L9
MSGBOX(&quot;BtU&quot;)
FOR J FROM 1 TO B 
DO
//
FOR A FROM 1 TO N DO
L9(RANDINT(N))▶L0(A) END;
FOR A FROM 1 TO  N1 DO
L0(A)▶L3(A) END;
FOR A FROM 1 TO  N2 DO
L0(N1+A)▶L4(A) END;
ABS(mean(L3)-mean(L4))▶QJ2
ΣLIST(L3)▶QJ11
ΣLIST(L4)▶QJ12
IF QJ11≥Q011 THEN M11=M11+1 END;
IF QJ12≥Q012 THEN M12=M12+1 END;
IF QJ2≥Q02 THEN M2=M2+1 END;
QJ11▶L5(J)
QJ2▶L6(J)
END;
//
SORT(L5)▶L5
SORT(L6)▶L6
{}▶L9
{}▶L0
N1,N2,[Q011,Q012,Q02],M11/B,M12/B,M2/B
END;
#end
//
</code></pre>
<h4 id="c">C</h4>
<h5 id="ch2vtlgpas">ch2VTLG.pas</h5>
<pre><code>//ch2VTLG(chi-squared,df)/D.G.SCHRAUSSER/2025
//e.g.ch2VTLG(2.65,1)[AdvancedGraphing]
#cas
ch2VTLG(C2569,A7485):=
BEGIN
G=Gamma(A7485/2)
//P=∫((1/(2^(A7485/2)*G))*X^((A7485/2)-1)*e^(-X/2),X,0,C)
P=CHISQUARE_CDF(A7485,C2569)
A7485▶A
C2569▶C
&quot;Y=(1/(2^(A/2)*G))*X^((A/2)-1)*e^(-X/2)&quot;▶V1
&quot;Y&lt;(1/(2^(A/2)*G))*X^((A/2)-1)*e^(-X/2) AND Y&gt;0 AND X&lt;C AND X&gt;0&quot;▶V2
STARTAPP(&quot;Erweiterte_Grafiken&quot;);
STARTVIEW(1);
[1-P]
END;
#end
//
</code></pre>
<h5 id="cixypas">CIXY.pas</h5>
<pre><code>//CIXY(x,y'CI)/D.G.SCHRAUSSER/2022  
//Standard error of prediction sy'x, CI  
//e.g.CIXY(3,0.99[ZWERT,Statistics_2_Var,Spreadsheet,AdvancedGraphing]  
#cas  
CIXY(X,C):=  
BEGIN  
//C1C2 provided  
STARTAPP(&quot;Statistiken_2_Var&quot;);  
STARTVIEW(−6)  
A=Corr  
B=sY  
D=MeanY  
PredY(X)▶L3(2);  
√(1-A^2)*B*NORMALD_ICDF(1-((1-C)/2))▶L3(4)  
L3(4)+L3(2)▶L3(3)  
L3(2)-L3(4)▶L3(1)  
C▶L3(5)  
ZWERT(L3(1),D,B)▶L4(1)  
ZWERT(L3(2),D,B)▶L4(2)  
ZWERT(L3(3),D,B)▶L4(3)  
L4(1)▶U  
L4(3)▶O  
ZWERT(X,MeanX,sX)▶Q  
STARTAPP(&quot;Arbeitsblatt&quot;);  
&quot;ŷ-&quot;▶A1;L3(1)▶B1;L4(1)▶C1  
&quot;ŷ&quot;▶A2;L3(2)▶B2;L4(2)▶C2  
&quot;ŷ+&quot;▶A3;L3(3)▶B3;L4(3)▶C3  
&quot;±&quot;▶A4;L3(4)▶B4;L3(4)/B▶C4  
&quot;CI&quot;▶A5;L3(5)▶B5;  
STARTAPP(&quot;Erweiterte_Grafiken&quot;);  
STARTVIEW(1)  
&quot;Y=A*X&quot;▶V3  
&quot;Y&gt;0 AND (Y&lt;A^(-1)*X AND Y&gt;A*X) OR Y&lt;0 AND   (Y&gt;A^(-1)*X AND Y&lt;A*X)&quot;▶V4  
&quot;Y=√(1-X^2)&quot;▶V5  
&quot;Y=-1*√(1-X^2)&quot;▶V6  
&quot;X&lt;A AND X&gt;0 AND Y&lt;A AND Y&gt;0&quot;▶V7  
CAS((X,Y)-&gt;((Y&lt;O) AND (Y&gt;U)) AND ((X==Q))▶V0)  
//y'-,y',y'+,CI,sy'x,CIp  
RETURN(L3);  
END;  
#end  
//  
</code></pre>
<h5 id="cplhxpas">CPLHX.pas</h5>
<pre><code>//CPLHX(complex number,a+bi)/D.G.SCHRAUSSER/2022  
//e.g.CPLHX(2+i/2),[AdvancedGraphing]  
#cas  
CPLHX(C):=  
BEGIN  
C▶Z1  
RE(Z1)▶R  
IM(Z1)▶I  
Z1▶L1(1)  
ABS(Z1)▶L1(2)  
ARG(Z1)▶L1(3)  
&quot;Y=R*X&quot;▶V1  
&quot;Y=I&quot;▶V2  
&quot;Y=√((R*X)^2+I^2)&quot;▶V3  
&quot;Y=(I/ABS(I))*(π/2)-ATAN((R/I)*X)&quot;▶V4  
STARTAPP(&quot;Erweiterte_Grafiken&quot;)  
STARTVIEW(1)  
RETURN(L1);  
END;  
#end  
//  
</code></pre>
<h5 id="cplxpas">CPLX.pas</h5>
<pre><code>//CPLX(complex number,a+bi)/D.G.SCHRAUSSER/2022  
//e.g.CPLX(2+i/2),[AdvancedGraphing]  
#cas  
CPLX(C):=  
BEGIN  
C▶Z1  
RE(Z1)▶R  
IM(Z1)▶I  
ABS(Z1)▶L1(1)  
R▶X▶J  
I▶K  
&quot;Y=√(1-X^2)&quot;▶V5  
&quot;Y=-1*√(1-X^2)&quot;▶V6  
&quot;Y=I&quot;▶V7  
&quot;X=R&quot;▶V8  
&quot;X&lt;R AND X&gt;0 AND Y&gt;0 AND Y≤0.01&quot;▶V0  
&quot;Y=(I/R)*X AND Y&gt;0 AND X&lt;R&quot;▶V9  
IF I&lt;0 THEN  
&quot;Y=(I/R)*X AND Y&lt;0 AND X&lt;R&quot;▶V9  
END;  
IF R&lt;0 THEN  
&quot;Y=(I/R)*X AND Y&gt;0 AND X&gt;R&quot;▶V9 
END;  
IF I&lt;0 AND R&lt;0 THEN  
&quot;Y=(I/R)*X AND Y&lt;0 AND X&gt;R&quot;▶V9  
END;  
ARG(Z1)▶L1(2)  
CONVERT(L1(2)_rad,1_deg)▶L1(3)  
RETURN(L1);  
END;  
#end  
//  
</code></pre>
<h5 id="cplxpas-1">CPLX.pas</h5>
<pre><code>//CPLX2(complex number,a+bi)/D.G.SCHRAUSSER/2022  
//e.g.CPLX2(2+i/2),[CPLX,Spreadsheet]//  
#cas  
CPLX2(C):=  
BEGIN  
CPLX(C)  
STARTAPP(&quot;Arbeitsblatt&quot;);  
&quot;z&quot;▶A1;Z1▶B1  
&quot;|z|&quot;▶A2;L1(1)▶B2  
&quot;∡π&quot;▶A3;L1(2)▶B3  
&quot;∡°&quot;▶A4;L1(3)▶B4  
END;  
#end  
//  
</code></pre>
<h4 id="d">D</h4>
<h5 id="delta2pas">DELTA2.pas</h5>
<pre><code>//DELTA2()/D.G.SCHRAUSSER/2025  
//Somers' D for binary values [0,1]  
#cas  
DELTA2():=  
BEGIN  
SIZE(L1)▶N  
{}▶L3  
0▶X01  
0▶X02  
FOR I FROM 1 TO N DO  
IF L1(I)=1 AND L2(I)=1 THEN  
X01=X01+1  
END;  
IF L1(I)=1 AND L2(I)=0 THEN  
X02=X02+1  
END;  
END;  
X01/N▶L3(1)  
X02/N▶L3(2)  
L3(1)-L3(2)▶L3(3)  
//pA,pB,D  
approx(L3)  
END;  
#end  
//  
</code></pre>
<h4 id="e">E</h4>
<h5 id="epsilonpas">EPSILON.pas</h5>
<pre><code>//EPSILON(x1,m1,m2,s12,d)/D.G.SCHRAUSSER/2022
//e.g.EPSILON(106,100,110,15,25)
#cas
EPSILON(X,M,N,S,D):=
BEGIN
G=Gamma((D+1)/2)/Gamma(D/2)
E=(N-M)/S
P=1-∫(G*(D*π)^(-1/2)*(1+(X^2/D))^(-(D+1)/2),X,−∞,E)
T=(((N+M)/2)-M)/S
H=1-∫(G*(D*π)^(-1/2)*(1+(X^2/D))^(-(D+1)/2),X,−∞,T)
Q=(X-M)/S
R=1-∫(G*(D*π)^(-1/2)*(1+(X^2/D))^(-(D+1)/2),X,−∞,Q)
U=∫(G*(D*π)^(-1/2)*(1+(X^2/D))^(-(D+1)/2),X,−∞,(X-N)/S)
B=1-U
&quot;X&gt;T AND X≤T&quot;▶V1
&quot;X&gt;Q AND X≤Q&quot;▶V2
//
D▶K
&quot;X&gt;0 AND X&lt;E AND Y&lt;0 AND Y&gt;-0.01&quot;▶V3
&quot;Y&lt;(G*(K*π)^(-1/2)*(1+((E-X)^2/K))^(-(K+1)/2)) AND Y&gt;0 AND X&gt;Q&quot;▶V6
&quot;Y&lt;(G*(K*π)^(-1/2)*(1+((E-X)^2/K))^(-(K+1)/2)) AND Y&gt;0 AND X&lt;Q&quot;▶V7
&quot;Y=(G*(K*π)^(-1/2)*(1+((E-X)^2/K))^(-(K+1)/2))&quot;▶V8
&quot;Y=(G*(K*π)^(-1/2)*(1+((X)^2/K))^(-(K+1)/2))&quot;▶V0
&quot;Y&lt;(G*(K*π)^(-1/2)*(1+((X)^2/K))^(-(K+1)/2)) AND Y&gt;0 AND X&gt;Q&quot;▶V9
E▶L2(1);P▶L3(1);N▶L1(1)
T▶L2(2);H▶L3(2);T*S+M▶L1(2)
Q▶L2(3);R▶L3(3);Q*S+M▶L1(3)
U▶L4(3);B▶L5(3)
STARTAPP(&quot;Arbeitsblatt&quot;);
&quot;ε&quot;▶A1;L2(3)▶B1;L2(2)▶C1;L2(1)▶D1;
&quot;x&quot;▶A2;L1(3)▶B2;L1(2)▶C2;L1(1)▶D2;
&quot;α&quot;▶A3;L3(3)▶B3;L3(2)▶C3;L3(1)▶D3;
&quot;β&quot;▶A4;L4(3)▶B4;
&quot;1-β&quot;▶A5;L5(3)▶B5;
STARTAPP(&quot;Erweiterte_Grafiken&quot;)
STARTVIEW(1)
RETURN(L2(1),L3(3),L4(3));
END;
#end
//
</code></pre>
<h5 id="epsilon2pas">EPSILON2.pas</h5>
<pre><code>//EPSILON2(epsilon,n,df,pcrit)/D.G.SCHRAUSSER/2022
//e.g.EPSILON2(0.38,100,99,0.95)
//optimal effect size epsilon
//optimal alpha t
//1-p(alpha opt t)
#cas
EPSILON2(E,N,D,K):=
BEGIN
#t opt niv
V=√(E^2*N)/2▶L6(2)
P=1-STUDENT_CDF(D,V)▶L6(3)
#e opt eff stke
L=√((2*STUDENT_ICDF(D,K))^2/N)▶L6(1)
&quot;X&gt;0 AND X&lt;L AND Y&lt;0 AND Y&gt;-0.02&quot;▶V1
RETURN(L6);
END;
#end
//
</code></pre>
<h5 id="epsolon3pas">EPSOLON3.pas</h5>
<pre><code>//EPSILON3(100,110,15,25,0.99)
#cas
EPSILON3(M,N,S,D,K):=
BEGIN
F=STUDENT_ICDF(D,K)
M+S*F▶L7(1)
N-S*F▶L7(2)
&quot;X&gt;E-F AND X≤E-F AND Y&gt;0 AND Y&lt;(G*(K*π)^(-1/2)*(1+((E-X)^2/K))^(-(K+1)/2))&quot;▶V4
&quot;X&gt;F AND X≤F AND Y&gt;0 AND Y&lt;(G*(K*π)^(-1/2)*(1+((X)^2/K))^(-(K+1)/2))&quot;▶V5
RETURN(L7);
END;
#end 
//
</code></pre>
<h4 id="f">F</h4>
<h5 id="fvtlgpas">FVTLG.pas</h5>
<pre><code>//FVTLG(F,df1,df2)/D.G.SCHRAUSSER/2025
//e.g.FVTLG(2.8,10,5)
#cas
FVTLG(F,A,B):=
BEGIN
F▶X
CAS(Gamma((A+B)/2))▶H
CAS(Gamma(A/2))▶D
CAS(Gamma(B/2))▶E
CAS(H/(D*E))▶C
CAS((X,Y)-&gt;Y=C*((A/B)^(A/2)*X^((A/2)-1)*(1+(A/B)*X)^(−(((A+B)/2)))) AND X&gt;0▶V2)
CAS((X,Y)-&gt;Y&lt;C*((A/B)^(A/2)*X^((A/2)-1)*(1+(A/B)*X)^(−(((A+B)/2)))) AND Y&gt;0 AND X&lt;F AND X&gt;0▶V1)
FISHER_CDF(A,B,X)▶P
STARTAPP(&quot;Erweiterte_Grafiken&quot;)
STARTVIEW(1)
P,[1-P]
END;
#end 
//
</code></pre>
<h5 id="fxpas">FX.pas</h5>
<pre><code>//FX_(cell count a,b,c,d)/D.G.SCHRAUSSER/2025
//e.g.FX_(1,2,3,1)
//Exact hypergeometric 4-field test according to R. A. Fisher
//(Fisher Exact Test): Hypergeometric probability p to cell a of the 4-field initial arrangement for all possible arrangements a
//Exact significance levels p[exact1], p[exact2]
#cas
FX_(a,b,c,d):=
BEGIN
{}▶L1
1▶S
0▶X
0▶P20
0▶P21
0▶P3
a+b+c+d▶N
a+b▶z1
c+d▶z2
a+c▶s1
b+d▶s2
P0= (z1!*z2!*s1!*s2!)/(N!*a!*b!*c!*d!);
P0▶L2(1)
PRINT(&quot;P0-&quot;)
PRINT(P0)
PRINT(&quot;Pi-&quot;)
//
IF z1&gt;s1 THEN max1=z1 ELSE max1=s1 END;
IF z1&gt;s2 THEN max2=z1 ELSE max2=s2 END;
IF z2&gt;s1 THEN max3=z2 ELSE max3=s1 END;
IF z2&gt;s2 THEN max4=z2 ELSE max4=s2 END;
//
FOR I FROM 0 TO max1 DO
FOR J FROM 0 TO max2 DO
FOR K FROM 0 TO max3 DO
FOR L FROM 0 TO max4 DO
a1=I+J;
a2=K+L; 
b1=I+K;
b2=J+L;
//
IF  
a1=z1 AND 
a2=z2 AND 
b1=s1 AND
b2=s2
THEN
IF  
I+J≠0 AND
K+L≠0 AND
I+K≠0 AND
J+L≠0
THEN
P10=(a1!*a2!*b1!*b2!)/(N!*I!*J!*K!*L!)
X+1▶X
P3=P10+P3
approx(P10)▶L1(X)
IF approx(P10)&lt;approx(P0) OR approx(P10)=approx(P0)
THEN
P20+P10▶P20 END;
PRINT(P10)
END;
END;
//
END;
END;
END;
END;
PRINT(&quot;p--&quot;)
PRINT(P3)
//
FOR I FROM 1 TO X DO
IF L1(I)=L2(1) THEN
0▶S
END;
IF S=1 THEN
L1(I)+P21▶P21
END;
END;
P21+P0▶P21
P22=1-(P21)
//sums, P0, C, p[exact1], 1-p[exact1], p[exact2]
z1,z2,s1,s2,N,[P0],X,P21,P22,[P20]
END;
#end
//
</code></pre>
<h5 id="fx_pas">FX_.pas</h5>
<pre><code>//FX_(cell count a,b,c,d)/D.G.SCHRAUSSER/2025
//e.g.FX_(1,2,3,1)
//Exact hypergeometric 4-field test according to R. A. Fisher
//(Fisher Exact Test): Hypergeometric probability p to cell a of the 4-field initial arrangement for all possible arrangements a
//Exact significance levels p[exact1], p[exact2]
//(slow algorithm)
#cas
FX_(a,b,c,d):=
BEGIN
{}▶L1
1▶S
0▶X
0▶P20
0▶P21
0▶P3
a+b+c+d▶N
a+b▶z1
c+d▶z2
a+c▶s1
b+d▶s2
P0= (z1!*z2!*s1!*s2!)/(N!*a!*b!*c!*d!);
PRINT(&quot;P0-&quot;)
PRINT(P0)
PRINT(&quot;Pi-&quot;)
//
FOR I FROM 0 TO N DO
FOR J FROM 0 TO N DO
FOR K FROM 0 TO N DO
FOR L FROM 0 TO N DO
a1=I+J; 
a2=K+L; 
b1=I+K;
b2=J+L;
//
IF  
a1=z1 AND 
a2=z2 AND 
b1=s1 AND
b2=s2
THEN
IF  
I+J≠0 AND
K+L≠0 AND
I+K≠0 AND
J+L≠0
THEN
P10=(a1!*a2!*b1!*b2!)/(N!*I!*J!*K!*L!)
X+1▶X
P3=P10+P3
approx(P10)▶L1(X)
IF approx(P10)&lt;approx(P0) OR 
   approx(P10)=approx(P0)
THEN
P20+P10▶P20 END;
PRINT(P10)
END;
END;
//
END;
END;
END;
END;
PRINT(&quot;p--&quot;)
PRINT(P3)
//
FOR I FROM 1 TO X DO
IF L1(I)=P0 THEN
0▶S
END;
IF S=1 THEN
L1(I)+P21▶P21
END;
END;
P21+P0▶P21
P22=1-(P21)
// sums, P0, C, p[exact1], 1-p[exact1], p[exact2]
z1,z2,s1,s2,N,[P0],X,P21,P22,[P20]
END;
#end
//
</code></pre>
<h4 id="g">G</h4>
<h5 id="gmvtlgpas">GMVTLG.pas</h5>
<pre><code>//GMVTLG(pA,r+1=n)/D.G.SCHRAUSSER/2025
//e.g.GMVTLG(1/6,10)
#cas
GMVTLG(P,N):=
BEGIN
MAKELIST(P*(1-P)^x,x,0,N-1)▶L1
MAKELIST(1-(1-P)^x,x,1,N)▶L2
STARTAPP(&quot;Statistiken_1_Var&quot;);
STARTVIEW(1);
&quot;L1&quot;▶H1(1);6▶H1(3);
&quot;L2&quot;▶H2(1);5▶H2(3);
//P,p
L1(N),L2(N)
END;
#end
//
</code></pre>
<h4 id="i">I</h4>
<h5 id="ic_mpas">IC_M.pas</h5>
<pre><code>//IC_M(n of variables k)/D.G.SCHRAUSSER/2025  
//Intercorrelation matrix, Pearson correlation r  
//e.g.IC_M(5)[pCor]  
//M1:r  
//M2:det%(r²×100)  
//M3:t-value  
//M4:2-tailed p  
#cas  
IC_M(K):=  
BEGIN  
//L1(n)(k) provided  
{}▶L4  
{}▶L5  
{}▶L6  
{}▶L7  
{}▶L8  
size(L1)▶N  
FOR I FROM 1 TO K DO  
FOR J FROM 1 TO K DO  
MAKELIST({L1(X,I),L1(X,J)},X,1,N)▶L4  
approx(correlation(L4))▶R125▶L5(I,J) //r  
approx(pCor(R125,N)(3))▶L0;L0(1)▶L6(I,J) //p2  
approx(pCor(R125,N)(1))▶L0;L0(1)▶L7(I,J) //t  
approx(R125^2*100)▶L8(I,J) //det  
END;  
END;  
L5▶M1  
L8▶M2  
L7▶M3  
L6▶M4  
//  
END;  
#end  
//  
</code></pre>
<h5 id="k">K</h5>
<h5 id="korpas">KOR.pas</h5>
<pre><code>//KOR()/D.G.SCHRAUSSER/2025  
//Pearson corr/[pCor]  
#cas  
KOR():=  
BEGIN  
//L1()(2) provided  
SIZE(L1)(1)▶N  
N-2▶df  
covariance_correlation(L1)▶L0  
L0(2)▶r  
L0(1)▶cv  
pCor(r,N)(2)▶p  
pCor(r,N)(3)▶p2  
r^2*100▶D  
{}▶L0  
//  
df,[cv,r,D],p,[p2]  
END;  
#end  
//  
</code></pre>
<h4 id="m">M</h4>
<h5 id="mcorr2pas">MCORR2.pas</h5>
<pre><code>//MCORR2()/D.G.SCHRAUSSER/2022  
//Multiple correlation R  
//[Statistiken_2_Var,Arbeitsblatt,Graph3D,FMCORR,MCORR]  
#cas  
MCORR2():=  
BEGIN  
//M1()(3) provided  
//C1C2 to S1  
STARTAPP(&quot;Statistiken_2_Var&quot;);  
STARTVIEW(−6)  
M2=TRN(M1)  
L7=M2(1);C1=L7  
L7=M2(2);C2=L7  
Do2VStats(S1)  
Corr▶L1(1)  
MeanX▶L3(1);sX▶L4(1)  
MeanY▶L3(2);sY▶L4(2)  
//  
L7=M2(1);C1=L7  
L7=M2(3);C2=L7  
Do2VStats(S1)  
Corr▶L1(2)  
MeanY▶L3(3);sY▶L4(3)  
//  
L7=M2(2);C1=L7  
L7=M2(3);C2=L7  
Do2VStats(S1)  
Corr▶L1(3)  
//  
C=MCORR(L1(1),L1(2),L1(3))  
C▶L2(1)  
F=FMCORR(C,NbItem)  
F▶L2(2)  
1-FISHER_CDF(3,NbItem-3-1,F)▶L2(5);  
NbItem-3-1▶L2(4)  
3▶L2(3)  
M2=0;L7={};  
1▶M2(1,1);L1(3)▶M2(1,2)  
L1(3)▶M2(2,1);1▶M2(2,2)  
L1(1)▶M3(1,1)  
L1(2)▶M3(2,1)  
M4=M2^-1*M3  
M4(1,1)*(L4(1)/L4(2))▶L5(1)  
M4(2,1)*(L4(1)/L4(3))▶L5(2)  
L3(1)-(L5(1)*L3(2)+L5(2)*L3(3))▶L6(1)  
#y=b1x1+b2x2+a  
&quot;L5(1)*X+L5(2)*Y+L6(1)&quot;▶FZ1  
STARTAPP(&quot;Arbeitsblatt&quot;);  
&quot;R&quot;▶A1;L2(1)▶B1;&quot;r1c&quot;▶C1;L1(1)▶D1  
&quot;F&quot;▶A2;L2(2)▶B2;&quot;r2c&quot;▶C2;L1(2)▶D2  
&quot;r23&quot;▶C3;L1(3)▶D3  
&quot;df1&quot;▶A3;L2(3)▶B3;&quot;b1&quot;▶C4;M4(1,1)▶D4  
&quot;df2&quot;▶A4;L2(4)▶B4;&quot;b2&quot;▶C5;M4(2,1)▶D5  
&quot;p&quot;▶A5;1-L2(5)▶B5;&quot;bc1&quot;▶C6;L5(1)▶D6  
&quot;α₂&quot;▶A6;L2(5)▶B6;&quot;bc2&quot;▶C7;L5(2)▶D7  
&quot;ac&quot;▶C8;L6(1)▶D8  
RETURN (L2);  
END;  
#end  
//  
</code></pre>
<h5 id="mdnpas">MDN.pas</h5>
<pre><code>//MDN()/D.G.SCHRAUSSER/2025  
//Mean dispersion D  
#cas  
MDN():=  
BEGIN  
approx(mean(L1))▶L4(1)  
MAKELIST(approx(ABS(L1(x)-L4(1))),x,1,SIZE(L1))▶L2  
FOR I FROM 1 TO SIZE(L1) DO  
1▶L3(I)  
IF L1(I)=L4(1) THEN  
0▶L3(I)  
END;  
END;  
stddev(L2)▶L4(2)  
Σ(L2)/Σ(L3)▶L4(3)  
L4(2)*L4(3)*(SIZE(L1)/(SIZE(L1)-1))▶L4(4)  
0.5*√((L4(2)*(SIZE(L1)/(SIZE(L1)-1)))/(SIZE(L1)))▶L4(5)  
//AM,s,D,D',sD  
END;  
#end  
//  
</code></pre>
<h5 id="mpu_pas">mPU_.pas</h5>
<pre><code>//mPU_(simulation cycles M)/D.G.SCHRAUSSER/2025
//Permutation test in the random sampling model,
//randomized permutation, p-value not randomized, mP
//2 independent samples (x|g)
//(including intraclass permutation)
//e.g.mPU_(100)
#cas
mPU_(M):=
BEGIN
//L1L2 provided
//
{}▶L3;{}▶L4
{}▶L5;{}▶L6
0▶M11
0▶M12
0▶M2
SIZE(L1)▶N1
SIZE(L2)▶N2
N=N1+N2
//N vector L7
FOR I FROM 1 TO N DO
I▶L7(I) END;
//
ABS(mean(L1)-mean(L2))▶Q02
ΣLIST(L1)▶Q011
ΣLIST(L2)▶Q012
//
MSGBOX(&quot;mPU&quot;)
FOR J FROM 1 TO M 
DO
//
//prm vector L9
FOR A FROM 1 TO N DO
{RANDOM(),L7(A)}▶L0(A) END;
sort(L0)▶L9
FOR A FROM 1 TO N DO
L9(A)▶L8;L8(2)▶L9(A) END;
{}▶L8
CONCAT(L1,L2)▶L8
//
FOR A FROM 1 TO  N1 DO
L8(L9(A))▶L3(A) END;
FOR A FROM 1 TO  N2 DO
L8(L9(N1+A))▶L4(A) END;
ABS(mean(L3)-mean(L4))▶QJ2
ΣLIST(L3)▶QJ11
ΣLIST(L4)▶QJ12
IF QJ11≥Q011 THEN M11=M11+1 END;
IF QJ12≥Q012 THEN M12=M12+1 END;
IF QJ2≥Q02 THEN M2=M2+1 END;
//
QJ11▶L5(J)
QJ2▶L6(J)
//
END;
//
SORT(L5)▶L5
SORT(L6)▶L6
{}▶L7
{}▶L8
{}▶L9
{}▶L0
//
N1,N2,[Q011,Q012,Q02],M11/M,M12/M,M2/M
END;
#end
//
</code></pre>
<h5 id="mpu2pas">mPU2.pas</h5>
<pre><code>//mPU2(simulation cycles M)/D.G.SCHRAUSSER/2025
//Permutation test in the random sampling model,
//randomized permutation, p-value not randomized, mP
//2 independent samples (x|g)
//e.g.mPU2(100)
#cas
mPU2(M):=
BEGIN
//L1L2 provided
//
{}▶L3;{}▶L4
{}▶L5;{}▶L6
0▶M11
0▶M12
0▶M02
0▶X
0▶J
SIZE(L1)▶N1
SIZE(L2)▶N2
N=N1+N2
//
MAKELIST(x+1,x,0,N-1)▶L7
MAKELIST(0,x,0,1)▶L51
MAKELIST(x+1,x,0,N1-1)▶L71
//
ABS(mean(L1)-mean(L2))▶Q02
ΣLIST(L1)▶Q011
ΣLIST(L2)▶Q012
//
MSGBOX(&quot;mPU&quot;)
WHILE J&lt;M 
DO
//
FOR A FROM 1 TO N DO
{RANDOM(),L7(A)}▶L0(A) END;
//diff 
{}▶L9
SORT(L0)▶L31 //
FOR A FROM 1 TO N1 DO
L31(A)▶L51;L51(2)▶L6(A)
END;
sort(L0)▶L9
FOR A FROM 1 TO N DO
L9(A)▶L8;L8(2)▶L9(A) END;
{}▶L8
L9==L7▶V
//
IF DIFFERENCE(L6,L71)≠{} AND V=0 
THEN
J+1▶J
CONCAT(L1,L2)▶L8
FOR A FROM 1 TO  N1 DO
L8(L9(A))▶L3(A) END;
FOR A FROM 1 TO  N2 DO
L8(L9(N1+A))▶L4(A) END;
ABS(mean(L3)-mean(L4))▶QJ2
ΣLIST(L3)▶QJ11
ΣLIST(L4)▶QJ12
IF QJ11≥Q011 THEN M11=M11+1 END;
IF QJ12≥Q012 THEN M12=M12+1 END;
IF QJ2≥Q02 THEN M02=M02+1 END;
//
QJ11▶L5(J)
QJ2▶L6(J)
//
ELSE
X+1▶X
END; //IF diff
END; //M
//
SORT(L5)▶L5
SORT(L6)▶L6
{}▶L8
{}▶L0
//
X,J,N1,N2,[Q011,Q012,Q02],M11/M,M12/M,[M02/M]
END;
#end
//
</code></pre>
<h5 id="mpv_pas">mPV_.pas</h5>
<pre><code>//mPV_(simulation cycles M)/D.G.SCHRAUSSER/2025
//Permutation test in the random sampling model,
//randomized permutation, p-value not randomized, mP
//2 paired samples (x1|x2)
//e.g.mPV_(100)
#cas
mPV_(M):=
BEGIN
//L1()(2) provided
0▶M11;0▶M12;0▶M2
SIZE(L1)(1)▶N
//sum,Q0
ΣLIST(L1)▶L2;ΣLIST(L2.^2)▶Q02
L2(1)▶Q011;L2(2)▶Q012
//
MSGBOX(&quot;mPV&quot;)
FOR A FROM 1 TO M DO //mP
//
FOR I FROM 1 TO N DO
RANDINT(0,1)▶L8(I) END;
FOR J FROM 1 TO N DO
IF L8(J)=1 THEN REVERSE(L1(J))▶L3(J)
ELSE L1(J)▶L3(J) END;
END; //J
ΣLIST(L3)▶L4;ΣLIST(L4.^2)▶Q2
L4(1)▶Q11;L4(2)▶Q12
Q11▶L5(A)
Q12▶L6(A)
Q2▶L7(A)
IF Q11≥Q011 THEN M11=M11+1 END;
IF Q12≥Q012 THEN M12=M12+1 END;
IF Q2≥Q02 THEN M2=M2+1 END;
//
END;//mP
//
SORT(L5)▶L5;SORT(L6)▶L6;SORT(L7)▶L7
//n,Q011,Q012,Q02,p11,p12,p2
N,[Q011,Q012,Q02],M11/M,M12/M,M2/M
END;
#end
//
</code></pre>
<h5 id="mzpas">mZ.pas</h5>
<pre><code>//mZ()/D.G.SCHRAUSSER/2025  
//Averaged Fisher-Z, mean r/[ZCor]  
#cas  
mZ():=  
BEGIN  
//L1(N)(2) provided  
size(L1)▶N  
FOR I FROM 1 TO N DO  
ZCor(L1(I,1),L1(I,2))(1)▶L3  
L3(1)▶L2(I)  
END;  
Σ((L1(I,2)-3)*L2(I),I,1,N)/Σ(L1(I,2)-3,I,1,N)▶L3(1)  
L3(2):=(e^(2*L3(1))-1)/(e^(2*L3(1))+1)  
//n,_Z,_r  
N,L3  
END;  
//  
</code></pre>
<h4 id="n">N</h4>
<h5 id="nbnmvtlgpas">NBNMVTLG.pas</h5>
<pre><code>//NBNMVTLG(k,p[e],n,m)/D.G.SCHRAUSSER/2025
//e.g.NBNMVTLG(1,0.2,8,10)
#cas
NBNMVTLG(K,P,N,M):=
BEGIN
B=0
FOR I FROM 0 TO N-K DO
 ((K+I-1)!/(I!*(K-1)!))*P^K*(1-P)^I▶L4(I+1)
 B=B+L4(I+1)
END
D7=L4;L4={}
FOR I FROM 0 TO M-1 DO
 ((K+I-1)!/(I!*(K-1)!))*P^K*(1-P)^I▶L5(I+1)
END
D8=L5;L5={};
STARTAPP(&quot;Statistiken_1_Var&quot;);
STARTVIEW(1);
&quot;D7&quot;▶H3(1);5▶H3(3);
&quot;D8&quot;▶H4(1);5▶H4(3);
//p
RETURN(B);
END;
#end 
//
</code></pre>
<h5 id="nvtlgpas">NVTLG.pas</h5>
<pre><code>//NVTLG(z[crit],tail[1/2])/D.G.SCHRAUSSER/2025
//e.g.NVTLG(1.96,2)
#cas
NVTLG(Z,S):=
BEGIN
&quot;Y=(1/√(2*π))*e^((-1/2)*(X)^2)&quot;▶V9
&quot;Y&lt;(1/√(2*π))*e^((-1/2)*(X)^2) AND Y&gt;0 AND X&lt;C&quot;▶V8
NORMALD_CDF(Z)▶P;
IF S=2 THEN
&quot;Y&lt;(1/√(2*π))*e^((-1/2)*(X)^2) AND Y&gt;0 AND X&lt;C AND X&gt;−C&quot;▶V8
P=P-(1-P);
END;
C=Z;
STARTAPP(&quot;Erweiterte_Grafiken&quot;);
STARTVIEW(1);
//p-value
RETURN(P);
END;
#end
//
</code></pre>
<h4 id="p">P</h4>
<h5 id="pcorpas">pCor.pas</h5>
<pre><code>//pCor(correlation r,n)/D.G.SCHRAUSSER/2025  
//e.g.pCor(0.94,4)  
#cas  
pCor(r,n):=  
BEGIN  
t=(r*√(n-2))/√(1-r^2)  
p=STUDENT_CDF(n-2,t)  
2*p▶p2  
IF p&gt;0.5 THEN 2*(1-p)▶p2 END;  
//t-value, p-value, p2  
[t],p,[p2]  
END;  
#end  
//  
</code></pre>
<h5 id="phcpas">PHC.pas</h5>
<pre><code>//PHC(cell count a,b,c,d)/D.G.SCHRAUSSER/2025  
//e.g.PHC(17,12,14,24)  
//Phi- and tetrachoric correlation  
#cas  
PHC(a,b,c,d):=  
BEGIN  
a+b+c+d▶N  
a+b▶z1;c+d▶z2  
a+c▶s1;b+d▶s2  
z1/z2▶Z01;s1/s2▶S01  
IF z2&lt;z1 THEN Z01=1/Z01 END;  
IF s2&lt;s1 THEN S01=1/S01 END;  
VFX=(N*(a*d-b*c)^2)/((a+b)*(c+d)*(a+c)*(b+d))  
VFC=1-CHISQUARE_CDF(1,VFX)  
KPH=(a*d-b*c)/sqrt(((a+b)*(c+d)*(a+c)*(b+d)))  
KPM=sqrt(Z01*S01)  
KTET=cos(π/(1.+√(b*c/(a*d))))  
STET=sqrt((((((a+b)/N))*(((a+c)/N))*(((c+d)/N))*(((b+d)/N))/N)))  
STET=STET*(1/(((1/(sqrt(2*π)))*e^(-normald_icdf(((c+d)/N))^2/2))*((1/(sqrt(2*π)))*e^(-normald_icdf(((b+d)/N))^2/2))))  
ZTET=KTET/STET  
P=normald_cdf(ZTET)  
IF P&gt;0.5 THEN  
P=1-P  
END;  
P*2▶P2  
//n,TET,z,p1,p2,phi,phimax,chi2,p2  
N,[KTET],[ZTET],P,[P2],[KPH,KPM],[VFX],[VFC]  
END;  
#end  
//  
</code></pre>
<h5 id="pkrpas">PKR.pas</h5>
<pre><code>//PKR()/D.G.SCHRAUSSER/2025  
//Partial corr rxy.z[pCor,ZCor]  
#cas  
PKR():=  
BEGIN  
//L1L2L3 provided  
size(L1)▶N  
df=N-2  
//rxy  
FOR I FROM 1 TO N DO  
L1(I)▶L5(1)  
L2(I)▶L5(2)  
L5▶L4(I)  
END;  
approx(correlation(L4))▶r0  
pCor(r0,N)(3)▶pr0  
//rxz  
FOR I FROM 1 TO N DO  
L1(I)▶L5(1)  
L3(I)▶L5(2)  
L5▶L4(I)  
END;  
approx(correlation(L4))▶r1  
pCor(r1,N)(3)▶pr1  
//ryz  
FOR I FROM 1 TO N DO  
L2(I)▶L5(1)  
L3(I)▶L5(2)  
L5▶L4(I)  
END;  
approx(correlation(L4))▶r2  
pCor(r2,N)(3)▶pr2  
//rxy.z  
rp=(r0-r1*r2)  
rp=rp/(sqrt(1-r1^2)*sqrt(1-r2^2))  
rp=approx(rp)  
ZCor(rp,N)(1)▶L0  
L0(1)*SQRT(N-2)▶zrp  
prp=NORMALD_CDF(zrp)  
IF prp&gt;0.5 THEN prp=1-prp END;  
prp2=2*prp  
//pCor(rp,N)(3)▶p  
//df,rxy,p2,rxz,p2,ryz,p2,rxy.z,p2  
df,[r0,pr0],[r1,pr1],[r2,pr2],[rp,prp]  
END;  
#end  
//  
</code></pre>
<h5 id="prm1pas">PRM1.pas</h5>
<pre><code>//PRM1(n perm)/D.G.SCHRAUSSER/2025  
//e.g.PRM1(5)/permutation vector (p)n from L1  
#cas  
PRM1(N):=  
BEGIN  
//L1(N) provided  
{}▶L0  
{}▶L2  
FOR A FROM 1 TO N DO  
{RANDOM(),L1(A)}▶L0(A) END;  
//  
sort(L0)▶L2  
FOR A FROM 1 TO N DO  
L2(A)▶L8;L8(2)▶L2(A) END;  
//  
L2  
END;  
#end  
//  
</code></pre>
<h5 id="prm2pas">PRM2.pas</h5>
<pre><code>//PRM2(elements n)/D.G.SCHRAUSSER/2025  
//Complete permutation matrix (P)n of elements n to 1 class,  
//where P=n!  
//e.g.PRM2(3)  
#cas  
PRM2(n):=  
BEGIN  
MAKELIST(1,P,1,n+1)▶L1  
P=PERM(n,n)  
0▶L1(1)  
{}▶L2  
0▶M1  
1▶J  
0▶I  
0▶SW  
//  
WHILE I≠n AND L1(I)≤n DO  
FOR I FROM 1 TO n DO  
IF I=1 THEN L1(1)+1▶L1(1) END;  
IF I=n AND L1(I)&gt;n THEN BREAK END;  
IF L1(I)&gt;n THEN  
1▶L1(I);L1(I+1)+1▶L1(I+1)  
END;  
END;//I  
//  
FOR K FROM 1 TO n DO  
FOR L FROM K+1 TO n DO  
IF L1(K)=L1(L) THEN  
1▶SW  
BREAK;  
END;  
END;  
END;  
IF SW=0 THEN  
SUPPRESS(L1,n+1)▶L2(J);J+1▶J  
END;  
0▶SW  
END;//while  
//  
L2▶M1  
IF n=2 THEN  
M1=[[1,2],[2,1]]  
M1▶L2  
END;  
//  
P,M1  
END;  
#end  
//  
</code></pre>
<h5 id="prm3pas">PRM3.pas</h5>
<pre><code>//PRM3(elements n, class m)/D.G.SCHRAUSSER/2025  
//Complete permutation matrix w(P)n(km,kn-m) of n elements to class m, where P=n!/IIki!;n&gt;=m  
//equivalent to combination without repetition Cn(m)  
//e.g.PRM3(6,3)[PRM3a]  
#cas  
PRM3(n,m):=  
BEGIN  
MAKELIST(1,P,1,n+1)▶L1  
0▶L1(1)  
{}▶L2  
0▶M1  
1▶J  
0▶I  
0▶SW  
//  
WHILE I≠m AND L1(I)&lt;n DO  
FOR I FROM 1 TO m DO  
IF I=1 THEN L1(1)+1▶L1(1) END;  
IF I=m AND L1(I)&gt;n THEN BREAK END;  
IF L1(I)&gt;n THEN  
1▶L1(I)  
L1(I+1)+1▶L1(I+1)  
END;  
END;//I  
//  
FOR K FROM 1 TO m DO  
FOR L FROM K+1 TO m DO  
IF L1(K)=L1(L) OR L1(K)&gt;L1(L) THEN //&lt;---  
1▶SW  
END;  
END;  
END;  
IF SW=0 THEN  
SUPPRESS(L1,n+1)▶L2(J)  
J+1▶J  
END;  
0▶SW  
END;//while  
//  
//L2▶M1  
PRM3a(n,m)//  
END;  
#end  
//  
</code></pre>
<h5 id="prm3apas">PRM3a.pas</h5>
<pre><code>//PRM3a(elements n,class m)/D.G.SCHRAUSSER/2025  
//e.g.PRM3a(6,3)  
#cas  
PRM3a(N,M):=  
BEGIN  
{}▶L3  
COMB(N,M)▶P  
MAKELIST(x+1-1,x,1,N)▶L1  
//  
FOR J FROM 1 TO P DO  
FOR I FROM 1 TO M DO  
L2(J,I)▶L3(I)  
END;  
L3▶L4(J)▶M1  
END;  
FOR I FROM 1 TO P DO  
L4(I)▶L5;DIFFERENCE(L5,L1)▶L7(I)  
END;  
FOR I FROM 1 TO P DO  
CONCAT(L4(I),L7(I))▶L8(I)  
END;  
L4▶L2;L8▶L3  
{}▶L4  
{}▶L8  
{}▶L5  
{}▶L6  
{}▶L7  
L3▶M2  
P,M2  
END;  
#end  
//  
</code></pre>
<h5 id="prm4pas">PRM4.pas</h5>
<pre><code>//PRM4(elements n, class m)/D.G.SCHRAUSSER/2025  
//variation matrix w(V)n(m), where V=n^m;n&gt;=m  
//e.g.PRM4(4,2)[PRM4a]  
#cas  
PRM4(n,m):=  
BEGIN  
MAKELIST(1,P,1,n+1)▶L1  
0▶L1(1)  
{}▶L2  
0▶M1  
1▶J  
0▶I  
0▶SW  
//  
WHILE I≠m AND L1(I)&lt;n DO  
FOR I FROM 1 TO m DO  
IF I=1 THEN L1(1)+1▶L1(1) END;  
IF I=m AND L1(I)&gt;n THEN BREAK END;  
IF L1(I)&gt;n THEN  
1▶L1(I)  
L1(I+1)+1▶L1(I+1)  
END;  
END;//I  
//  
SUPPRESS(L1,n+1)▶L2(J);J+1▶J  
END;//while  
//  
//L2▶M1  
PRM4a(n,m)  
END;  
#end  
//  
</code></pre>
<h5 id="prm4apas">PRM4a.pas</h5>
<pre><code>//PRM4a(elements n, class m)/D.G.SCHRAUSSER/2025  
//e.g.PRM4a(4,3)  
#cas  
PRM4a(N,M):=  
BEGIN  
//L2 provided  
V=N^M  
X=M+1  
FOR I FROM 1 TO V DO  
L2(I)▶L3  
SUPPRESS(L3,X,N)▶L4(I)  
END;  
L4▶M1  
V,M1  
END;  
#end  
//  
</code></pre>
<h5 id="prm5pas">PRM5.pas</h5>
<pre><code>//PRM5(cases m)/D.G.SCHRAUSSER/2025  
//variation matrix w(V)2(m) for paired 2 sample design PV_,  
//where V=2^m  
//e.g.PRM5(3)  
#cas  
PRM5(N):=  
BEGIN  
M1=0  
2^N▶P  
X=−1  
0▶Z  
P/2▶A  
//  
FOR I FROM 1 TO N DO  
FOR J FROM 1 TO P DO  
X▶M1(J,I)  
Z=Z+1  
IF Z=A THEN X=X*−1;0▶Z; END;  
END;  
0▶Z  
// A=A/2 //  
A=A*0.5  
END;  
//  
M1▶L3  
P,M1  
END;  
#end  
//  
</code></pre>
<h5 id="prmdatpas">PRMDAT.pas</h5>
<pre><code>//PRMDAT(rows n, cols k)/D.G.SCHRAUSSER/2025  
//e.g.PRMDAT(720,6)  
#cas  
PRMDAT(N,K):=  
BEGIN  
//L3 provided  
FOR J FROM 1 TO N DO  
FOR I FROM 1 TO K DO  
L3(M1(J,I))▶M2(J,I)  
END;  
END;  
END;  
#end  
//  
</code></pre>
<h4 id="q">Q</h4>
<h5 id="q01_pas">Q01_.pas</h5>
<pre><code>//Q01_()/D.G.SCHRAUSSER/2025
//Statistical parameters 1.0
//[L1:Raw]
//L2:Distribution
//L3:z-value
//L4:z´-value
#cas
Q01_():=
BEGIN
//L1 provided
SORT(L1)▶L2 //distr
SIZE(L1)▶N
mean(L1)▶AM
stddev(L1)▶SD
stddevp(L1)▶SD1
variance(L1)▶VA
VA1=VA*(N/(N-1)) //SD1^2
SEM=sqrt((VA1/N))
VQ=SD/AM
QGM= N NTHROOT(product(L1))
QHM=N/Σ(1/L1)
approx(MAKELIST(((L2(X)-AM)/SD),X,1,N))▶L3 //z
approx(MAKELIST(((L2(X)-AM)/SD1),X,1,N))▶L4 //z´
//
approx(N,[AM,SEM],SD,SD1,VA,VA1,VQ,[QGM,QHM])
END;
#end
//
</code></pre>
<h5 id="q02_pas">Q02_.pas</h5>
<pre><code>//Q02_()/D.G.SCHRAUSSER/2025
//Statistical parameters 2.0
//[L1:Raw]
//L2:Distribution
//L3:z-value
#cas
Q02_():=
BEGIN
//L1 provided
SORT(L1)▶L2 //
SIZE(L1)▶N
mean(L1)▶AM
stddev(L1)▶SD
stddevp(L1)▶SD1
approx(MAKELIST(((L2(X)-AM)/SD),X,1,N))▶L3 //
Σ(L3.^3)/N▶A3
sqrt(6/N)▶SA3
Σ(L3.^4)/N-3▶A4
2*SA3▶SA4
Σ((L1 .- AM) .^ 3)*N/((N-1)*(N-2)*SD1^3)▶A31
A41=((N-1)*(N-2)*(N-3)*SD1^4)
EX1=Σ((L1 .- AM) .^ 4)*N*(N+1)
EX2=Σ((L1 .- AM) .^ 2)
EX2=3*EX2*EX2*(N-1)
A41=(EX1-EX2)/A41
NORMALD_CDF(A3/SA3)▶PA3
P2A3=2*PA3
IF PA3&gt;0.5 THEN P2A3=2*(1-PA3) END;
NORMALD_CDF(A4/SA4)▶PA4
P2A4=2*PA4
IF PA4&gt;0.5 THEN P2A4=2*(1-PA4) END;
//
approx(N,[A3,A31],A3/SA3,[P2A3],[A4,A41],A4/SA4,[P2A4])
END;
#end
//
</code></pre>
<h4 id="r">R</h4>
<h5 id="rdiffpas">rDiff.pas</h5>
<pre><code>//rDiff(r1,n1,r2,n2)/D.G.SCHRAUSSER/2025  
//e.g.rDiff(0.78,12,0.34,8)[ZCor]  
#cas  
rDiff(R1,N1,R2,N2):=  
BEGIN  
ZCor(R1,N1)(1)▶L2  
L2(1)▶L1(1)  
ZCor(R2,N2)(1)▶L2  
L2(1)▶L1(2)  
L1(1)-L1(2)▶L2(1)  
sqrt((1/(N1-3))+1/(N2-3))▶L2(2)  
L2(1)/L2(2)▶L2(3)  
NORMALD_CDF(L2(3))▶L2(4)  
1-L2(4)▶L2(5)  
2*L2(5)▶L2(6)  
IF L2(5)&gt;0.5 THEN  
2*L2(4)▶L2(6)  
END;  
//Zd,sZd,z,p,1-p,p2  
[L2(1),L2(2)],[L2(3)],L2(4),L2(5),[L2(6)]  
END;  
#end  
//  
</code></pre>
<h5 id="rhopas">RHO.pas</h5>
<pre><code>//RHO()/D.G.SCHRAUSSER/2025  
//Spearman's rank correlation coefficient rho rs/[pCor]  
#cas  
RHO():=  
BEGIN  
//L1()(2) provided  
size(L1)▶N  
mean(L1)▶L3  
MAKELIST((L1(I)(1)-L1(I)(2))^2,I,1,N)▶L2  
Σ(L2)▶SUM  
RHO=1-((6*SUM)/(N*(N^2-1)))  
pCor(RHO,N)▶L4  
//n,rho,r,p2rho  
approx(N,[RHO,correlation(L1)],L4(3))  
END;  
#end  
//
</code></pre>
<h5 id="rnkpas">RNK.pas</h5>
<pre><code>//RNK()/D.G.SCHRAUSSER/2025
//L1 to ranking L3
#cas
RNK():=
BEGIN
//L1 provided
{}▶L2
{}▶L3
1▶R
0▶S1
0▶Q01
0▶V
SIZE(L1)▶N
SORT(L1)▶L2
FOR I FROM 2 TO N+1 DO
R▶L3(I-1)
IF L2(I-1)≠L2(I) THEN
IF S1=1 THEN
R=R+Q01
Q01=0;S1=0
1+V▶V
END;
R+1▶R
ELSE
Q01+1▶Q01;S1=1
END;
END;
Σ(L3)▶SR
//n,ties,Rsum,meanR
approx(N,V,SR,[SR/N])
END;
#end
//
</code></pre>
<h4 id="t">T</h4>
<h5 id="tkvpas">TKV.pas</h5>
<pre><code>//TKV()/D.G.SCHRAUSSER/2025  
//Variance p/[C2V,pCor]  
#cas  
TKV():=  
BEGIN  
//L1L2 provided  
SIZE(L1)▶N  
variance(L1)▶s21  
variance(L2)▶s22  
N-2▶df  
C2V(2)  
approx(correlation(L6))▶r  
pCor(r,N)(2)▶pr  
approx(((s21-s22)*sqrt(N-2))/(2*sqrt(s21*s22*(1-r^2))))▶t  
STUDENT_CDF(df,t)▶p  
1-p▶p1  
2*p▶p2  
IF p2&gt;1 THEN 2*(1-p)▶p2 END;  
df,[r],[pr],[t],p,p1,[p2]  
END;  
#end  
//  
</code></pre>
<h5 id="tt_pas">TT_.pas</h5>
<pre><code>//TT_(y)/D.G.SCHRAUSSER/2025
//One-sample t-test for test variable y
//e.g TT_(5.3)
#cas
TT_(Y):=
BEGIN
//L1 provided
size(L1)▶N
N-1▶df
mean(L1)▶x
variance(L1)▶s2
approx((x-Y)/(sqrt(s2/(N-1))))▶t
p=STUDENT_CDF(df,t)
1-p▶p1
2*p▶p2
IF p2&gt;1 THEN 2*(1-p)▶p2 END;
df,[t],p,p1,[p2]
END;
#end
//
</code></pre>
<h5 id="tu_pas">TU_.pas</h5>
<pre><code>//TU_()/D.G.SCHRAUSSER/2025
//t-test for unpaired samples 
#cas
TU_():=
BEGIN
//L1L2 provided
0▶Sx1
0▶Sx2
SIZE(L1)▶n1
SIZE(L2)▶n2
n1+n2-2▶df
mean(L1)▶x1
mean(L2)▶x2
FOR I FROM 1 TO n1 DO
Sx1=Sx1+(L1(I)-x1)^2 END;
FOR I FROM 1 TO n2 DO
Sx2=Sx2+(L2(I)-x2)^2 END;
t=x1-x2
t=t/(√((Sx1+Sx2)/((n1-1)+(n2-1)))*√(1/n1+1/n2))
t=approx(t)
p=STUDENT_CDF(df,t)
p1=p
IF p&gt;0.5 THEN p1=1-p END;
p2=2*p1
df,[t],p,p1,[p2]
END;
#end
//
</code></pre>
<h5 id="tv_pas">TV_.pas</h5>
<pre><code>//TV_()/D.G.SCHRAUSSER/2025
//t-test for paired samples
#cas
TV_():=
BEGIN
//L1()(2) provided
size(L1)▶n
n-1▶df
FOR I FROM 1 TO n DO
L1(I)(1)-L1(I)(2)▶L2(I)
L2(I)^2▶L3(I)
END;
ΣLIST(L2)▶Sxd
ΣLIST(L3)▶Sxd2
t=Sxd/n
t = (t/((√((Sxd2-Sxd^2/n)/(n-1)))/(√n)))
t=approx(t)
p=STUDENT_CDF(df,t)
p▶p1
IF p&gt;0.5 THEN 1-p▶p1 END;
2*p1▶p2
//
df,[t],p,p1,[p2]
END;
#end
//
</code></pre>
<h5 id="tvtlgpas">tVTLG.pas</h5>
<pre><code>//tVTLG(t,df)/D.G.SCHRAUSSER/2025
//e.g.tVTLG(2.65,8)[AdvancedGraphing]
#cas
tVTLG(T857,D187):=
BEGIN
G25478=Gamma((D187+1)/2)/Gamma(D187/2)
P=∫(G25478*(D187*π)^(-1/2)*(1+(X^2/D187))^(-(D187+1)/2),X,−∞,T857)
T857▶C
D187▶D
G25478▶G
&quot;Y=(G*(D*π)^(-1/2)*(1+(X^2/D))^(-(D+1)/2))&quot;▶V1
&quot;Y&lt;(G*(D*π)^(-1/2)*(1+(X^2/D))^(-(D+1)/2)) AND Y&gt;0 AND X&lt;C&quot;▶V2
&quot;Y=(1/√(2*π))*e^((-1/2)*(X)^2)&quot;▶V3
STARTAPP(&quot;Erweiterte_Grafiken&quot;);
STARTVIEW(1);
IF P&gt;0.5 THEN P=1-P END;
P,[2*P]
END;
#end
//
</code></pre>
<h4 id="v">V</h4>
<h5 id="var1pas">VAR1.pas</h5>
<pre><code>//VAR1(n var)/D.G.SCHRAUSSER/2025  
//e.g.VAR1(5)/variation vector (v)n from L1  
#cas  
VAR1(N):=  
BEGIN  
//L1(N) provided  
{}▶L2  
FOR A FROM 1 TO N DO  
L1(RANDINT(1,N))▶L2(A) END;  
//  
L2  
END;  
#end  
//  
</code></pre>
<h5 id="vfc0pas">VFC0.pas</h5>
<pre><code>//VFC0(cell count a,b,c,d)/D.G.SCHRAUSSER/2025  
//2×2 chi-squared test for independence  
//Observed frequencies abcd fb  
//Chi-squared, McNemar with 2-tailed p  
//e.g.VFC0(17,12,14,24)  
#cas  
VFC0(a,b,c,d):=  
BEGIN  
a+b+c+d▶N  
a+b▶z1;c+d▶z2  
a+c▶s1;b+d▶s2  
z1/z2▶Z01;s1/s2▶S01  
VFX=(N*(a*d-b*c)^2)/((a+b)*(c+d)*(a+c)*(b+d))  
VFC=1-CHISQUARE_CDF(1,VFX)  
MNX=(b-c)^2/(b+c)  
//McNemar Yates corr.  
IF b+c&lt;30 AND b+c&gt;20 THEN  
MNX=(ABS(b-c)-0.5)^2/(b+c)  
END  
pMNX=1-CHISQUARE_CDF(1,MNX)  
//n,mnchi2,mnp2,chi2,p2  
N,[MNX],[pMNX],[VFX],[VFC]  
END;  
#end  
//  
</code></pre>
<h5 id="vfchpas">VFCH.pas</h5>
<pre><code>//VFCH(cell count a,b,c,d)/D.G.SCHRAUSSER/2025  
//2×2 chi-squared test for independence  
//L0: Observed frequencies abcd fb  
//L1: Expected frequencies fe  
//L2,L3,L4: Probabilities p(A^B), p(B|A), p(A|B)  
//L5: Chi-squared,(w. Yates corr.)  
//L6: 2-tailed sig. p2  
//Chi-square McNemar (w. Yates corr.) with p2  
//e.g.VFCH(17,12,14,24)  
//  
#cas  
VFCH(a,b,c,d):=  
BEGIN  
a▶L0(1)  
b▶L0(2)  
c▶L0(3)  
d▶L0(4)  
a+b+c+d▶N  
a+b▶z1  
c+d▶z2  
a+c▶s1  
b+d▶s2  
z1/z2▶Z01  
s1/s2▶S0  
//p  
a/N▶L1(1)  
b/N▶L1(2)  
z1/N▶L1(5)  
c/N▶L1(3)  
d/N▶L1(4)  
z2/N▶L1(6)  
s1/N▶L1(7)  
s2/N▶L1(8)  
//fe  
L1(7)*z1▶L2(1)  
L1(8)*z1▶L2(2)  
L1(7)*z2▶L2(3)  
L1(8)*z2▶L2(4)  
//p(B|A)  
L0(1)/s1▶L3(1)  
L0(2)/s2▶L3(2)  
L0(3)/s1▶L3(3)  
L0(4)/s2▶L3(4)  
//p(A|B)  
L0(1)/z1▶L4(1)  
L0(2)/z1▶L4(2)  
L0(3)/z2▶L4(3)  
L0(4)/z2▶L4(4)  
//  
//CHI2  
L0(1)-L2(1)▶L5(1);L5(1)^2▶L6(1);L6(1)/L2(1)▶L5(1)  
L0(2)-L2(2)▶L5(2);L5(2)^2▶L6(1);L6(1)/L2(2)▶L5(2)  
L0(3)-L2(3)▶L5(3);L5(3)^2▶L6(1);L6(1)/L2(3)▶L5(3)  
L0(4)-L2(4)▶L5(4);L5(4)^2▶L6(1);L6(1)/L2(4)▶L5(4)  
L5(1)+L5(2)▶L5(5)  
L5(3)+L5(4)▶L5(6)  
L5(1)+L5(3)▶L5(7)  
L5(2)+L5(4)▶L5(8)  
L5(7)+L5(8)▶L5(9)  
1-CHISQUARE_CDF(1,L5(1))▶L6(1)  
1-CHISQUARE_CDF(1,L5(2))▶L6(2)  
1-CHISQUARE_CDF(1,L5(3))▶L6(3)  
1-CHISQUARE_CDF(1,L5(4))▶L6(4)  
1-CHISQUARE_CDF(1,L5(5))▶L6(5)  
1-CHISQUARE_CDF(1,L5(6))▶L6(6)  
1-CHISQUARE_CDF(1,L5(7))▶L6(7)  
1-CHISQUARE_CDF(1,L5(8))▶L6(8)  
1-CHISQUARE_CDF(1,L5(9))▶L6(9)  
//Yates corr.  
IF L2(1)&lt;7  AND L2(1)&gt;4  OR  
L2(2)&lt;7 AND L2(2)&gt;4  OR  
L2(3)&lt;7 AND L2(3)&gt;4  OR  
L2(4)&lt;7 AND L2(4)&gt;4  
THEN  
(N*(ABS((L0(1)*L0(4)-L0(2)*L0(3))-N/2))^2/(z1*z2*s1*s2))▶L5(9)  
1-CHISQUARE_CDF(1,L5(9))▶L6(9)  
END;  
MNX=(b-c)^2/(b+c)  
//Yates corr.  
IF b+c&lt;30 AND b+c&gt;20 THEN  
MNX=(ABS(b-c)-0.5)^2/(b+c)  
END   
pMNX=1-CHISQUARE_CDF(1,MNX)  
//n,mnchi2,mnp2,chi2,p2  
N,[MNX],[pMNX],[L5(9)],[L6(9)]  
END;  
#end  
//  
</code></pre>
<h4 id="z">Z</h4>
<h5 id="zcorpas">ZCor.pas</h5>
<pre><code>//ZCor(correlation r,n)/D.G.SCHRAUSSER/2025  
//e.g.ZCor(0.94,6)  
#cas  
ZCor(r,n):=  
BEGIN  
Z=0.5*ln(((1+r)/(1-r)))  
sZ=√(1/(n-3))  
zV=Z/sZ  
Pz=NORMALD_CDF(zV)  
P2=2*Pz  
IF Pz&gt;0.5 THEN  
P2=2*(1-Pz)  
END;  
//Fisher-Z,sZ,z,p,1-p,p2  
approx([Z,sZ],[zV],Pz,1-Pz,[P2])  
END;  
#end  
//  
</code></pre>
<br>
<h3 id="user-functions">User functions</h3>
<br>
<p><code>define.cas</code> [Shift][Define]...</p>
<br>
<h4 id="a-1">A</h4>
<h5 id="addppn">ADDP(p,n)</h5>
<p><code>1.000-(1.000-P)^N</code></p>
<h5 id="aevevtv">AEv(Ev,Tv)</h5>
<p><code>√(2^E*T)/T</code></p>
<h5 id="avsa0s0s">AvS(A0,S0,S)</h5>
<p><code>A*e^(0.5*LN(S^(-1)*B))</code></p>
<h5 id="avska0s-steps-kegavsk1003">AvSk(A0,S steps k),e.g.AvSk(100,3)</h5>
<p><code>A*((2) NTHROOT (2))^K</code></p>
<h5 id="avtvav0tv0tv">AvTv(Av0,Tv0,Tv)</h5>
<p><code>A*e^(0.5*LN((T/B)))</code></p>
<h5 id="avtvkav0tv-steps-kegavtvk8-3">AvTvk(Av0,Tv steps k),e.g.AvTvk(8,-3)</h5>
<p><code>A*(√2^K)</code></p>
<h4 id="b-1">B</h4>
<h5 id="binomab">BINOM(a,b)</h5>
<p><code>Σ(((A+B)!/(I!*(A+B-I)!))*2.000^(-I)*2.000^(-(A+B-I)),I,0.000,A)</code></p>
<h4 id="c-1">C</h4>
<h5 id="cf2rcohens-f">Cf2(R)'Cohen's f²'</h5>
<p><code>R^2/(1-R^2)</code></p>
<h5 id="chp1p2cohens-h-for-proportion-differences">Ch(p1,p2)'Cohen's h for proportion differences'</h5>
<p><code>2*ASIN(√A)-2*ASIN(√B)</code></p>
<h5 id="chap2h">ChA(p2,h)</h5>
<p><code>SIN((1/2)*(2*ASIN(√B)+H))^2</code></p>
<h5 id="chbp1h">ChB(p1,h)</h5>
<p><code>(-SIN((1/2)*(-2*ASIN(√A)+H)))^2</code></p>
<h5 id="cirpsrcircisdx-or-sdyrxyegcir0990342098">CIr(P,S,R),CIr(ci%,sdx or sdy,rxy),e.g.CIr(0.99,0.342,0.98)</h5>
<p><code>NORMALD_CDF(1-(1-P)/2)*S*√(1-R^2)</code></p>
<h5 id="cix-cixbcacixncisdeg67cix100991676">CIx [CIx(B,C,A),CIx(n,ci%,sd),e.g.6.7+CIx(10,0.99,1.676)]</h5>
<p><code>STUDENT_ICDF(B-1,1-((1-C)/2))*√((A^2)*(B/(B-1))/B)</code></p>
<h4 id="d-1">D</h4>
<h5 id="d2rdeg">D2R(deg)</h5>
<p><code>X/180.000*π</code></p>
<h4 id="e-1">E</h4>
<h5 id="efgx1l1epsilon-cohens-d">EFG(x1,L1)'epsilon, Cohen's d'</h5>
<p><code>(A-mean(L1))/stddevp(L1)</code></p>
<h5 id="efrx1l1repsilon-cohens-d-for-paired-samples">EFR(x1,L1,R)'epsilon, Cohen's d for paired samples'</h5>
<p><code>((A-mean(L1))/stddevp(L1))/√(1-R)</code></p>
<h5 id="evtvavexposure-value">Ev(Tv,Av)'exposure value'</h5>
<p><code>(LN(2))^(-1)*LN(T*A^2)</code></p>
<h4 id="f-1">F</h4>
<h5 id="fmcorrrn">FMCORR(R,n)</h5>
<p><code>A^2.000*(B-4.000)/(3.000*(-(A^2.000)+1.000))</code></p>
<h4 id="h">H</h4>
<h5 id="hm_eghm_l2">HM_[e.g.HM_(L2)]</h5>
<p><code>SIZE(L1)/Σ((L1(B))^(-1),B,1,SIZE(L1))</code></p>
<h4 id="i-1">I</h4>
<h5 id="isoaiso">ISOA(iso°)</h5>
<p><code>10^(0.1*(S-1))</code></p>
<h5 id="isoliso">ISOL(iso)</h5>
<p><code>(10*LN(S)/LN(10))+1</code></p>
<h4 id="m-1">M</h4>
<h5 id="mcorrr1r2r3">MCORR(r1,r2,r3)</h5>
<p><code>√((A^2.000+B^2.000-2.000*C*A*B)/(1.000-C^2.000))</code></p>
<h5 id="mrr1n1r2n2">mr(r1,n1,r2,n2)</h5>
<p><code>(e^(2*(((0.5*LN(((1+A)/(1-A)))*(B-3))+(0.5*LN(((1+C)/(1-C)))*(D-3)))/((B-3)+(D-3))))-1)/(e^(2*(((0.5*LN(((1+A)/(1-A)))*(B-3))+(0.5*LN(((1+C)/(1-C)))*(D-3)))/((B-3)+(D-3))))+1)</code></p>
<h5 id="mzr1n1r2n2">mZ(r1,n1,r2,n2)</h5>
<p><code>((0.5*LN(((1+A)/(1-A)))*(B-3))+(0.5*LN(((1+C)/(1-C)))*(D-3)))/((B-3)+(D-3))</code></p>
<h4 id="n-1">N</h4>
<h5 id="nbinomkpn">NBINOM(k,p,n)</h5>
<p><code>Σ(((K+I-1.000)!/(I!*(K-1.000)!))*P^K*(1.000-P)^I,I,0.000,N-K)</code></p>
<h5 id="nknk">nk(N,K)</h5>
<p><code>N!/(K!*(N-K)!)</code></p>
<h5 id="npzxamsdnpz-quantity-at-negnpz160100158109">npz(x,am,sd,N)'p&gt;=z quantity at N',e.g.npz(160,100,15,8*10^9)</h5>
<p><code>(1-NORMALD_CDF(((X-A)/S)))*N</code></p>
<h4 id="p-1">P</h4>
<h5 id="p2fab">p2F(a,b)</h5>
<p><code>(1-CHISQUARE_CDF(1,((A-((A+B)/2))^2/((A+B)/2))+((B-((A+B)/2))^2/((A+B)/2))))/2</code></p>
<h5 id="p4fabcd">p4F(a,b,c,d)</h5>
<p><code>(1-CHISQUARE_CDF(1,((A+B+C+D)*(A*D-B*C)^2/((A+B)*(C+D)*(A+C)*(B+D)))))/2</code></p>
<h5 id="p4fyabcd-yates-corr-4fe7">p4FY(a,b,c,d) Yates corr 4&lt;fe&lt;7</h5>
<p><code>(1-CHISQUARE_CDF(1,((A+B+C+D)*(ABS(A*D-B*C)-((A+B+C+D)/2))^2/((A+B)*(C+D)*(A+C)*(B+D)))))/2</code></p>
<h5 id="phiadbc">PHI(a,d,b,c)</h5>
<p><code>(A*D-B*C)/(sqrt((A+C)*(B+D)*(A+B)*(C+D)))</code></p>
<h5 id="pmcorrnr">pMCORR(n,R)</h5>
<p><code>(1-FISHER_CDF(3,(B-4),(A^2*(B-4)/(3*(-(A^2)+1)))))/2</code></p>
<h5 id="pmnbc">pMN(b,c)</h5>
<p><code>(1-CHISQUARE_CDF(1,((A-B)^2/(A+B))))/2</code></p>
<h5 id="pmnybc-yates-corr-20bc30">pMNY(b,c) Yates corr 20&lt;b+c&lt;30</h5>
<p><code>(1-CHISQUARE_CDF(1,((ABS(A-B)-0.5)^2/(A+B))))/2</code></p>
<h5 id="polx">POL(x)</h5>
<p><code>polar_coordinates(X)</code></p>
<h5 id="pphiadbc">pPHI(a,d,b,c)</h5>
<p><code>(1-CHISQUARE_CDF(1,((A*D-B*C)/(√((A+C)*(B+D)*(A+B)*(C+D))))^2*(A+B+C+D)))/2</code></p>
<h5 id="prbisl1l2">prbis(L1,L2)</h5>
<p><code>normald_cdf((((mean(L1)-mean(L2))/stddev(CONCAT(L1,L2)))*SIZE(L1)*SIZE(L2)/((1/(√(2*π)))*e^(-(NORMALD_ICDF((SIZE(L2)/SIZE(CONCAT(L1,L2))))^2)/2)*SIZE(CONCAT(L1,L2))^2))/(√(SIZE(L1)*SIZE(L2))/((√SIZE(CONCAT(L1,L2))*SIZE(CONCAT(L1,L2))*1/(√(2*π)))*e^(-(NORMALD_ICDF((SIZE(L2)/SIZE(CONCAT(L1,L2))))^2)/2))))</code></p>
<h5 id="prbisrl1l2">prbisR(L1,L2)</h5>
<p><code>normald_cdf(((size(L1)*size(L2)+(((size(L1))^2+size(L1))/2)-ΣLIST(L1)-size(L1)*size(L2)/2)/(sqrt(size(L1)*size(L2)*(size(L1)+size(L2)+1)/12))))</code></p>
<h5 id="prrr1r2n1n2">prr(r1,r2,n1,n2)</h5>
<p><code>NORMALD_CDF(((0.5*LN(((1+A)/(1-A)))-0.5*LN(((1+B)/(1-B))))/(√((1/(C-3))+1/(D-3)))))</code></p>
<h5 id="prtetbcad">prtet(b,c,a,d)</h5>
<p><code>NORMALD_CDF((COS((π/(1+√(B*C/(A*D)))))/(√(((A+B)/(A+B+C+D))*((A+C)/(A+B+C+D))*((C+D)/(A+B+C+D))*((B+D)/(A+B+C+D))/(A+B+C+D))*(1/(((1/(√(2*π)))*e^(-(NORMALD_ICDF(((C+D)/(A+B+C+D)))^2)/2))*((1/(√(2*π)))*e^(-(NORMALD_ICDF(((B+D)/(A+B+C+D)))^2)/2)))))))</code></p>
<h5 id="prwl1l2">pRW(L1,L2)</h5>
<p><code>STUDENT_CDF(SIZE(L1)-2,((correlation(L1,L2)*√(SIZE(L1)-2))/(√(1-correlation(L1,L2)^2))))</code></p>
<h5 id="prwxnr">pRWx(n,r)</h5>
<p><code>STUDENT_CDF(B-2,((A*√(B-2))/(√(1-A^2))))</code></p>
<h5 id="prxy_zrxy_zn">prxy_z(rxy_z,n)</h5>
<p><code>NORMALD_CDF(0.5*LN(((1+A)/(1-A)))*√(B-2))</code></p>
<h5 id="ptgab">PTG(a,b)</h5>
<p><code>√(A^2+B^2)</code></p>
<h5 id="ptkvl1l2">pTKV(L1,L2)</h5>
<p><code>STUDENT_CDF(SIZE(L1)+SIZE(L2)-2,(((variance(L1)-variance(L2))*√(SIZE(L1)-2))/(2*√(variance(L1)*variance(L2)*(1-correlation(L1,L2))))))</code></p>
<h5 id="ptt_l1y">pTT_(L1,y)</h5>
<p>STUDENT_CDF(SIZE(L1)-1,((mean(L1)-A)/(√(stddev(L1)^2/<code>(SIZE(L1)-1)))))</code></p>
<h5 id="ptu_l1l2">pTU_(L1,L2)</h5>
<p><code>STUDENT_CDF(SIZE(L1)+SIZE(L2)-2,(mean(L1)-mean(L2))/(√((ΣLIST(MAKELIST((L1(A)-mean(L1))^2,A,1,SIZE(L1)))+ΣLIST(MAKELIST((L2(A)-mean(L2))^2,A,1,SIZE(L2))))/(SIZE(L1)-1+SIZE(L2)-1))*(√(1/SIZE(L1))+√(1/SIZE(L2)))))</code></p>
<h5 id="ptuxn1n2x1x2s21s22">pTUX(n1,n2,x1,x2,s21,s22)</h5>
<p><code>STUDENT_CDF(D+F-2,((A-B)/(√((C*D+E*F)/(D-1+F-1))*(√(1/D)+√(1/F)))))</code></p>
<h5 id="ptv_l1">pTV_(L1)</h5>
<p><code>student_cdf(size(L1)-1,ΣLIST(MAKELIST(L1(A)-(L2(A)),A,1,size(L1)))/(size(L1))/(√((ΣLIST(MAKELIST((L1(A)-(L2(A)))^2,A,1,size(L1)))-ΣLIST(MAKELIST(L1(A)-(L2(A)),A,1,size(L1)))^2/(size(L1)))/(size(L1)-1))*1/(√(size(L1)-1))))</code></p>
<h5 id="pz4fabcdegpz4f11208058">pz4F(a,b,c,d)[e.g.pz4F(11,20,80,58)]</h5>
<p><code>NORMALD_CDF(((D-(A+B+C+D)*((D+B)*(C+D)/(A+B+C+D)^2))/(√((A+B+C+D)*(1-((D+B)*(C+D)/(A+B+C+D)^2))-(A+B+C+D)*(A+B+C+D-1)*((D+B)*(C+D)/(A+B+C+D)^2)*(((D+B)*(C+D)/(A+B+C+D)^2)-((D+B-1)*(C+D-1)/(A+B+C+D-1)^2))))))</code></p>
<h5 id="pzbnab">pzBN(a,b)</h5>
<p><code>NORMALD_CDF(((A-(A+B)/2)/(√((A+B)/4))))</code></p>
<h4 id="r-1">R</h4>
<h5 id="r2drad">R2D(rad)</h5>
<p><code>X/π*180.000</code></p>
<h5 id="rbisl1l2">rbis(L1,L2)</h5>
<p><code>((mean(L1)-mean(L2))/stddev(CONCAT(L1,L2)))*SIZE(L1)*SIZE(L2)/((1/(√(2*π)))*e^(-(NORMALD_ICDF((SIZE(L2)/SIZE(CONCAT(L1,L2))))^2)/2)*SIZE(CONCAT(L1,L2))^2)</code></p>
<h5 id="rbisrl1l2">rbisR(L1,L2)</h5>
<p><code>(2/(SIZE(L1)+SIZE(L2)))*(mean(L1)-mean(L2))</code></p>
<h5 id="redr">RED(r)</h5>
<p><code>A^2*100</code></p>
<h5 id="rnd1n">RND1(n)</h5>
<p><code>MAKELIST(RANDNORM,A,1,B)</code></p>
<h5 id="rnd2n">RND2(n)</h5>
<p><code>MAKELIST(RANDOM,A,1,B</code>)</p>
<h5 id="rpbisl1l2">rpbis(L1,L2)</h5>
<p><code>(mean(L1)-mean(L2))/stddev(CONCAT(L1,L2))*√(SIZE(L1)*SIZE(L2)/(SIZE(CONCAT(L1,L2)))^2)</code></p>
<h5 id="rxyx1x2">rxy(x1,x2)</h5>
<p><code>approx(correlation(L1,L2))</code></p>
<h5 id="rtetbcadrad">rtet(b,c,a,d),rad</h5>
<p><code>COS((π/(1+√(B*C/(A*D)))))</code></p>
<h5 id="rxy_zrxyrxzryz">rxy_z(rxy,rxz,ryz)</h5>
<p><code>(A-B*C)/(√(1-B^2)*√(1-C^2))</code></p>
<h5 id="ry_xzrxyrxzryz">ry_xz(rxy,rxz,ryz)</h5>
<p><code>(A-B*C)/(sqrt(1-B^2))</code></p>
<h5 id="rzz">rZ(Z)</h5>
<p><code>(e^(2*A)-1)/(e^(2*A)+1)</code></p>
<h4 id="s">S</h4>
<h5 id="scrnkr">SCR(n,k,R)</h5>
<p><code>1.00-((A-3.00)/(A-B-2.00))*((1.00-C^2.00)+((2.00/(A-B)))*(1.00-C^2.00)^2.00)</code></p>
<h5 id="smg-smgabsmgsdn">SMG [SMG(A,B),SMG(sd,n)]</h5>
<p><code>√((A^2)*(B/(B-1))/B)</code></p>
<h5 id="sqrx">SQR(x)</h5>
<p><code>A^2</code></p>
<h5 id="srbisl1l2">srbis(L1,L2)</h5>
<p><code>√(SIZE(L1)*SIZE(L2))/((√SIZE(CONCAT(L1,L2))*SIZE(CONCAT(L1,L2))*1/(√(2*π)))*e^(-(NORMALD_ICDF((SIZE(L2)/SIZE(CONCAT(L1,L2))))^2)/2))</code></p>
<h5 id="srtetbcad">srtet(b,c,a,d)</h5>
<p><code>√(((A+B)/(A+B+C+D))*((A+C)/(A+B+C+D))*((C+D)/(A+B+C+D))*((B+D)/(A+B+C+D))/(A+B+C+D))*(1/(((1/(√(2*π)))*e^(-(NORMALD_ICDF(((C+D)/(A+B+C+D)))^2)/2))*((1/(√(2*π)))*e^(-(NORMALD_ICDF(((B+D)/(A+B+C+D)))^2)/2))))</code></p>
<h5 id="sumd2l1">sumd2(L1)</h5>
<p><code>ΣLIST(MAKELIST((L1(A)-(L2(A)))^2,A,1,SIZE(L1)))</code><br />
CAS input<br />
<code>L4:=ΣLIST(L3:=MAKELIST((L1(x)-(L2(x)))^2,x,1,size(L1)))</code><br />
<code>L4:=ΣLIST(L3:=MAKELIST(L1(x)-(L2(x)),x,1,size(L1)))</code><br />
<code>L4:=ΣLIST(L3:=MAKELIST(L1(x)-(L2(x)),x,1,size(L1)))^2</code></p>
<h5 id="sumx2l1l2">sumx2(L1,L2)</h5>
<p>CAS input<br />
<code>ΣLIST(L3:=approx(MAKELIST((L1(x)-mean(L1))^2,x,1,size(L1))))</code></p>
<h4 id="t-1">T</h4>
<h5 id="taupas">TAU.pas</h5>
<p><code>//TAU()/D.G.SCHRAUSSER/2025</code><br />
<code>//Kendall's τ coefficient (tau-a)</code><br />
<code>#cas</code><br />
<code>TAU():=</code><br />
<code>BEGIN</code><br />
<code>//L1,L2 provided</code><br />
<code>SIZE(L1)▶N</code><br />
<code>FOR I FROM 1 TO N-1 DO</code><br />
<code>((SIGN(L1(I)-L1(I+1))))*((SIGN(L2(I)-L2(I+1))))▶L3(I)</code><br />
<code>IF L3(I)=1 THEN 0▶L3(I) END;</code><br />
<code>IF L3(I)=-1 THEN 1▶L3(I) END;</code><br />
<code>888▶M1(L1(I),L2(I))</code><br />
<code>END;</code><br />
<code>888▶M1(L1(N),L2(N))</code><br />
<code>//(N*(N-1)/2-2*ΣLIST(L3))/(N*(N-1)/2)▶tau_a</code><br />
<code>approx(1-2*ΣLIST(L3)/(N*(N-1)/2))▶L4(1)</code><br />
<code>approx((√(N*(N-1)))/(√(2*(2*N+5)))*3*L4(1))▶L4(2)</code><br />
<code>NORMALD_CDF(L4(2))▶L4(3)</code><br />
<code>1-L4(3)▶L4(4)</code><br />
<code>L4(4)*2▶L4(5)</code><br />
<code>IF L4(3)&lt;0.5 THEN 2*L4(3)▶L4(5)</code><br />
<code>((COMB(N,2)-ΣLIST(L3))-ΣLIST(L3))/√((1/18)*N*(N-1)*(2*N+5))▶L4(6)</code><br />
<code>END;</code><br />
<code>//taua,z,p,1-p,p2</code><br />
<code>[L4(1)],[L4(2)],L4(3),L4(4),[L4(5)]</code><br />
<code>END;</code><br />
<code>#end</code><br />
<code>//</code></p>
<h5 id="tevevav">TEv(Ev,Av)</h5>
<p><code>2^E/A^2</code></p>
<h5 id="trrn">tr(r,n)</h5>
<p><code>(R*√(N-2))/(√(1-R^2))</code></p>
<h5 id="trwl1l2">TRW(L1,L2)</h5>
<p><code>(correlation(L1,L2)*√(SIZE(L1)-2))/(√(1-correlation(L1,L2)^2))</code></p>
<h5 id="ttkvl1l2">tTKV(L1,L2)</h5>
<p><code>((variance(L1)-variance(L2))*sqrt(size(L1)-2))/(2*sqrt(variance(L1)*variance(L2)*(1-correlation(L1,L2))))</code></p>
<h5 id="ttt_l1y">tTT_(L1,y)</h5>
<p><code>(mean(L1)-A)/(√(stddev(L1)^2/(SIZE(L1)-1)))</code></p>
<h5 id="ttu_l1l2">tTU_(L1,L2)</h5>
<p><code>(mean(L1)-mean(L2))/(sqrt((ΣLIST(MAKELIST((L1(x)-mean(L1))^2,x,1,size(L1)))+ ΣLIST(MAKELIST((L2(x)-mean(L2))^2,x,1,size(L2))))/ (size(L1)-1+size(L2)-1))*(sqrt(1/size(L1))+sqrt(1/size(L2))))</code></p>
<h5 id="ttuxx1x2s21n1s22n2">tTUX(x1,x2,s21,n1,s22,n2)</h5>
<p><code>(A-B)/(√((C*D+E*F)/(D-1+F-1))*(√(1/D)+√(1/F)))</code></p>
<h5 id="ttv_l1">tTV_(L1)</h5>
<p><code>ΣLIST(MAKELIST(L1(A)-(L2(A)),A,1,size(L1)))/(size(L1))/(√((ΣLIST(MAKELIST((L1(A)-(L2(A)))^2,A,1,size(L1)))-ΣLIST(MAKELIST(L1(A)-(L2(A)),A,1,size(L1)))^2/(size(L1)))/(size(L1)-1))*1/(√(size(L1)-1)))</code></p>
<h4 id="u">U</h4>
<h5 id="u_1l1l2">U_1(L1,L2)</h5>
<p><code>SIZE(L1)*SIZE(L2)+(((SIZE(L1))^2+SIZE(L1))/2)-ΣLIST(L1)</code></p>
<h5 id="u_2l1l2">U_2(L1,L2)</h5>
<p><code>SIZE(L1)*SIZE(L2)+(((SIZE(L2))^2+SIZE(L2))/2)-ΣLIST(L2)</code></p>
<h4 id="x">X</h4>
<h5 id="x2fab">x2F(a,b)</h5>
<p><code>((A-((A+B)/2))^2/((A+B)/2))+(B-((A+B)/2))^2/((A+B)/2)</code></p>
<h5 id="x4fabcd">x4F(a,b,c,d)</h5>
<p><code>(A+B+C+D)*(A*D-B*C)^2/((A+B)*(C+D)*(A+C)*(B+D))</code></p>
<h5 id="x4fyabcd-yates-corr-4fe7">x4FY(a,b,c,d) Yates corr 4&lt;fe&lt;7</h5>
<p><code>(A+B+C+D)*(ABS(A*D-B*C)-((A+B+C+D)/2))^2/((A+B)*(C+D)*(A+C)*(B+D))</code></p>
<h5 id="xmnbc">xMN(b,c)</h5>
<p><code>(A-B)^2/(A+B)</code></p>
<h5 id="xmnybc-yates-corr-20bc30">xMNY(b,c) Yates corr 20&lt;b+c&lt;30</h5>
<p><code>(ABS(A-B)-0.5)^2/(A+B)</code></p>
<h5 id="xphiadbc">xPHI(a,d,b,c)</h5>
<p><code>(((A*D-B*C)/(√((A+C)*(B+D)*(A+B)*(C+D)))))^2*(A+B+C+D)</code></p>
<h4 id="z-1">Z</h4>
<h5 id="z4fabcd">z4F(a,b,c,d)</h5>
<p><code>(D-(A+B+C+D)*((D+B)*(C+D)/(A+B+C+D)^2))/(√((A+B+C+D)*(1-((D+B)*(C+D)/(A+B+C+D)^2))-(A+B+C+D)*(A+B+C+D-1)*((D+B)*(C+D)/(A+B+C+D)^2)*(((D+B)*(C+D)/(A+B+C+D)^2)-((D+B-1)*(C+D-1)/(A+B+C+D-1)^2))))</code></p>
<h5 id="zbnab">zBN(a,b)</h5>
<p><code>(A-(A+B)/2)/(√((A+B)/4))</code></p>
<h5 id="zrr">Zr(r)</h5>
<p><code>0.5*LN((1+A)/(1-A))</code></p>
<h5 id="zrbisl1l2">zrbis(L1,L2)</h5>
<p><code>(((mean(L1)-mean(L2))/stddev(CONCAT(L1,L2)))*SIZE(L1)*SIZE(L2)/((1/(√(2*π)))*e^(-(NORMALD_ICDF((SIZE(L2)/SIZE(CONCAT(L1,L2))))^2)/2)*SIZE(CONCAT(L1,L2))^2))/(√(SIZE(L1)*SIZE(L2))/((√SIZE(CONCAT(L1,L2))*SIZE(CONCAT(L1,L2))*1/(√(2*π)))*e^(-(NORMALD_ICDF((SIZE(L2)/SIZE(CONCAT(L1,L2))))^2)/2)))</code></p>
<h5 id="zrbisrl1l2">zrbisR(L1,L2)</h5>
<p><code>(size(L1)*size(L2)+(((size(L1))^2+size(L1))/2)-ΣLIST(L1)-size(L1)*size(L2)/2)/(sqrt(size(L1)*size(L2)*(size(L1)+size(L2)+1)/12))</code></p>
<h5 id="zrrr1r2n1n2">zrr(r1,r2,n1,n2)</h5>
<p><code>(0.5*LN((1+A)/(1-A))-0.5*LN((1+B)/(1-B)))/(√(1/(C-3)+1/(D-3)))</code></p>
<h5 id="zrxy_zrxy_zn">zrxy_z(rxy_z,n)</h5>
<p><code>0.5*LN(((1+A)/(1-A)))*√(B-2)</code></p>
<h5 id="zwertx1xs">ZWERT(x1,x,s)</h5>
<p><code>(A-B)/C</code></p>
<h5 id="zvall1egl2zvall1">zVAL(L1),e.g.L2:=zVAL(L1)</h5>
<p><code>approx(MAKELIST(((L1(X)-mean(L1))/stddev(L1)),X,1,SIZE(L1)))</code></p>
<h5 id="zvalpl1egl3zvall1">zVALp(L1),e.g.L3:=zVAL(L1)</h5>
<p><code>approx(MAKELIST(((L1(X)-mean(L1))/stddevp(L1)),X,1,SIZE(L1)))</code></p>
<br>
<br>
<h3 id="application-functions">Application functions</h3>
<h4 id="function">Function</h4>
<h5 id="f01pas">F01.pas</h5>
<p><code>//F01()/D.G.SCHRAUSSER/2022</code><br />
<code>//Function: Equations 1.0</code><br />
<code>EXPORT F01()</code><br />
<code>BEGIN</code><br />
<code>&quot;√(1-((X-W)/A)^2)*A+V&quot;▶F1;</code><br />
<code>&quot;−√(1-((X-W)/A)^2)*A+V&quot;▶F2;</code><br />
<code>&quot;√(1-((X-T)/B)^2)*B+U&quot;▶F3;</code><br />
<code>&quot;-√(1-((X-T)/B)^2)*B+U&quot;▶F4;</code><br />
<code>&quot;√(1-((X-R)/C)^2)*C+S&quot;▶F5;</code><br />
<code>&quot;-√(1-((X-R)/C)^2)*C+S&quot;▶F6;</code><br />
<code>200▶A;</code><br />
<code>150▶B;</code><br />
<code>344▶C;</code><br />
<code>1▶W;</code><br />
<code>450▶T;</code><br />
<code>1000▶R;</code><br />
<code>&quot;Function: Equations 1.0&quot;</code><br />
<code>END;</code><br />
<code>//</code></p>
<h5 id="f02pas">F02.pas</h5>
<p><code>//F02()/D.G.SCHRAUSSER/2022</code><br />
<code>//Function: Equations 2.0</code><br />
<code>EXPORT F02()</code><br />
<code>BEGIN</code><br />
<code>&quot;NORMALD_CDF(0,1,X)&quot;▶F1;</code><br />
<code>&quot;NORMALD(0,1,X)&quot;▶F2;</code><br />
<code>&quot;STUDENT(50,X)&quot;▶F3;</code><br />
<code>&quot;STUDENT_CDF(50,X)&quot;▶F4;</code><br />
<code>&quot;CHISQUARE(1,X)&quot;▶F5;</code><br />
<code>&quot;CHISQUARE_CDF(1,X)&quot;▶F6;</code><br />
<code>&quot;FISHER_CDF(25,3,X)&quot;▶F7;</code><br />
<code>&quot;FISHER(25,3,X)&quot;▶F8;</code><br />
<code>&quot;0&quot;▶F9;</code><br />
<code>&quot;0&quot;▶F0;</code><br />
<code>&quot;Function: Equations 2.0&quot;</code><br />
<code>END;</code><br />
<code>//</code></p>
<h5 id="f03pas">F03.pas</h5>
<p><code>//F03()/D.G.SCHRAUSSER/2025</code><br />
<code>//Function: Equations 3.0</code><br />
<code>//F3-7:Derivatives of the standard normal distribution function, f'(z)-f'''''(z)</code><br />
<code>EXPORT F03()</code><br />
<code>BEGIN</code><br />
<code>&quot;NORMALD_CDF(0,1,X)&quot;▶F1;</code><br />
<code>&quot;NORMALD(0,1,X)&quot;▶F2;</code><br />
<code>&quot;∂((1/√(2*π))*e^((-1/2)*X^2),X=X)&quot;▶F3;</code><br />
<code>&quot;∂(∂((1/√(2*π))*e^((-1/2)*X^2),X),X)&quot;▶F4;</code><br />
<code>&quot;∂(∂(∂((1/√(2*π))*e^((-1/2)*X^2),X),X),X)&quot;▶F5;</code><br />
<code>&quot;∂(∂(∂(∂((1/√(2*π))*e^((-1/2)*X^2),X),X),X),X)&quot;▶F6;</code><br />
<code>&quot;∂(∂(∂(∂(∂((1/√(2*π))*e^((-1/2)*X^2),X),X),X),X),X)&quot;▶F7;</code><br />
<code>&quot;0&quot;▶F8;</code><br />
<code>&quot;0&quot;▶F9;</code><br />
<code>&quot;0&quot;▶F0;</code><br />
<code>&quot;Function: Equations 3.0&quot;</code><br />
<code>END;</code><br />
<code>//</code></p>
<h5 id="f04pas">F04.pas</h5>
<p><code>//F04()/D.G.SCHRAUSSER/2025</code><br />
<code>//Function: Equations 4.0</code><br />
<code>//F3:Derivative of Gamma, f'(x)</code><br />
<code>//F5-7:Derivatives of the exponential function, f(x)={f'(x)-f'''(x)...}</code><br />
<code>EXPORT F04()</code><br />
<code>BEGIN</code><br />
<code>&quot;CAS.Gamma(X)&quot;▶F1;</code><br />
<code>&quot;(X)!&quot;▶F2;</code><br />
<code>&quot;∂(Gamma(X),X=X)&quot;▶F3;</code><br />
<code>&quot;EXP(X)&quot;▶F4;</code><br />
<code>&quot;∂(e^X,X = X)&quot;▶F5;</code><br />
<code>&quot;∂(∂(e^X,X=X),X=X)&quot;▶F6;</code><br />
<code>&quot;∂(∂(∂(e^X,X=X),X=X),X=X)&quot;▶F7;</code><br />
<code>&quot;0&quot;▶F8;</code><br />
<code>&quot;0&quot;▶F9;</code><br />
<code>&quot;0&quot;▶F0;</code><br />
<code>&quot;Function: Equations 4.0&quot;</code><br />
<code>END;</code><br />
<code>//</code></p>
<h5 id="f05pas">F05.pas</h5>
<p><code>//F05()/D.G.SCHRAUSSER/2025</code><br />
<code>//Function: Equations 5.0</code><br />
<code>//F2-5,F7-0:Derivatives of the circular function, f'(t)-f''''(t)</code><br />
<code>EXPORT F05()</code><br />
<code>BEGIN</code><br />
<code>&quot;√(1-X^2)&quot;▶F1;</code><br />
<code>&quot;∂(√(1-X^2),X)&quot;▶F2;</code><br />
<code>&quot;∂(∂(√(1-X^2),X),X)&quot;▶F3;</code><br />
<code>&quot;∂(∂(∂(√(1-X^2),X),X),X)&quot;▶F4;</code><br />
<code>&quot;∂(∂(∂(∂(√(1-X^2),X),X),X),X)&quot;▶F5;</code><br />
<code>&quot;-√(1-X^2)&quot;▶F6;</code><br />
<code>&quot;∂(-√(1-X^2),X)&quot;▶F7;</code><br />
<code>&quot;∂(∂(-√(1-X^2),X),X)&quot;▶F8;</code><br />
<code>&quot;∂(∂(∂(-√(1-X^2),X),X),X)&quot;▶F9;</code><br />
<code>&quot;∂(∂(∂(∂(-√(1-X^2),X),X),X),X)&quot;▶F0;</code><br />
<code>&quot;Function: Equations 5.0&quot;</code><br />
<code>END;</code><br />
<code>//</code></p>
<h5 id="f06pas">F06.pas</h5>
<p><code>//F06()/D.G.SCHRAUSSER/2025</code><br />
<code>//Function: Equations 6.0</code><br />
<code>//F3-6:Derivatives of Student's-t, f'(t)-f''''(t)</code>
<code>EXPORT F06()</code><br />
<code>BEGIN</code><br />
<code>&quot;STUDENT_CDF(D,X)&quot;▶F1;</code><br />
<code>&quot;(Gamma(((D+1)/2))/Gamma((D/2)))*(D*π)^(-1/2)*(1+(X^2/D))^(-(D+1)/2)&quot;▶F2;</code><br />
<code>&quot;∂(G*(D*π)^(-1/2)*(1+(X^2/D))^(-(D+1)/2),X)&quot;▶F3;</code><br />
<code>&quot;∂(∂(G*(D*π)^(-1/2)*(1+(X^2/D))^(-(D+1)/2),X),X)&quot;▶F4;</code><br />
<code>&quot;∂(∂(∂(G*(D*π)^(-1/2)*(1+(X^2/D))^(-(D+1)/2),X),X),X)&quot;▶F5;</code><br />
<code>&quot;∂(∂(∂(∂(G*(D*π)^(-1/2)*(1+(X^2/D))^(-(D+1)/2),X),X),X),X)&quot;▶F6;</code><br />
<code>&quot;&quot;▶F7;</code><br />
<code>&quot;&quot;▶F8;</code><br />
<code>&quot;&quot;▶F9;</code><br />
<code>&quot;&quot;▶F0;</code><br />
<code>&quot;Function: Equations 6.0&quot;</code><br />
<code>END;</code><br />
<code>//</code></p>
<h5 id="f06_pas">F06_.pas</h5>
<pre><code>//F06_(df)/D.G.SCHRAUSSER/2025
//Derivatives of Student's-t
//e.g.F06_(5)[F06]
#cas
F06_(DF):=
BEGIN
F06
DF▶D
G=Gamma(((DF+1)/2))/Gamma((DF/2))
D,G
END;
#end
//
</code></pre>
<h5 id="f07pas">F07.pas</h5>
<p><code>//F07()/D.G.SCHRAUSSER/2025</code><br />
<code>//Function: Equations 7.0</code><br />
<code>//F3-6:Derivatives of chi², f'(t)-f''''(t)</code><br />
<code>EXPORT F07()</code><br />
<code>BEGIN</code><br />
<code>&quot;CHISQUARE_CDF(D,X)&quot;▶F1;</code><br />
<code>&quot;(1/(2^(D/2)*G))*X^((D/2)-1)*e^(-X/2)&quot;▶F2;</code><br />
<code>&quot;∂((1/(2^(D/2)*G))*X^((D/2)-1)*e^(-X/2),X)&quot;▶F3;</code><br />
<code>&quot;∂(∂((1/(2^(D/2)*G))*X^((D/2)-1)*e^(-X/2),X),X)&quot;▶F4;</code><br />
<code>&quot;∂(∂(∂((1/(2^(D/2)*G))*X^((D/2)-1)*e^(-X/2),X),X),X)&quot;▶F5;</code><br />
<code>&quot;∂(∂(∂(∂((1/(2^(D/2)*G))*X^((D/2)-1)*e^(-X/2),X),X),X),X)&quot;▶F6;</code><br />
<code>&quot;&quot;▶F7;</code><br />
<code>&quot;&quot;▶F8;</code><br />
<code>&quot;&quot;▶F9;</code><br />
<code>&quot;&quot;▶F0;</code><br />
<code>&quot;Function: Equations 7.0&quot;</code><br />
<code>END;</code><br />
<code>//</code></p>
<h5 id="f07_pas">F07_.pas</h5>
<pre><code>//F07_(df)/D.G.SCHRAUSSER/2025
//Derivatives of chi²
//e.g.F07_(1)[F07]
#cas
F07_(DF):=
BEGIN
F07
DF▶D
G=Gamma(DF/2)
D,G
END;
#end
//
</code></pre>
<br>
<h4 id="graph-3d">Graph 3D</h4>
<h5 id="f01zpas">F01Z.pas</h5>
<p><code>//F01Z()/D.G.SCHRAUSSER/2025</code><br />
<code>//Graph 3D: Equations 1.0</code><br />
<code>//FZ1-2:Gamma</code><br />
<code>//FZ4-9:Spherical functions</code><br />
<code>//FZ0:Sine</code><br />
<code>EXPORT F01Z()</code><br />
<code>BEGIN</code><br />
<code>&quot;CAS.Gamma(Y)&quot;▶FZ1;</code><br />
<code>&quot;X^(Y-1)*e^(-X)&quot;▶FZ2;</code><br />
<code>&quot;(1/(2*π*√(1-R^2)))*e^((-1/(2*(1-R^2)))*(X^2-2*R*X*Y+Y^2))&quot;▶FZ3;</code><br />
<code>&quot;√((1-X^2)+(1-Y^2))&quot;▶FZ4;</code><br />
<code>&quot;−1*√((1-X^2)+(1-Y^2))&quot;▶FZ5;</code><br />
<code>&quot;√(1-X^2-Y^2)&quot;▶FZ6;</code><br />
<code>&quot;−1*√(1-X^2-Y^2)&quot;▶FZ7;</code><br />
<code>&quot;√((X-X^2)+(Y-Y^2))&quot;▶FZ8;</code><br />
<code>&quot;−1*√((X-X^2)+(Y-Y^2))&quot;▶FZ9;</code><br />
<code>&quot;SIN(X)*SIN(Y)*1.5&quot;▶FZ0;</code><br />
<code>&quot;Graph 3D: Equations 1.0&quot;</code><br />
<code>END;</code><br />
<code>//</code></p>
<h5 id="f02zpas">F02Z.pas</h5>
<p><code>//F02Z()/D.G.SCHRAUSSER/2025</code><br />
<code>//Graph 3D: Equations 2.0</code><br />
<code>//Complex plane f(z)=z,</code><br />
<code>//with z=|x+i|</code><br />
<code>//where f(x,y=i)=√x²+y²</code><br />
<code>EXPORT F02Z()</code><br />
<code>BEGIN</code><br />
<code>&quot;√(X^2+Y^2)&quot;▶FZ1;</code><br />
<code>&quot;(1/π)*e^(-(ABS(√(X^2+Y^2))^2))&quot;▶FZ2;</code><br />
<code>&quot;X^((√(X^2+Y^2))-1)*e^(-X)&quot;▶FZ3;</code><br />
<code>&quot;&quot;▶FZ4;</code><br />
<code>&quot;&quot;▶FZ5;</code><br />
<code>&quot;&quot;▶FZ6;</code><br />
<code>&quot;&quot;▶FZ7;</code><br />
<code>&quot;&quot;▶FZ8;</code><br />
<code>&quot;&quot;▶FZ9;</code><br />
<code>&quot;&quot;▶FZ0;</code><br />
<code>&quot;Graph 3D: Equations 2.0&quot;</code><br />
<code>END;</code><br />
<code>//</code></p>
<h5 id="f03zpas">F03Z.pas</h5>
<p><code>//F03Z()/D.G.SCHRAUSSER/2025</code><br />
<code>//Graph 3D: Equations 3.0</code><br />
<code>//FZ1:Student's-t surface, f(t,df),x[-4,4],y[1,10],z[0,0.5]</code><br />
<code>//FZ2:chi² surface, f(chi²,df), x[0,5],y[1,10],z[0,0.5]</code><br />
<code>//FZ3-6:F space, f(F,df2);df1={1,5,9,13},x[0,5],y[0,20],z[0,5]</code><br />
<code>EXPORT F03Z()</code><br />
<code>BEGIN</code><br />
<code>&quot;(Gamma(((Y+1)/2))/Gamma((Y/2)))*(Y*π)^(-1/2)*(1+(X^2/Y))^(-(Y+1)/2)&quot;▶FZ1;</code><br />
<code>&quot;(1/(2^(Y/2)*Gamma((Y/2))))*X^((Y/2)-1)*e^(-X/2)&quot;▶FZ2;</code><br />
<code>&quot;(Gamma(((1+Y)/2))/(Gamma((1/2))*Gamma((1/2))))*(1/Y)^(1/2)*X^((1/2)-1)*(1+(1/Y)*X)^(-(1+Y)/2)&quot;▶FZ3;</code><br />
<code>&quot;(Gamma(((5+Y)/2))/(Gamma((5/2))*Gamma((5/2))))*(5/Y)^(5/2)*X^((5/2)-1)*(1+(5/Y)*X)^(-(5+Y)/2)&quot;▶FZ4;</code><br />
<code>&quot;(Gamma(((9+Y)/2))/(Gamma((9/2))*Gamma((9/2))))*(9/Y)^(9/2)*X^((9/2)-1)*(1+(9/Y)*X)^(-(9+Y)/2)&quot;▶FZ5;</code><br />
<code>&quot;(Gamma(((13+Y)/2))/(Gamma((13/2))*Gamma((13/2))))*(13/Y)^(13/2)*X^((13/2)-1)*(1+(13/Y)*X)^(-(13+Y)/2)&quot;▶FZ6;</code><br />
<code>&quot;&quot;▶FZ7;</code><br />
<code>&quot;&quot;▶FZ8;</code><br />
<code>&quot;&quot;▶FZ9;</code><br />
<code>&quot;&quot;▶FZ0;</code><br />
<code>&quot;Graph 3D: Equations 3.0&quot;</code><br />
<code>END;</code><br />
<code>//</code></p>
<br>
<h4 id="solve">Solve</h4>
<h5 id="e01pas">E01.pas</h5>
<p><code>//E01()/D.G.SCHRAUSSER/2025</code><br />
<code>//Solve: Equations 1.0</code><br />
<code>//E1: Additive probability (P=p,X=pb,N=n; special addition theorem)</code><br />
<code>//E2: Negative binomial probability (P=p,N=pnb,R=n,K=1,2..,I=0.0)</code><br />
<code>//E3: Binomial probability (b,c)</code><br />
<code>//E4: Standard normal distribution z(0,1)</code><br />
<code>//E5: Effect size epsilon</code><br />
<code>//E6: Linear regression y'</code><br />
<code>//E7: Standard error of prediction y'+-C with zcrit</code><br />
<code>//E8: p of correlation r with n (2-tailed sig p2=2*(1-p);p&gt;0.5)</code><br />
<code>EXPORT E01()</code><br />
<code>BEGIN</code><br />
<code>&quot;X=1-(1-P)^N&quot;▶E1;</code><br />
<code>&quot;P=Σ(((A+B)!/(I!*(A+B-I)!))*2^(-I)*2^(-(A+B-I)),I,0,A)&quot;▶E3;</code><br />
<code>&quot;N=Σ(((K+I-1)!/(I!*(K-1)!))*P^K*(1-P)^I,I,0,R-K)&quot;▶E2;</code><br />
<code>&quot;P=NORMALD_CDF(0,1,Z)&quot;▶E4;</code><br />
<code>&quot;E=(A-X)/S&quot;▶E5;</code><br />
<code>&quot;Y=A*X+B&quot;▶E6;</code><br />
<code>&quot;C=(Z*√(1-R^2))*S&quot;▶E7;</code><br />
<code>&quot;P=STUDENT_CDF(N-2,(R*√(N-2))/√(1-R^2))&quot;▶E8;</code><br />
<code>&quot;&quot;▶E9;</code><br />
<code>&quot;&quot;▶E0;</code><br />
<code>&quot;Solve: Equations 1.0&quot;</code><br />
<code>END;</code><br />
<code>//</code></p>
<h5 id="e02pas">E02.pas</h5>
<pre><code>//E02()/D.G.SCHRAUSSER/2022
//Solve: Equations 2.0
//E1: Aperture value A for exposure value E and shutter speed T
//E2: Exposure value E for illuminance lux L and  ISO I
//E3: Magnification M at focal length F
//E4: Angle of view W at focal length F
EXPORT E02()
BEGIN
&quot;A=1/√(2^(-E)*T)&quot;▶E1;
&quot;E=LN((L*I/250))*(LN(2))^(-1)&quot;▶E2;
&quot;M=F/50&quot;▶E3;
&quot;W=-0.95908335982/(1-1.00293098572*e^(3.51450126836ᴇ−4*F))&quot;▶E4;
&quot;&quot;▶E5;
&quot;&quot;▶E6;
&quot;&quot;▶E7;
&quot;&quot;▶E8;
&quot;&quot;▶E9;
&quot;&quot;▶E0;
&quot;Solve: Equations 2.0&quot;
END;
//
</code></pre>
<h5 id="e03pas">E03.pas</h5>
<pre><code>//E03()/D.G.SCHRAUSSER/2025
//Solve: Equations 3.0
//E1: Resolution R from R0, f0, f1
//E2: Exposure Value E from Tv, Av
//E3: Aperture B from Av0, ISO0, ISO1
EXPORT E03()
BEGIN
&quot;(A*B^2/F^2)=R&quot;▶E1;
&quot;(LN(2))^(-1)*LN(T*A^2)=E&quot;▶E2;
&quot;A*e^(0.5*LN(S^(-1)*I))=B&quot;▶E3;
&quot;&quot;▶E4;
&quot;&quot;▶E5;
&quot;&quot;▶E6;
&quot;&quot;▶E7;
&quot;&quot;▶E8;
&quot;&quot;▶E9;
&quot;&quot;▶E0;
&quot;Solve: Equations 3.0&quot;
END;
//
</code></pre>
<h5 id="e04pas">E04.pas</h5>
<pre><code>//E04()/D.G.SCHRAUSSER/2025
//Solve: Equations 4.0
//E1: Astronomical unit A from meters M
//E2: Parsec P from astronomical unit A
//E3: Parsec P from parallax X in milliarcseconds mas
//E4: Light-year L from parsec P
//E5: Speed of light C from m/c M
//E6: Luminosity distance P from distance modulus M
//E7: Radius R at a given distance D with angular diameter V°
//E8: Illuminance in lux E from apparent magnitude M
EXPORT E04()
BEGIN
&quot;A=M/149597870700&quot;▶E1;
&quot;P=(648000/π)*A&quot;▶E2;
&quot;P=1/(X/1000)&quot;▶E3;
&quot;L=P*3.26156&quot;▶E4;
&quot;C=299792458*M&quot;▶E5;
&quot;P=10^((M/5)+1)&quot;▶E6;
&quot;R=D*TAN(((π/180)*V/2))&quot;▶E7;
&quot;E=10^((-14.18-M)/2.5)&quot;▶E8;
&quot;&quot;▶E9;
&quot;&quot;▶E0;
&quot;Solve: Equations 4.0&quot;
END;
//
</code></pre>
<br>
<h3 id="data">Data</h3>
<h4 id="functions-1">Functions</h4>
<p><code>//C2V(k[2..5])//D.G.SCHRAUSSER/2025</code><br />
<code>//e.g.C2V(3)</code><br />
<code>//from L1()..L5() to L6()(k)</code><br />
<code>#cas</code><br />
<code>C2V(K):=</code><br />
<code>BEGIN</code><br />
<code>size(L1)▶N</code><br />
<code>{}▶L0</code><br />
<code>IF K&lt;6 THEN</code><br />
<code>FOR I FROM 1 TO N DO</code><br />
<code>L1(I)▶L0(1)</code><br />
<code>L2(I)▶L0(2)</code><br />
<code>IF K&gt;2 THEN L3(I)▶L0(3) END;</code><br />
<code>IF K&gt;3 THEN L4(I)▶L0(4) END;</code><br />
<code>IF K&gt;4 THEN L5(I)▶L0(5) END;</code><br />
<code>//</code><br />
<code>L0▶L6(I)</code><br />
<code>END;</code><br />
<code>END;</code><br />
<code>L6▶M1</code><br />
<code>L6</code><br />
<code>END;</code><br />
<code>#end</code><br />
<code>//</code></p>
<p>//RNDNR(cases n)/D.G.SCHRAUSSER/2025<code>//e.g.RNDNR(100)</code><br />
<a class="hashtag" href="#" onclick="lookUpHashTag(this);">#cas</a><code>RNDNR(N):=</code><br />
BEGIN<code>MAKELIST(RANDOM(),L,0,N-1)▶L7</code><br />
MAKELIST(RANDNORM(0,1),L,0,N-1)▶L8<code>SORT(L7)▶L7</code><br />
SORT(L8)▶L8<code>L8</code><br />
END;<code>#end</code><br />
//`</p>
<p>//V2C()//D.G.SCHRAUSSER/2025<code>//from L1()(2) to L1() L2()</code><br />
<a class="hashtag" href="#" onclick="lookUpHashTag(this);">#cas</a><code>V2C():=</code><br />
BEGIN<code>L1▶M1</code><br />
TRN(M1)▶L3<code>L3(1)▶L1</code><br />
L3(2)▶L2<code>{}▶L3</code><br />
L1,L2<code>END;</code><br />
<a class="hashtag" href="#" onclick="lookUpHashTag(this);">#end</a><code>//</code></p>
<h4 id="lists">Lists</h4>
<h5 id="r-2">R</h5>
<p><code>{2.,9.,6.,6.,3.}▶L1</code><br />
<code>{5.,9.,6.,3.,5.}▶L2</code><br />
<code>{6.,5.,2.,8.,3.}▶L3</code></p>
<p><code>{{3.,1.},{2.,1.},{1.,1.},{4.,4.},{6.,4.},{5.,4.}}▶L1</code></p>
<p><code>{{5.,8.,5.,8.,5.},{9.,8.,5.,0.,0.},{2.,3.,5.,8.,9.},{1.,5.,8.,9.,6.},{5.,8.,7.,5.,5.},{6.,2.,3.,1.,6.},{9.,5.,8.,2.,5.},{9.,6.,9.,5.,8.},{4.,9.,6.,5.,8.},{3.,6.,5.,8.,9.},{6.,5.,5.,6.,5.},{2.,5.,8.,9.,6.},{5.,5.,5.,8.,5.},{6.,9.,5.,8.,8.},{4.,9.,5.,6.,5.}}▶L1</code></p>
<h5 id="rbisr">rbisR</h5>
<p><code>{1.,2.,3.,5.,8.,9.,12.,13.,14.,18.,19.}▶L1</code><br />
<code>{4.,6.,7.,10.,11.,15.,16.,17.,20.}▶L2</code></p>
<h5 id="rho">Rho</h5>
<p><code>{{1.,5.},{2.,4.},{3.,7.},{4.,2.},{5.,1.},{6.,3.},{7.,10.},{8.,9.},{9.,8.},{10.,6.}}▶L1</code></p>
<h5 id="pkrrpbis">PKR,rpbis</h5>
<p><code>{8.,9.,6.,5.,4.,7.,8.,5.,6.,9.}▶L1</code><br />
<code>{8.,9.,6.,5.,8.,9.,6.,5.,3.,5.}▶L2</code><br />
<code>{4.,8.,5.,9.,9.,5.,8.,2.,6.,2.}▶L3</code></p>
<h5 id="rpbis-vs-r-correlationl1l2">rpbis vs r [correlation(L1,L2)]</h5>
<p><code>{8.,9.,6.,5.,4.,7.,8.,5.,6.,9.,8.,9.,6.,5.,8.,9.,6.,5.,3.,5.}▶L1</code><br />
<code>{2.,2.,2.,2.,2.,2.,2.,2.,2.,2.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.}▶L2</code></p>
<h5 id="tau-a">Tau-a</h5>
<p><code>{1.,2.,3.,4.,5.,6.,7.,8.,9.,10.,11.,12.}▶L1</code><br />
<code>{1.,2.,3.,5.,4.,7.,6.,8.,10.,9.,11.,12.}▶L2</code></p>
<p><code>{1.,2.,3.,4.,5.,6.,7.,8.,9.,10.,11.,12.}▶L1</code><br />
<code>{1.,2.,4.,3.,6.,5.,8.,7.,10.,9.,12.,11.}▶L2</code></p>
<h5 id="d-01">D [0,1]</h5>
<p><code>{0.,0.,0.,0.,0.,0.}▶L1</code><br />
<code>{0.,0.,0.,0.,0.,0.}▶L2</code></p>
<p><code>{1.,1.,1.,1.,1.,1.}▶L1</code><br />
<code>{1.,1.,1.,1.,1.,1.}▶L2</code></p>
<p><code>{0.,0.,0.,1.,1.,1.}▶L1</code><br />
<code>{1.,1.,1.,0.,0.,0.}▶L2</code></p>
<p><code>{1.,0.,0.,1.,0.,1.}▶L1</code><br />
<code>{0.,1.,0.,1.,0.,0.}▶L2</code></p>
<p><code>{1.,0.,0.,1.,0.,1.,1.,1.,1.,0.,0.,1.,0.,0.,0.,1.,1.,0.,1.,0.,0.,0.,0.,0.,1.}▶L1</code><br />
<code>{0.,1.,0.,1.,0.,0.,1.,1.,1.,1.,0.,0.,1.,1.,0.,1.,1.,1.,0.,1.,1.,1.,0.,0.,1.}▶L2</code></p>
<h5 id="pv_">PV_</h5>
<p><code>{{1.,4.},{2.,6.},{2.,7.}}▶L1</code><br />
(Scambor, <a href="https://doi.org/10.13140/RG.2.2.28632.06405">1997</a>; Scambor & Schrausser, <a href="https://www.academia.edu/94993376">2022</a>, p. 7)</p>
<p><code>{{8.,9.},{2.,3.},{9.,7.},{2.,7.},{9.,8.}}▶L1</code></p>
<p><code>{{8.6,4.5},{9.2,4.1},{6.7,1.2},{9.6,4.5},{6.2,7.6},{6.1,8.5},{9.3,4.2},{7.3,8.2}}▶L1</code></p>
<h5 id="pu_">PU_</h5>
<p><code>{18.,30.,54.}▶L1</code><br />
<code>{6.,12.}▶L2</code><br />
(Schrausser, <a href="https://doi.org/10.13140/RG.2.2.24500.32640/1">1996</a>, <a href="https://www.academia.edu/82224369">2022b</a>, p. 2)</p>
<p><code>{18.04,10.07,22.27,1.96,18.88,12.81,3.08,21.49,1.96,24.93}▶L1</code><br />
<code>{24.96,22.39}▶L2</code><br />
(Schrausser, <a href="https://doi.org/10.13140/RG.2.2.14805.91369">1997</a>, <a href="https://doi.org/10.13140/RG.2.2.19532.69768">1998a</a>)</p>
<br>
<br>

<h2 id="references">References</h2>
<p>Abraham Bar Hiyya Savasorda, &amp; et al. (1450). <em>Ma’aseh Hoshev</em>. Retrieved from the Library of Congress. <a href="https://www.loc.gov/item/2021667539/">https://www.loc.gov/item/2021667539/</a></p>
<p>Agresti, A. (1992). A Survey of Exact Inference for Contingency Tables. <em>Statistical Science 7</em> (1): 131–53. <a href="https://doi.org/10.1214/ss/1177011454">https://doi.org/10.1214/ss/1177011454</a></p>
<p>Allbright, G. S. (1991). Emulsion Speed Rating Systems. <em>The Journal of Photographic Science 39</em> (2): 95–99. <a href="https://doi.org/10.1080/00223638.1991.11737126">https://doi.org/10.1080/00223638.1991.11737126</a></p>
<p>Alten, H. -W., Naini, A. D., Eick, B., Folkerts, M., Schlosser, H., Schlote, K. -H., Wesemüller-Kock, H., &amp; Wussing, H. (2014). Algebra Im Europa Des Mittelalters Und Der Renaissance. In <em>4000 Jahre Algebra: Geschichte – Kulturen – Menschen</em>, 207–63. Berlin, Heidelberg: Springer. <a href="https://doi.org/10.1007/978-3-642-38239-0_4">https://doi.org/10.1007/978-3-642-38239-0_4</a></p>
<p>Anglin, W. S., &amp; Lambek, J. (1995). Mathematics in the Renaissance. In <em>The Heritage of Thales</em>, 125–31. New York, NY: Springer. <a href="https://doi.org/10.1007/978-1-4612-0803-7_25">https://doi.org/10.1007/978-1-4612-0803-7_25</a></p>
<p>Argand, R. (1813). Essai sur une manière de représenter les quantités imaginaires dans les constructions géométriques. <em>Annales de Mathématiques Pures Et Appliquées 4</em>: 133–47. <a href="https://fr.wikisource.org/wiki/Annales_de_math%C3%A9matiques_pures_et_appliqu%C3%A9es/Tome_04/Philosophie_math%C3%A9matique,_article_4">https://fr.wikisource.org/wiki/Annales_de_math%C3%A9matiques_pures_et_appliqu%C3%A9es/Tome_04/Philosophie_math%C3%A9matique,_article_4</a></p>
<p>———. (1874). <em>Essai sur une manière de représenter les quantités imaginaires dans les constructions géométriques. Précédée d’une préface par M. J. Hoüel, et suivie d’un appendice contenant des extraits des &quot;Annales de Gergonne&quot;, relatifs à la question des imaginaires</em>. 2nd ed. Paris: Gauthier-Villars. <a href="http://catalogue.bnf.fr/ark:/12148/cb300261909">http://catalogue.bnf.fr/ark:/12148/cb300261909</a></p>
<p>Arnauld, A., &amp; Nicole, P. (1662). <em>La logique ou L’art de penser</em>. 1st ed. A Paris: Chez Charles Savreux, au pied de la Tour de Nostre Dame. <a href="https://gallica.bnf.fr/ark:/12148/bpt6k574432.image">https://gallica.bnf.fr/ark:/12148/bpt6k574432.image</a></p>
<p>———. (1682). <em>Logica Sive Ars Cogitandi: In Qua Praeter Vulgares Regulas Plura Nova Habentur Ad Rationem Dirigendam Utilia</em>. Editio optima &amp; ultima. Lugduni Batavorum: Apud Jacobum Gaal. <a href="https://books.google.com/books?id=XQVaAAAAcAAJ">https://books.google.com/books?id=XQVaAAAAcAAJ</a></p>
<p>———. (1693). <em>Logic: Or, the Art of Thinking: In Which Besides the Common, Are Contain’d Many Excellent New Rules, Very Profitable for Directing of Reason, and Acquiring of Judgment, in Things as Well Relating to the Instruction of a Man’s Self, as of Others. In Four Parts. I. Consistin of Reflections Upon Ideas, or Upon the First Operation of the Mind, Which Is Called Apprehension, &amp;c. II. Of Considerations of Men about Proper Judgments, &amp;c. III. Of the Nature and Various Kinds of Reasoning, &amp;c. IV. Treats of the Most Profitable Method for Demonstrating or Illustrating Any Truth to Which Is Added an Index to the Whole Book. For the Excellency of the Matter, Printed Many Times in French and Latin, and Now for Publick Good Translated into English by Several Hands</em>. 2nd ed. London: Printed by T. B. for John Taylor at the Ship at St. Paul’s Church Yard. <a href="https://archive.org/details/logicorartofthin00arnaiala">https://archive.org/details/logicorartofthin00arnaiala</a></p>
<p>Arnauld, A., Claire, P., Girbal, F., &amp; Nicole, P. (1970). <em>La Logique: Ou, l’art de Penser: Contenant, Outre Les Regles Communes, Plusieurs Observations Nouvelles, Propres a Former Le Jugement</em>. Edited by Nicole, P. Paris: Flammarion. <a href="https://philpapers.org/rec/ARNLLO-8">https://philpapers.org/rec/ARNLLO-8</a></p>
<p>Bayes, T., &amp; Price, R. (1763). An Essay Towards Solving a Problem in the Doctrine of Chances. By the Late Rev. Mr. Bayes, f. R. S. Communicated by Mr. Price, in a Letter to John Canton, a. M. F. R. s. <em>Philosophical Transactions (1683-1775) 53</em>: 370–418. <a href="http://www.jstor.org/stable/105741">http://www.jstor.org/stable/105741</a></p>
<p>Beasley, W. H., &amp; Rodgers, J. L. (2009). Resampling Methods. In <em>The Sage Handbook of Quantitative Methods in Psychology</em>, edited by Millsap, R. E., &amp; Maydeu-Olivares, A., 362–86. Thousand Oaks, California: Sage Publications Ltd. <a href="https://psycnet.apa.org/doi/10.4135/9780857020994.n16">https://psycnet.apa.org/doi/10.4135/9780857020994.n16</a></p>
<p>Bernoulli, D. (1729). Lettre XLVII. D. Bernoulli a Goldbach. St.-Petersbourg ce 6. octobre 1729. <a href="https://commons.m.wikimedia.org/wiki/File:DanielBernoulliLetterToGoldbach-1729-10-06.jpg">https://commons.m.wikimedia.org/wiki/File:DanielBernoulliLetterToGoldbach-1729-10-06.jpg</a></p>
<p>Bernoulli, J. (1713). <em>Ars conjectandi, opus posthumum. Accedit Tractatus de seriebus infinitis, et epistola gallicé scripta de ludo pilae reticularis</em>. Basileae: Impensis Thurnisiorum, Fratrum. <a href="https://www.e-rara.ch/zut/doi/10.3931/e-rara-9001">https://www.e-rara.ch/zut/doi/10.3931/e-rara-9001</a></p>
<p>Bochner, S. (1978). The Emergence of Analysis in the Renaissance and After. <em>Rice Institute Pamphlet - Rice University Studies 64</em> (2-3). <a href="https://hdl.handle.net/1911/63315">https://hdl.handle.net/1911/63315</a></p>
<p>Bonett, D. G., &amp; Price, R. M. (2005). Inferential Methods for the Tetrachoric Correlation Coefficient. <em>Journal of Educational and Behavioral Statistics 30</em> (2): 213–25. <a href="http://www.jstor.org/stable/3701350">http://www.jstor.org/stable/3701350</a></p>
<p>Borenstein, M., Rothstein, H., &amp; Cohen, J. (1997). Power and Precision : A Computer Program for Statistical Power Analysis and Confidence Intervals. <em>Computer Science</em>. <a href="https://www.semanticscholar.org/paper/Power-and-precision-%3A-a-computer-program-for-power-Borenstein-Rothstein/f379f13a460b01488c35aea408e355436dbae839">https://www.semanticscholar.org/paper/Power-and-precision-%3A-a-computer-program-for-power-Borenstein-Rothstein/f379f13a460b01488c35aea408e355436dbae839</a></p>
<p>Borenstein, M., Rothstein, H., Cohen, J., Schoenfeld, D., Berlin, J., &amp; Lakatos, E. (2001). <em>Power and Precision: A Computer Program for Statistical Power Analysis and Confidence Intervals</em>. Englewood, NJ: Biostat, Inc. <a href="https://books.google.com/books?id=tYg02XZBeNAC&amp;printsec=frontcover&amp;hl=de#v=onepage&amp;q&amp;f=false">https://books.google.com/books?id=tYg02XZBeNAC&amp;printsec=frontcover&amp;hl=de#v=onepage&amp;q&amp;f=false</a></p>
<p>Bortz, J., &amp; Schuster, C. (2010). <em>Statistik Für Human- Und Sozialwissenschaftler: Limitierte Sonderausgabe</em>. 7th ed. Springer-Lehrbuch. Berlin, Heidelberg: Springer. <a href="https://doi.org/10.1007/978-3-642-12770-0">https://doi.org/10.1007/978-3-642-12770-0</a></p>
<p>Bortz, J., &amp; Weber, R. (2005). <em>Statistik: Für Human- Und Sozialwissenschaftler</em>. 6th ed. Springer-Lehrbuch. Berlin, Heidelberg: Springer. <a href="https://doi.org/10.1007/b137571">https://doi.org/10.1007/b137571</a></p>
<p>Boyer, C. B. (1968). <em>A History of Mathematics</em>. 1st ed. New York: John Wiley &amp; Sons, Inc. <a href="https://archive.org/details/ahistoryofmathematicscarlbboyer1968_315_t">https://archive.org/details/ahistoryofmathematicscarlbboyer1968_315_t</a></p>
<p>Bravais, A. (1844). <em>Analyse Mathematique. Sur les probabilités des erreurs de situation d’un point</em>. Paris: Imprimerie Royale. <a href="https://books.google.com/books?id=7g_hAQAACAAJ">https://books.google.com/books?id=7g_hAQAACAAJ</a></p>
<p>Brown, M. B. (1977). Algorithm AS 116: The Tetrachoric Correlation and Its Asymptotic Standard Error. <em>Journal of the Royal Statistical Society. Series C (Applied Statistics) 26</em> (3): 343–51. <a href="http://www.jstor.org/stable/2346985">http://www.jstor.org/stable/2346985</a></p>
<p>Burckel, R. B. (2021). <em>Classical Analysis in the Complex Plane</em>. New York, NY: Springer. <a href="https://doi.org/10.1007/978-1-0716-1965-0">https://doi.org/10.1007/978-1-0716-1965-0</a></p>
<p>Cajori, F. (1919). Who Was the First Inventor of the Calculus? <em>The American Mathematical Monthly 26</em> (1): 15–20. <a href="http://www.jstor.org/stable/2974042">http://www.jstor.org/stable/2974042</a></p>
<p>Calinger, R. S. (2016). <em>Leonhard Euler: Mathematical Genius in the Enlightenment</em>. Princeton, New Jersey: Princeton University Press. <a href="http://www.jstor.org/stable/j.ctv7h0smb">http://www.jstor.org/stable/j.ctv7h0smb</a></p>
<p>Cardano, G. (1545a). <em>Ars magna or The Rules of Algebra</em>. New York: Dover (published 1993). <a href="https://archive.org/details/arsmagnaorruleso0000card">https://archive.org/details/arsmagnaorruleso0000card</a></p>
<p>———. (1545b). <em>Artis Magnae, Sive De Regvlis Algebraicis, Liber Vnvs</em>. S. P. D: Andreae Osiandro viro eruditiss. <a href="https://web.archive.org/web/20220201093634/http://www.filosofia.unimi.it/cardano/testi/operaomnia/vol_4_s_4.pdf">https://web.archive.org/web/20220201093634/http://www.filosofia.unimi.it/cardano/testi/operaomnia/vol_4_s_4.pdf</a></p>
<p>Cassirer, E. (1943). Newton and Leibniz. <em>The Philosophical Review 52</em> (4): 366–91. <a href="http://www.jstor.org/stable/2180670">http://www.jstor.org/stable/2180670</a></p>
<p>Cattell, R. B. (1966). The Scree Test for the Number of Factors. <em>Multivariate Behavioral Research 1</em> (2): 245–76. <a href="https://doi.org/10.1207/s15327906mbr0102_10">https://doi.org/10.1207/s15327906mbr0102_10</a></p>
<p>Cohen, J. (1977). <em>Statistical Power Analysis for the Behavioral Science</em>. Amsterdam: Elsevier Academic Press. <a href="https://doi.org/10.1016/C2013-0-10517-X">https://doi.org/10.1016/C2013-0-10517-X</a></p>
<p>———. (1988). <em>Statistical Power Analysis for the Behavioral Science</em>. 2nd ed. Hillsdale, NJ: Lawrence Erlbaum Associates. <a href="https://www.scirp.org/(S(lz5mqp453edsnp55rrgjct55))/reference/ReferencesPapers.aspx?ReferenceID=2041144">https://www.scirp.org/(S(lz5mqp453edsnp55rrgjct55))/reference/ReferencesPapers.aspx?ReferenceID=2041144</a></p>
<p>———. (1992). A Power Primer. <em>Psychological Bulletin 112</em> (1): 155–59. <a href="https://doi.org/10.1037/0033-2909.112.1.15">https://doi.org/10.1037/0033-2909.112.1.15</a></p>
<p>Collins, J. (1671). <em>Extracts from a letter from James Gregory to John Collins, 15 February 1671.</em> Cambridge: University Library. <a href="https://archivesearch.lib.cam.ac.uk/repositories/2/archival_objects/566767">https://archivesearch.lib.cam.ac.uk/repositories/2/archival_objects/566767</a></p>
<p>Cox, D. R., &amp; Hinkley, D. V. (1974). <em>Theoretical Statistics</em>. 1st ed. New York: Chapman; Hall/CRC. <a href="https://doi.org/10.1201/b14832">https://doi.org/10.1201/b14832</a></p>
<p>Dallal, G. E. (1986). STATOOLS: Statistical Utility Programs. <em>The American Statistician 40</em> (3): 236–36. <a href="http://www.jstor.org/stable/2684555">http://www.jstor.org/stable/2684555</a></p>
<p>———. (1988). PITMAN: A FORTRAN Program for Exact Randomization Tests. <em>Computers and Biomedical Research 21</em> (1): 9–15. <a href="https://doi.org/https://doi.org/10.1016/0010-4809(88)90037-7">https://doi.org/https://doi.org/10.1016/0010-4809(88)90037-7</a></p>
<p>De Moivre, A. (1711). De mensura sortis, seu, de probabilitate eventuum in ludis a casu fortuito pendentibus. <em>Philosophical Transactions of the Royal Society of London 27</em> (329): 213–64. <a href="https://doi.org/10.1098/rstl.1710.0018">https://doi.org/10.1098/rstl.1710.0018</a></p>
<p>———. (1718). <em>The Doctrine of Chances: Or, A Method of Calculating the Probability of Events in Play</em>. 1st ed. London: W. Pearson. <a href="https://books.google.com/books?id=3EPac6QpbuMC">https://books.google.com/books?id=3EPac6QpbuMC</a></p>
<p>———. (1738). <em>The Doctrine of Chances: Or, A Method of Calculating the Probability of Events in Play</em>. 2nd ed. London: H. Woodfall. <a href="https://books.google.com/books?id=PII_AAAAcAAJ">https://books.google.com/books?id=PII_AAAAcAAJ</a></p>
<p>Descartes, R. (1664). <em>La Géométrie</em>. A Paris: Chez Charles Angot, Libraire Iuré, ruë S. Iacques, au Lion d’Or. M. DC. LXIV. Avec Privilege du Roy. <a href="https://books.google.com/books?id=VtFcAAAAcAAJ">https://books.google.com/books?id=VtFcAAAAcAAJ</a></p>
<p>———. (2012). <em>The Geometry of René Descartes: With a Facsimile of the First Edition</em>. Dover Books on Mathematics. New York: Dover Publications. <a href="https://books.google.com/books?id=MB7F32p0y5MC">https://books.google.com/books?id=MB7F32p0y5MC</a></p>
<p>Dessì, P., &amp; Albury, W. R. (1997). Book Reviews. <em>History and Philosophy of Logic 18</em> (2): 121–22. <a href="https://doi.org/10.1080/01445349708837281">https://doi.org/10.1080/01445349708837281</a></p>
<p>Digby, P. G. N. (1983). Approximating the Tetrachoric Correlation Coefficient. <em>Biometrics 39</em> (3): 753–57. <a href="http://www.jstor.org/stable/2531104">http://www.jstor.org/stable/2531104</a></p>
<p>Divakaran, P. P. (2007). The First Textbook of Calculus: &quot;Yuktibhāṣā&quot;. <em>Journal of Indian Philosophy 35</em> (5/6): 417–43. <a href="http://www.jstor.org/stable/23497280">http://www.jstor.org/stable/23497280</a></p>
<p>Efron, B. (1979). Bootstrap Methods: Another Look at the Jackknife. <em>The Annals of Statistics 7</em> (1): 1–26. <a href="https://doi.org/10.1214/aos/1176344552">https://doi.org/10.1214/aos/1176344552</a></p>
<p>———. (1981). Nonparametric Estimates of Standard Error: The Jackknife, the Bootstrap and Other Methods. <em>Biometrika 68</em> (3): 589–99. <a href="https://doi.org/10.1093/biomet/68.3.589">https://doi.org/10.1093/biomet/68.3.589</a></p>
<p>———. (1982). <em>The Jackknife, the Bootstrap and Other Resampling Plans</em>. CBMS-NSF Regional Conference Series in Applied Mathematics, Monograph 38. Philadelphia: SIAM, Society for Industrial and Applied Mathematics. <a href="https://doi.org/10.1137/1.9781611970319">https://doi.org/10.1137/1.9781611970319</a></p>
<p>Edgington, E. S. (1964). Randomization Tests. <em>The Journal of Psychology: Interdisciplinary and Applied 57</em> (2): 445–49. <a href="https://doi.org/10.1080/00223980.1964.9916711">https://doi.org/10.1080/00223980.1964.9916711</a></p>
<p>———. (1980). Validity of Randomization Tests for One-Subject Experiments. <em>Journal of Educational Statistics 5</em> (3): 235–51. <a href="https://doi.org/10.2307/1164966">https://doi.org/10.2307/1164966</a></p>
<p>———. (1987). Randomized Single-Subject Experiments and Statistical Tests. <em>Journal of Counseling Psychology 34</em> (4): 437–42. <a href="https://doi.org/10.1037/0022-0167.34.4.437">https://doi.org/10.1037/0022-0167.34.4.437</a></p>
<p>———. (2011). Randomization Tests. In <em>International Encyclopedia of Statistical Science</em>, edited by Lovric, M., 1182–83. Berlin, Heidelberg: Springer. <a href="https://doi.org/10.1007/978-3-642-04898-2_56">https://doi.org/10.1007/978-3-642-04898-2_56</a></p>
<p>Edgington, E. S., &amp; Onghena, P. (2007). <em>Randomization Tests</em>. 4th ed. New York: Chapman and Hall/CRC. <a href="https://doi.org/10.1201/9781420011814">https://doi.org/10.1201/9781420011814</a></p>
<p>Elderton, W. P. (1902). Tables for Testing the Goodness of Fit of Theory to Observation. <em>Biometrika 1</em> (2): 155–63. <a href="https://doi.org/10.1093/biomet/1.2.155">https://doi.org/10.1093/biomet/1.2.155</a></p>
<p>Ettingshausen, A. (1826). <em>Die combinatorische Analysis: als Vorbereitungslehre zum Studium der theoretischen höhern Mathematik</em>. Wien: Wallishausser. <a href="https://archive.org/details/diecombinatoris00ettigoog/page/n70/mode/1up?view=theater">https://archive.org/details/diecombinatoris00ettigoog/page/n70/mode/1up?view=theater</a></p>
<p>Euler, L. (1738). De progressionibus transcendentibus seu quarum termini generales algebraice dari nequeunt. <em>Commentarii Academiae Scientiarum Petropolitanae 5</em>: 36–57. <a href="https://scholarlycommons.pacific.edu/euler-works/19/">https://scholarlycommons.pacific.edu/euler-works/19/</a></p>
<p>———. (1748a). <em>Introductio in analysin infinitorum</em>. Vol. 1. Lausannae: Apud Marcum-Michaelem Bousqujet &amp; Socio. <a href="https://scholarlycommons.pacific.edu/euler-works/101/">https://scholarlycommons.pacific.edu/euler-works/101/</a></p>
<p>———. (1748b). <em>Introductio in analysin infinitorum</em>. Vol. 2. Lausannae: Apud Marcum-Michaelem Bousqujet &amp; Socio. <a href="https://scholarlycommons.pacific.edu/euler-works/102/">https://scholarlycommons.pacific.edu/euler-works/102/</a></p>
<p>———. (1753). Calcul de la probabilité dans le jeu de rencontre. <em>Mémoires de l’académie Des Sciences de Berlin 7</em>: 255–70. <a href="">https://scholarlycommons.pacific.edu/euler-works/201/</a></p>
<p>Everitt, P. F. (1910). Tables of the Tetrachoric Functions for Fourfold Correlation Tables. <em>Biometrika 7</em> (4): 437–51. <a href="https://doi.org/10.1093/biomet/7.4.437">https://doi.org/10.1093/biomet/7.4.437</a></p>
<p>———. (1912). Supplementary Tables for Finding the Correlation Coefficient from Tetrachoric Groupings. <em>Biometrika 8</em> (3/4): 385–95. <a href="http://www.jstor.org/stable/2331587">http://www.jstor.org/stable/2331587</a></p>
<p>Ewald, W. B. (1996a). <em>From Kant to Hilbert: A Source Book in the Foundations of Mathematics</em>. Vol. 1. Oxford: Oxford University Press OUP. <a href="https://philpapers.org/rec/BRAFKT">https://philpapers.org/rec/BRAFKT</a></p>
<p>———. (1996b). <em>From Kant to Hilbert: A Source Book in the Foundations of Mathematics</em>. Vol. 2. Oxford: Oxford University Press OUP. <a href="https://philpapers.org/rec/EWAFKT-4">https://philpapers.org/rec/EWAFKT-4</a></p>
<p>Feigenbaum, L. (1985). Brook Taylor and the Method of Increments. <em>Archive for History of Exact Sciences 34</em> (1/2): 1–140. <a href="http://www.jstor.org/stable/41133765">http://www.jstor.org/stable/41133765</a></p>
<p>Finkel, B. F. (1897). Biography: Leonhard Euler. <em>The American Mathematical Monthly 4</em> (12): 297–302. <a href="http://www.jstor.org/stable/2968971">http://www.jstor.org/stable/2968971</a></p>
<p>Finocchiaro, M. A. (1997). The Port-Royal Logic’s Theory of Argument. <em>Argumentation 11</em> (4): 393–410. <a href="https://doi.org/10.1023/A:1007756105432">https://doi.org/10.1023/A:1007756105432</a></p>
<p>Fisher, R. A. (1915). Frequency Distribution of the Values of the Correlation Coefficient in Samples from an Indefinitely Large Population. <em>Biometrika 10</em> (4): 507–21. <a href="https://doi.org/10.2307/2331838">https://doi.org/10.2307/2331838</a></p>
<p>———. (1918). The Correlation Between Relatives on the Supposition of Mendelian Inheritance. <em>Philosophical Transactions of the Royal Society of Edinburgh 52</em>: 399–433. <a href="https://hdl.handle.net/2440/15097">https://hdl.handle.net/2440/15097</a></p>
<p>———. (1921). On the &quot;Probable Error&quot; of a Coefficient of Correlation Deduced from a Small Sample. <em>Metron 1</em>: 3–32. <a href="https://hdl.handle.net/2440/15169">https://hdl.handle.net/2440/15169</a></p>
<p>———. (1922). On the Interpretation of χ2 from Contingency Tables, and the Calculation of p. <em>Journal of the Royal Statistical Society 85</em> (1): 87–94. <a href="https://doi.org/10.2307/2340521">https://doi.org/10.2307/2340521</a></p>
<p>———. (1924). On a Distribution Yielding the Error Functions of Several Well-Known Statistics. <em>Proceedings International Mathematical Congress, Toronto 2</em>: 805–13. <a href="https://repository.rothamsted.ac.uk/item/8w2q9/on-a-distribution-yielding-the-error-functions-of-several-well-known-statistics">https://repository.rothamsted.ac.uk/item/8w2q9/on-a-distribution-yielding-the-error-functions-of-several-well-known-statistics</a></p>
<p>———. (1925). <em>Statistical Methods for Research Workers</em>. 1st ed. Edinburgh: Oliver; Boyd. <a href="https://www.scirp.org/(S(i43dyn45teexjx455qlt3d2q))/reference/ReferencesPapers.aspx?ReferenceID=2056938">https://www.scirp.org/(S(i43dyn45teexjx455qlt3d2q))/reference/ReferencesPapers.aspx?ReferenceID=2056938</a></p>
<p>———. (1926). The Arrangement of Field Experiments. <em>Journal of the Ministry of Agriculture 33</em>: 503–15. <a href="https://doi.org/10.23637/rothamsted.8v61q">https://doi.org/10.23637/rothamsted.8v61q</a></p>
<p>———. (1935). <em>The Design of Experiments</em>. 1st ed. Edinburgh: Oliver &amp; Boyd. <a href="https://psycnet.apa.org/record/1939-04964-000">https://psycnet.apa.org/record/1939-04964-000</a></p>
<p>———. (1954). <em>Statistical Methods for Research Workers</em>. 12th ed. Edinburgh: Oliver; Boyd. <a href="https://www.worldcat.org/de/title/statistical-methods-for-research-workers/oclc/312138">https://www.worldcat.org/de/title/statistical-methods-for-research-workers/oclc/312138</a></p>
<p>———. (1966). <em>The Design of Experiments</em>. 8th ed. Edinburgh: Hafner. <a href="https://scirp.org/reference/referencespapers.aspx?referenceid=895747">https://scirp.org/reference/referencespapers.aspx?referenceid=895747</a></p>
<p>———. (1971). <em>The Design of Experiments</em>. 9th ed. New York: Hafner Press. <a href="https://home.iitk.ac.in/~shalab/anova/DOE-RAF.pdf">https://home.iitk.ac.in/~shalab/anova/DOE-RAF.pdf</a></p>
<p>———. (1973). <em>Statistical Methods for Research Workers</em>. 14th ed. New York: ‎Hafner Publishing Company. <a href="https://www.amazon.com/Statistical-methods-research-workers-Fourteenth/dp/0050021702">https://www.amazon.com/Statistical-methods-research-workers-Fourteenth/dp/0050021702</a></p>
<p>———. (2017). <em>Statistical Methods for Research Workers</em>. 14th rev. ed. New Delhi: ‎‎Gyan Books. <a href="https://www.amazon.com/Statistical-Methods-Research-Workers-Fisher/dp/9351286584">https://www.amazon.com/Statistical-Methods-Research-Workers-Fisher/dp/9351286584</a></p>
<p>Galton, F. (1877). Typical Laws of Heredity 1. <em>Nature 15</em>: 492–95. <a href="https://doi.org/10.1038/015492a0">https://doi.org/10.1038/015492a0</a></p>
<p>Gauss, C. F. (1809). <em>Theoria motvs corporvm coelestivm in sectionibvs conicis Solem ambientivm</em>. Hambvrgi: Svmtibvs F. Perthes et I. H. Besser. <a href="https://archive.org/details/theoriamotuscor00gausgoog/page/n1/mode/1up">https://archive.org/details/theoriamotuscor00gausgoog/page/n1/mode/1up</a></p>
<p>———. (1823). <em>Theoria Combinationis Observationum Erroribus Minimis Obnoxiae</em>. Göttingen: apud Henricum Dieterich. <a href="https://doi.org/10.3931/e-rara-2857">https://doi.org/10.3931/e-rara-2857</a></p>
<p>———. (1828). <em>Theoria residuorum biquadraticorum: commentatio prima</em>. Gottingae: typis Dieterichianis. <a href="https://doi.org/10.3931/e-rara-61066">https://doi.org/10.3931/e-rara-61066</a></p>
<p>———. (1832). <em>Theoria residuorum biquadraticorum: commentatio secunda</em>. Gottingae: typis Dieterichianis. <a href="https://doi.org/10.3931/e-rara-61067">https://doi.org/10.3931/e-rara-61067</a></p>
<p>Gerhardt, C. I. (1848). <em>Die Entdeckung der Differentialrechnung durch Leibniz mit Benutzung der Leibnizischen Manuscripte auf der Königlichen Bibliothek zu Hannover</em>. Halle: H. W. Schmidt. <a href="https://doi.org/10.3931/e-rara-4272">https://doi.org/10.3931/e-rara-4272</a></p>
<p>Good, P. (2006). <em>Resampling Methods</em>. 3rd ed. Basel: Birkhäuser. <a href="https://www.amazon.com/Resampling-Methods-Practical-Guide-Analysis/dp/0817643869">https://www.amazon.com/Resampling-Methods-Practical-Guide-Analysis/dp/0817643869</a></p>
<p>Gosset, W. S. (1908). The Probable Error of a Mean. <em>Biometrika 6</em> (1): 1–25. <a href="https://doi.org/10.2307/2331554">https://doi.org/10.2307/2331554</a></p>
<p>Gregory, J. (1668a). <em>Exercitationes Geometricae</em>. Londini: Typis Guilielmi Godbid, &amp; Impensis Mosis Pitt Bibliopolae, in vico vulgo vocato Little Britain. <a href="https://books.google.com/books?id=ZtRYqgyD5YsC">https://books.google.com/books?id=ZtRYqgyD5YsC</a></p>
<p>———. (1668b). <em>Geometriae Pars Universalis, Inferuiens Quantitatum Curvarum transmutationi &amp; mensurae</em>. Patavii: Typis Heredum Pauli Frambotti. <a href="https://archive.org/details/gregory_universalis">https://archive.org/details/gregory_universalis</a></p>
<p>Gregory, J., &amp; Collins, J. (1939). <em>James Gregory: Tercentenary Memorial Volume, Containing His Correspondence with John Collins and His Hitherto Unpublished Mathematical Manuscripts, Together with Addresses and Essays Communicated to the Royal Society of Edinburgh, July 4, 1938</em>. Edited by Turnbull, H.W., &amp; Royal Society of Edinburgh. Edinburgh: Royal Society of Edinburgh. <a href="https://books.google.com/books?id=_eruAAAAMAAJ">https://books.google.com/books?id=_eruAAAAMAAJ</a></p>
<p>Gupta, R. C. (1974). An Indian Form of Third Order Taylor Series Approximation of the Sine. <em>Historia Mathematica 1</em> (3): 287–89. <a href="https://doi.org/10.1016/0315-0860(74)90067-6">https://doi.org/10.1016/0315-0860(74)90067-6</a></p>
<p>Hacking, I. (1975). <em>The Emergence of Probability: A Philosophical Study of Early Ideas About Probability, Induction and Statistical Inference</em>. Cambridge University Press. <a href="https://philpapers.org/rec/HACTEO-8">https://philpapers.org/rec/HACTEO-8</a></p>
<p>Hald, A. (1990). <em>History of Probability and Statistics and Their Applications before 1750</em>. New York: Wiley Series in Probability; Statistics, Wiley-Interscience. <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/0471725161">https://onlinelibrary.wiley.com/doi/book/10.1002/0471725161</a></p>
<p>———. (1998). <em>A History of Mathematical Statistics from 1750 to 1930</em>. New York: Wiley. <a href="https://www.abebooks.com/History-Mathematical-Statistics-1750-1930-Wiley/31042381048/bd">https://www.abebooks.com/History-Mathematical-Statistics-1750-1930-Wiley/31042381048/bd</a></p>
<p>———. (2003). <em>A History of Probability and Statistics and Their Applications before 1750</em>. Hoboken, NJ: Wiley-Interscience. <a href="https://www.wiley.com/en-us/A%2BHistory%2Bof%2BProbability%2Band%2BStatistics%2Band%2BTheir%2BApplications%2Bbefore%2B1750-p-9780471725176">https://www.wiley.com/en-us/A+History+of+Probability+and+Statistics+and+Their+Applications+before+1750-p-9780471725176</a></p>
<p>———. (2007). <em>A History of Parametric Statistical Inference from Bernoulli to Fisher, 1713–1935</em>. New York: Springer. <a href="https://link.springer.com/book/10.1007/978-0-387-46409-1#bibliographic-information">https://link.springer.com/book/10.1007/978-0-387-46409-1#bibliographic-information</a></p>
<p>Heath, T. L. (1908a). <em>The Thirteen Books of Euclid’s Elements Translated from the Text of Heiberg with Introduction and Commentary</em>. Vol. I Introduction and Books I, II. Cambridge: University Press. <a href="https://archive.org/details/thirteenbookseu02heibgoog">https://archive.org/details/thirteenbookseu02heibgoog</a></p>
<p>———. (1908b). <em>The Thirteen Books of Euclid’s Elements Translated from the Text of Heiberg with Introduction and Commentary</em>. Vol. II Books III–IX. Cambridge: University Press. <a href="https://archive.org/details/thirteenbookseu00heibgoog">https://archive.org/details/thirteenbookseu00heibgoog</a></p>
<p>———. (1908c). <em>The Thirteen Books of Euclid’s Elements Translated from the Text of Heiberg with Introduction and Commentary</em>. Vol. III Books X–XIII and Appendix. Cambridge: University Press. <a href="https://archive.org/details/thirteenbookseu03heibgoog">https://archive.org/details/thirteenbookseu03heibgoog</a></p>
<p>———. (1921a). <em>A History of Greek Mathematics. Vol. I From Thales to Euclid</em>. Oxford: At the Clarendon Press. <a href="https://archive.org/details/cu31924008704219">https://archive.org/details/cu31924008704219</a></p>
<p>———. (1921b). <em>A History of Greek Mathematics. Vol. II From Aristarchus to Diophantus</em>. Oxford: At the Clarendon Press. <a href="https://archive.org/details/historyofgreekma029268mbp/page/n5/mode/1up">https://archive.org/details/historyofgreekma029268mbp/page/n5/mode/1up</a></p>
<p>Helmert, F. R. (1876). Ueber die Wahrscheinlichkeit der Potenzsummen der Beobachtungsfehler und über einige damit im Zusammenhange stehende Fragen. <em>Zeitschrift für Mathematik und Physik 21</em>: 192–219. <a href="https://gdz.sub.uni-goettingen.de/id/PPN599415665_0021">https://gdz.sub.uni-goettingen.de/id/PPN599415665_0021</a></p>
<p>Howie, J. M. (2001). The Logarithmic and Exponential Functions. In <em>Real Analysis</em>, 165–79. London: Springer. <a href="https://doi.org/10.1007/978-1-4471-0341-7_6">https://doi.org/10.1007/978-1-4471-0341-7_6</a></p>
<p>HP Inc. (2017). <em>HP Prime Graphing Calculator: Manual</em>. 3rd ed. Stanford Research Park, Palo Alto, California, U.S.: HP Development Company, L.P. <a href="https://www.hpcalc.org/details/7445">https://www.hpcalc.org/details/7445</a></p>
<p>Jahnke, E., &amp; Emde, F. (1909). <em>Funktionentafeln Mit Formeln Und Kurven</em>. 1st ed. Mathematisch-Physikalische Schriften für Ingenieure Und Studierende. Leipzig: B. G. Teubner. <a href="https://books.google.com/books?id=BVRzvgAACAAJ">https://books.google.com/books?id=BVRzvgAACAAJ</a></p>
<p>———. (1933). <em>Funktionentafeln Mit Formeln Und Kurven</em>. 2nd ed. Mathematisch-Physikalische Schriften für Ingenieure Und Studierende. Leipzig: B. G. Teubner. <a href="https://books.google.com/books?id=SB5tAAAAMAAJ">https://books.google.com/books?id=SB5tAAAAMAAJ</a></p>
<p>———. (1938). <em>Funktionentafeln Mit Formeln Und Kurven</em>. 3rd ed. Leipzig: Teubner. <a href="https://books.google.com/books?id=5vlrAAAAIAAJ">https://books.google.com/books?id=5vlrAAAAIAAJ</a></p>
<p>———. (1945). <em>Funktionentafeln Mit Formeln Und Kurven</em>. 4th ed. Dover Book. New York: Dover Publications. <a href="https://archive.org/details/tablesoffunction0000jahn">https://archive.org/details/tablesoffunction0000jahn</a></p>
<p>Jahnke, E., Emde, F., &amp; Lösch, F. (1966). <em>Tafeln höherer Funktionen</em>. 7th ed. Stuttgart: B. G. Teubner Verlagsgesellschaft. <a href="https://dokumen.pub/jahnke-emde-lsch-tafeln-hherer-funktionen-tables-of-higher-functions-7nbsped.html">https://dokumen.pub/jahnke-emde-lsch-tafeln-hherer-funktionen-tables-of-higher-functions-7nbsped.html</a></p>
<p>Jyesthadeva. (1530). <em>Ganita-Yukti-Bhasa (Rationales in Mathematical Astronomy)</em>. Kingdom of Cochin: Kerala school of astronomy; mathematics. <a href="https://archive.org/details/raswhishNA-124">https://archive.org/details/raswhishNA-124</a></p>
<p>Katz, V. (2009). <em>Elementary Probability. A History of Mathematics: An Introduction</em>. 3rd ed. London: Pearson. <a href="https://www.gettextbooks.com/isbn/9780321387004/">https://www.gettextbooks.com/isbn/9780321387004/</a></p>
<p>Kendall, M. G. (1938). A New Measure of Rank Correlation. <em>Biometrika 30</em> (1/2): 81–93. <a href="http://www.jstor.org/stable/2332226">http://www.jstor.org/stable/2332226</a></p>
<p>Knoll, F. (1939). Funktionentafeln mit Formeln und Kurven. <em>Monatshefte Für Mathematik Und Physik</em>. <a href="https://doi.org/10.1007/BF01695545">https://doi.org/10.1007/BF01695545</a></p>
<p>Kossovsky, A. E. (2020). The Bitter Dispute with Leibniz over Calculus Priority. In <em>The Birth of Science</em>, 161–61. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-51744-1_33">https://doi.org/10.1007/978-3-030-51744-1_33</a></p>
<p>Koyama, S. -J., &amp; Kurokawa, N. (2005). Euler’s Integrals and Multiple Sine Functions. <em>Proceedings of the American Mathematical Society 133</em> (5): 1257–65. <a href="http://www.jstor.org/stable/4097775">http://www.jstor.org/stable/4097775</a></p>
<p>Krauth, J. (1993). <em>Einführung in die Konfigurationsfrequenzanalyse (KFA): Ein multivariates nichtparametrisches Verfahren zum Nachweis und zur Interpretation von Typen und Syndromen</em>. Weinheim: BELTZ Psychologie Verlags Union. <a href="https://books.google.com/books?id=4oeIAAAACAAJ">https://books.google.com/books?id=4oeIAAAACAAJ</a></p>
<p>Krauth, J., &amp; Lienert, G. (1973). <em>Die Konfigurationsfrequenzanalyse (KFA) und ihre Anwendung in Psychologie und Medizin: Ein multivariates nichtparametrisches Verfahren zum Aufdeckung von Typen und Syndromen; mit 70 Tab</em>. Freiburg: Alber. <a href="https://d-nb.info/740097938">https://d-nb.info/740097938</a></p>
<p>Lehmann, E. L. (1993). The Fisher, Neyman-Pearson Theories of Testing Hypotheses: One Theory or Two? <em>Journal of the American Statistical Association 88</em> (424): 1242–49. <a href="https://doi.org/10.1080/01621459.1993.10476404">https://doi.org/10.1080/01621459.1993.10476404</a></p>
<p>Lehmann, E. L., &amp; Romano, J. P. (2008). <em>Testing Statistical Hypotheses</em>. 3rd ed. Springer Texts in Statistics. New York: Springer. <a href="https://books.google.com/books?id=IlJE_9_e8UEC">https://books.google.com/books?id=IlJE_9_e8UEC</a></p>
<p>Leibniz, G. W. (1682). De vera proportione circuli ad quadratum circumscriptum in numeris rationalibus. <em>Acta Eruditorum Anno MDCLXXXII</em>, 41–46. <a href="https://books.google.com/books/about/Acta_eruditorum.html?id=E7MasYIsMKQC">https://books.google.com/books/about/Acta_eruditorum.html?id=E7MasYIsMKQC</a></p>
<p>———. (1684). Nova methodus pro maximis et minimis itemque tangentibus, quae nec fractas nec irrationales quantitates moratur, et singulare pro illis calculi genus, per G.G.L. <em>Acta Eruditorum Anno MDCLXXXIV</em>, 467–73. <a href="https://gdz.sub.uni-goettingen.de/id/PPN788262599">https://gdz.sub.uni-goettingen.de/id/PPN788262599</a></p>
<p>———. (1686). De geometria recondita et analysi indivisibilium atque infinitorum. <em>Acta Eruditorum Anno MDCLXXXVI</em>, 292–300. <a href="https://gdz.sub.uni-goettingen.de/id/PPN788262947">https://gdz.sub.uni-goettingen.de/id/PPN788262947</a></p>
<p>———. (1693). Supplementum geometriae dimensoriae, seu generalissima omnium tetragonismorum effectio per motum: similiterque multiplex constructio lineae ex data tangentium conditione. <em>Acta Eruditorum Anno MDCLXCIII</em>, 385–92. <a href="https://archive.org/details/s1id13206590">https://archive.org/details/s1id13206590</a></p>
<p>———. (2012). <em>Sämtliche Schriften und Briefe</em>. Edited by Berlin-Brandenburgischen Akademie der Wissenschaften, &amp; Akademie der Wissenschaften zu Göttingen. Vol. 6: 1673–1676: Arithmetische Kreisquadratur. 7: Mathematische Schriften. Berlin: Akademie Verlag. <a href="https://doi.org/10.26015/adwdocs-1924">https://doi.org/10.26015/adwdocs-1924</a></p>
<p>Long, M. A., Berry, K. J., &amp; Mielke, P. W. (2009). Tetrachoric Correlation: A Permutation Alternative. <em>Educational and Psychological Measurement 69</em> (3): 429–37. <a href="https://doi.org/10.1177/0013164408324463">https://doi.org/10.1177/0013164408324463</a></p>
<p>Lüroth, J. (1876). Vergleichung von zwei Werthen des wahrscheinlichen Fehlers. <em>Astronomische Nachrichten 87</em> (14): 209–20. <a href="https://doi.org/10.1002/asna.18760871402">https://doi.org/10.1002/asna.18760871402</a></p>
<p>MacMohan, P. A. (1915). <em>Combinatory Analysis</em>. Vol. 1. Cambridge: University Press. <a href="https://openlibrary.org/works/OL1109964W/Combinatory_analysis">https://openlibrary.org/works/OL1109964W/Combinatory_analysis</a></p>
<p>———. (1916). <em>Combinatory Analysis</em>. Vol. 2. Cambridge: University Press. <a href="https://books.google.com/books/about/Combinatory_Analysis.html?id=A_PuAAAAMAAJ&amp;redir_esc=y">https://books.google.com/books/about/Combinatory_Analysis.html?id=A_PuAAAAMAAJ&amp;redir_esc=y</a></p>
<p>Malet, A. (2006). Renaissance Notions of Number and Magnitude. <em>Historia Mathematica 33</em> (1): 63–81. <a href="https://doi.org/10.1016/j.hm.2004.11.011">https://doi.org/10.1016/j.hm.2004.11.011</a></p>
<p>Mann, H. B., &amp; Whitney, D. R. (1947). On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other. <em>The Annals of Mathematical Statistics 18</em> (1): 50–60. <a href="https://doi.org/10.1214/aoms/1177730491">https://doi.org/10.1214/aoms/1177730491</a></p>
<p>Marsden, J., &amp; Weinstein, A. (1985). Exponentials and Logarithms. In <em>Calculus i</em>, 307–35. New York, NY: Springer. <a href="https://doi.org/10.1007/978-1-4612-5024-1_9">https://doi.org/10.1007/978-1-4612-5024-1_9</a></p>
<p>McNemar, Q. (1947). Note on the Sampling Error of the Difference Between Correlated Proportions or Percentages. <em>Psychometrika 12</em> (2): 153–157. <a href="https://doi.org/10.1007/BF02295996">https://doi.org/10.1007/BF02295996</a></p>
<p>Mehta, C. R., Patel, N. R., Senchaudhuri, P., &amp; Corcoran, C. D. (2014). StatXact. In <em>Wiley StatsRef: Statistics Reference Online</em>. John Wiley &amp; Sons, Ltd. <a href="https://doi.org/10.1002/9781118445112.stat04892">https://doi.org/10.1002/9781118445112.stat04892</a></p>
<p>Merzbach, U. C., &amp; Boyer, C. B. (2011). <em>A History of Mathematics</em>. 3rd ed. Hoboken, New Jersey: John Wiley &amp; Sons, Inc. <a href="https://books.google.com/books/about/A_History_of_Mathematics.html?id=bR9HAAAAQBAJ">https://books.google.com/books/about/A_History_of_Mathematics.html?id=bR9HAAAAQBAJ</a></p>
<p>Metropolis, N., &amp; Ulam, S. (1949). The Monte Carlo Method. <em>Journal of the American Statistical Association 44</em> (247): 335–41. <a href="https://doi.org/10.1080/01621459.1949.10483310">https://doi.org/10.1080/01621459.1949.10483310</a></p>
<p>Meyberg, K., &amp; Vachenauer, P. (2001a). <em>Höhere Mathematik 1: Differential- und Integralrechnung Vektor- und Matrizenrechnung</em>. Berlin, Heidelberg: Springer. <a href="https://doi.org/10.1007/978-3-642-56654-7">https://doi.org/10.1007/978-3-642-56654-7</a></p>
<p>———. (2001b). <em>Höhere Mathematik 2: Differentialgleichungen, Funktionentheorie, Fourier-Analysis, Variationsrechnung</em>. Berlin, Heidelberg: Springer. <a href="https://doi.org/10.1007/978-3-642-56655-4">https://doi.org/10.1007/978-3-642-56655-4</a></p>
<p>Neugebauer, O. (1969). <em>The Exact Sciences in Antiquity</em>. 2nd ed. Acta Historica Scientiarum Naturalium Et Medicinalium. New York: Dover Publications. <a href="https://books.google.com/books?id=JVhTtVA2zr8C">https://books.google.com/books?id=JVhTtVA2zr8C</a></p>
<p>Newton, I. (1669). <em>De analysi per aequationes numero terminorum infinitas</em>. Sent by Dr. Barrow to Mr. Collins in a Letter dated July 31. 1669. London: Royal Society Library. <a href="https://www.newtonproject.ox.ac.uk/view/texts/normalized/NATP00204">https://www.newtonproject.ox.ac.uk/view/texts/normalized/NATP00204</a></p>
<p>———. (1687). <em>Philosophiae naturalis principia mathematica</em>. 1st ed. Londini: Jussu Societatis Regiae ac typis Josephi Streater, prostant venales apud Sam. Smith. <a href="https://books.google.com/books?id=XJwx0lnKvOgC">https://books.google.com/books?id=XJwx0lnKvOgC</a></p>
<p>———. (1711). <em>Analysis per quantitatum series, fluxiones, ac differentias: cum enumeratione linearum tertii ordinis</em>. Londini: ex officina Pearsoniana. <a href="https://doi.org/10.3931/e-rara-8934">https://doi.org/10.3931/e-rara-8934</a></p>
<p>———. (1713). <em>Philosophiae naturalis principia mathematica</em>. 2nd ed. Cantabrigiae: Newton, I. <a href="https://digital.onb.ac.com/OnbViewer/viewer.faces?doc=ABO_%2BZ180810706&amp;order=7&amp;view=SINGLE">https://digital.onb.ac.com/OnbViewer/viewer.faces?doc=ABO_%2BZ180810706&amp;order=7&amp;view=SINGLE</a></p>
<p>———. (1726). <em>Philosophiae naturalis principia mathematica</em>. 3rd ed. Londini: Apud Guil. &amp; Joh. Innys. <a href="https://gdz.sub.uni-goettingen.de/id/PPN512261393">https://gdz.sub.uni-goettingen.de/id/PPN512261393</a></p>
<p>Neyman, J. (1923). Sur les applications de la theorie des probabilites aux experience agricoles: Essay de principes. <em>Roczniki Nank Polniczek 10</em>: 1–51. <a href="https://link.springer.com/chapter/10.1007/978-94-015-8816-4_10">https://link.springer.com/chapter/10.1007/978-94-015-8816-4_10</a></p>
<p>———. (1937). Outline of a Theory of Statistical Estimation Based on the Classical Theory of Probability. <em>Philosophical Transactions of the Royal Society of London A, Mathematical and physical sciences, 236</em> (767): 333–80. <a href="https://doi.org/10.1098/rsta.1937.0005">https://doi.org/10.1098/rsta.1937.0005</a></p>
<p>Neyman, J., &amp; Pearson, E. S. (1933). On the problem of the most efficient tests of statistical hypotheses. <em>Phil. Trans. R. Soc. Lond. A. 231</em> (694–706): 289–337. <a href="https://doi.org/10.1098/rsta.1933.0009">https://doi.org/10.1098/rsta.1933.0009</a></p>
<p>Olkin, I., &amp; Pratt, J. W. (1958). Unbiased Estimation of Certain Correlation Coefficients. <em>The Annals of Mathematical Statistics 29</em> (1): 201–11. <a href="https://doi.org/10.1214/aoms/1177706717">https://doi.org/10.1214/aoms/1177706717</a></p>
<p>Pascal, B. (1665). <em>Traite´ du triangle arithmetique : auec quelques autres petits traitez sur la mesme matie‘re</em>. A Paris: Chez Guillaume Desprez, rue Saint Jacques, a Saint Prosper. <a href="[https://gallica.bnf.fr/ark:/12148/btv1b86262012.image#">https://gallica.bnf.fr/ark:/12148/btv1b86262012.image#</a></p>
<p>Pearson, K. (1895). Contributions to the Mathematical Theory of Evolution. II. Skew Variation in Homogeneous Material. <em>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 186</em>: 343–414. <a href="https://doi.org/10.1098/rsta.1895.0010">https://doi.org/10.1098/rsta.1895.0010</a></p>
<p>———. (1900a). I. Mathematical Contributions to the Theory of Evolution. —VII. On the Correlation of Characters Not Quantitatively Measurable. <em>Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 195</em> (262-273): 1–47. <a href="https://doi.org/10.1098/rsta.1900.0022">https://doi.org/10.1098/rsta.1900.0022</a></p>
<p>———. (1900b). X. On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. <em>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 50</em> (302): 157–75. <a href="https://doi.org/10.1080/14786440009463897">https://doi.org/10.1080/14786440009463897</a></p>
<p>———. (1904). <em>Mathematical contributions to the theory of evolution. XIII. On the Theory of Contingency and its Relation to Association and Normal Correlation. Drapers’ Company research memoirs</em>. Biometric Series, I. Department of Applied Mathematics. University College, University of London: Dulau &amp; Co. <a href="https://openlibrary.org/books/OL24168960M">https://openlibrary.org/books/OL24168960M</a></p>
<p>———. (1905). <em>Mathematical contributions to the theory of evolution. XIV. On the general theory of skew correlation and non-linear regression. Drapers’ Company research memoirs</em>. Biometric Series, II. Department of Applied Mathematics. University College, University of London: Dulau &amp; Co. <a href="https://openlibrary.org/books/OL6555066M">https://openlibrary.org/books/OL6555066M</a></p>
<p>———. (1909). On a New Method of Determining Correlation Between a Measured Character a, and a Character b, of Which Only the Percentage of Cases Wherein b Exceeds (or Falls Short of) a Given Intensity Is Recorded for Each Grade of a. <em>Biometrika 7</em> (1/2): 96–105. <a href="http://www.jstor.org/stable/2345365">http://www.jstor.org/stable/2345365</a></p>
<p>———. (1914). On the Probability That Two Independent Distributions of Frequency Are Really Samples of the Same Population, with Special Reference to Recent Work on the Identity of Trypanosome Strains. <em>Biometrika 10</em>: 85–154. <a href="https://doi.org/10.1093/biomet/10.1.85">https://doi.org/10.1093/biomet/10.1.85</a></p>
<p>Pitman, E. J. G. (1937a). Significance Tests Which May Be Applied to Samples from Any Populations. <em>Supplement to the Journal of the Royal Statistical Society 4</em> (1): 119–30. <a href="http://www.jstor.org/stable/2984124">http://www.jstor.org/stable/2984124</a></p>
<p>———. (1937b). Significance Tests Which May Be Applied to Samples from Any Populations. II. The Correlation Coefficient Test. <em>Supplement to the Journal of the Royal Statistical Society 4</em> (2): 225–32. <a href="http://www.jstor.org/stable/2983647">http://www.jstor.org/stable/2983647</a></p>
<p>———. (1938). Significance Tests Which May Be Applied to Samples from Any Populations: III. The Analysis of Variance Test. <em>Biometrika 29</em> (3/4): 322–35. <a href="http://www.jstor.org/stable/2332008">http://www.jstor.org/stable/2332008</a></p>
<p>Peladeau, N. (1993). SIMSTAT: Bootstrap computer simulation and statistical program for IBM personal computers. <em>Behavior Research Methods, Instruments, &amp; Computers 25</em> (3): 410–13. <a href="https://doi.org/10.3758/BF03204533">https://doi.org/10.3758/BF03204533</a></p>
<p>Plackett, R. L. (1983). Karl Pearson and the Chi-Squared Test. <em>International Statistical Review / Revue Internationale de Statistique 51</em> (1): 59–72. <a href="https://doi.org/10.2307/1402731">https://doi.org/10.2307/1402731</a></p>
<p>Quenouille, M. H. (1949). Approximate Tests of Correlation in Time-Series. <em>Journal of the Royal Statistical Society B, Methodological, 11</em> (1): 68–84. <a href="https://doi.org/10.1111/j.2517-6161.1949.tb00023.x">https://doi.org/10.1111/j.2517-6161.1949.tb00023.x</a></p>
<p>Rabinovitch, N. L. (1970). Rabbi Levi Ben Gershon and the Origins of Mathematical Induction. <em>Archive for History of Exact Sciences, no. 3</em>: 237–48. <a href="http://www.jstor.org/stable/41133303">http://www.jstor.org/stable/41133303</a></p>
<p>Ratdolt, E. (1482). <em>Euclides. Elementa geometriae</em>. Edited by Campano da Novara. Venice: Erhard Ratdolt. <a href="https://catalog.lindahall.org/discovery/delivery/01LINDAHALL_INST:LHL/1286816310005961">https://catalog.lindahall.org/discovery/delivery/01LINDAHALL_INST:LHL/1286816310005961</a></p>
<p>Remmert, R. (1998). The Gamma Function. In <em>Classical Topics in Complex Function Theory</em>, 33–72. New York, NY: Springer. <a href="https://doi.org/10.1007/978-1-4757-2956-6_2">https://doi.org/10.1007/978-1-4757-2956-6_2</a></p>
<p>Remmert, R., &amp; Schumacher, G. (2002). <em>Funktionentheorie 1</em>. Berlin, Heidelberg: Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/978-3-642-56281-5">https://doi.org/10.1007/978-3-642-56281-5</a></p>
<p>Roero, C. S. (2005). Chapter 4 - Gottfried Wilhelm Leibniz, First Three Papers on the Calculus (1684, 1686, 1693). In <em>Landmark Writings in Western Mathematics 1640-1940</em>, edited by Grattan-Guinness, I., Cooke, R., Corry, L., Crépel, P., &amp; Guicciardini, N., 46–58. Amsterdam: Elsevier Science. <a href="https://doi.org/https://doi.org/10.1016/B978-044450871-3/50085-1">https://doi.org/https://doi.org/10.1016/B978-044450871-3/50085-1</a></p>
<p>Rosenthal, A. (1951). The History of Calculus. <em>The American Mathematical Monthly 58</em> (2): 75–86. <a href="http://www.jstor.org/stable/2308368">http://www.jstor.org/stable/2308368</a></p>
<p>Scambor, C. (1997). <em>Permutationsverfahren in Einzelfallstudien: Grundlagen, Anwendungen und Teststärke</em>. Universität Graz: Naturwissenschaftliche Fakultät. <a href="https://doi.org/10.13140/RG.2.2.28632.06405">https://doi.org/10.13140/RG.2.2.28632.06405</a></p>
<p>Scambor, C., &amp; Schrausser, D. G. (2023). Introduction (part II, permutation tests for repeated measurement designs). <em>Thesis Chapters. Academia</em>. <a href="https://www.academia.edu/94993376">https://www.academia.edu/94993376</a></p>
<p>Scheffé, H. (1959). <em>The Analysis of Variance</em>. New York: Wiley. <a href="https://psycnet.apa.org/record/1961-00074-000">https://psycnet.apa.org/record/1961-00074-000</a></p>
<p>Schneider, I. (2005a). Chapter 6 - Jakob Bernoulli, Ars conjectandi (1713). In <em>Landmark Writings in Western Mathematics 1640-1940</em>, edited by Grattan-Guinness, I., Cooke, R., Corry, L., Crépel, P., &amp; Guicciardini, N., 88–104. Amsterdam: Elsevier Science. <a href="https://doi.org/10.1016/B978-044450871-3/50087-5">https://doi.org/10.1016/B978-044450871-3/50087-5</a></p>
<p>———. (2005b). Chapter 7 -Abraham De Moivre, The Doctrine of Chances (1718, 1738, 1756). In <em>Landmark Writings in Western Mathematics 1640-1940</em>, edited by Grattan-Guinness, I., Cooke, R., Corry, L., Crépel, P., &amp; Guicciardini, N., 105–20. Amsterdam: Elsevier Science. <a href="https://doi.org/10.1016/B978-044450871-3/50087-5">https://doi.org/10.1016/B978-044450871-3/50087-5</a></p>
<p>Schrader, D. V. (1962). The Newton-Leibniz Controversy Concerning the Discovery of the Calculus. <em>The Mathematics Teacher 55</em> (5): 385–96. <a href="http://www.jstor.org/stable/27956626">http://www.jstor.org/stable/27956626</a></p>
<p>Schrausser, D. G. (1996). <em>Permutationstests: Theoretische und praktische Arbeitsweise von Permutationsverfahren beim unverbundenen 2 Stichprobenproblem</em>. Universität Graz: Naturwissenschaftliche Fakultät. <a href="https://doi.org/10.13140/RG.2.2.24500.32640/1">https://doi.org/10.13140/RG.2.2.24500.32640/1</a></p>
<p>———. (1997). Exakte Verfahren oder Asymptotische Approximation? In <em>3. Tagung der österreichischen Gesellschaft für Psychologie (ÖGP)</em>. Salzburg, Österreich: Universität Salzburg. <a href="https://doi.org/10.13140/RG.2.2.14805.91369">https://doi.org/10.13140/RG.2.2.14805.91369</a></p>
<p>———. (1998a). Die Permutationsmethode: Voraussetzungsfrei testen. In <em>41. Kongreß der Deutschen Gesellschaft für Psychologie</em>. Dresden, Deutschland: Technische Universität Dresden. <a href="https://doi.org/10.13140/RG.2.2.19532.69768">https://doi.org/10.13140/RG.2.2.19532.69768</a></p>
<p>———. (1998b). Exakte Verfahren oder Asymptotische Approximation? In <em>Perspektiven Psychologischer Forschung in Österreich</em>, edited by Glück, J., Jirasco, M., &amp; Rollett, B. Vol. 2. WUV-Univ.-Verl., Wien. <a href="https://doi.org/10.5281/zenodo.11673333">https://doi.org/10.5281/zenodo.11673333</a></p>
<p>———. (2022a). Mathematical-Statistical Algorithm Interpreter, SCHRAUSSER-MAT: Function Index, Manual. <em>Handbooks. Academia</em>. <a href="https://www.academia.edu/81395688">https://www.academia.edu/81395688</a></p>
<p>———. (2022b). Thesis chapter 1: Introduction. <em>Thesis Chapters. Academia</em>. <a href="https://www.academia.edu/82224369">https://www.academia.edu/82224369</a></p>
<p>———. (2023a). <em>Schrausser/ConsoleApp_DistributionFunctions: Console applicationes for distribution functions</em> (version v1.0.0). Zenodo. <a href="https://doi.org/10.5281/zenodo.7664141">https://doi.org/10.5281/zenodo.7664141</a></p>
<p>———. (2023b). <em>Schrausser/ConsoleApp_Integral: Console applications for integral and interpolation</em> (version v1.0.0). Zenodo. <a href="https://doi.org/10.5281/zenodo.7655056">https://doi.org/10.5281/zenodo.7655056</a></p>
<p>———. (2023c). <em>Schrausser/FunktionWin: Windows Interface for distribution functions</em> (version v1.0.0). Zenodo. <a href="https://doi.org/10.5281/zenodo.7651660">https://doi.org/10.5281/zenodo.7651660</a></p>
<p>———. (2023d). <em>Schrausser/PCE500_MATH: Mathematical and statistical applications for SHARP PC-E500</em> (version v1.0.0). Zenodo. <a href="https://doi.org/10.5281/zenodo.7664088">https://doi.org/10.5281/zenodo.7664088</a></p>
<p>———. (2024a). <em>Handbook: Distribution Functions (Verteilungs Funktionen)</em>. PsyArXiv. <a href="https://doi.org/10.5281/zenodo.10969144">https://doi.org/10.5281/zenodo.10969144</a></p>
<p>———. (2024b). Ptolemy’s Table of Chords: Implications Considered and Discussed. <em>Zenodo. May 2024</em>.
<a href="https://doi.org/10.5281/zenodo.11356370">https://doi.org/10.5281/zenodo.11356370</a></p>
<p>———. (2024c). <em>Schrausser/Abh_wkt: 1.5</em> (version v1.5.0). Zenodo. <a href="https://doi.org/10.5281/zenodo.14183565">https://doi.org/10.5281/zenodo.14183565</a></p>
<p>———. (2024d). <em>Schrausser/Various_programs: 3.5</em> (version v3.5.2). Zenodo. <a href="https://doi.org/10.5281/zenodo.14280500">https://doi.org/10.5281/zenodo.14280500</a></p>
<p>———. (2025). Schrausser/HP_Prime_MATH: 1.5. <em>Zenodo. January 2025</em>. <a href="https://doi.org/10.5281/zenodo.14721085">https://doi.org/10.5281/zenodo.14721085</a></p>
<p>Siegel, I. H. (1942). Index-Number Differences: Geometric Means. <em>Journal of the American Statistical Association 37</em> (218): 271–74. <a href="https://doi.org/10.1080/01621459.1942.10500636">https://doi.org/10.1080/01621459.1942.10500636</a></p>
<p>Snedecor, G. W. (1934). <em>Calculation and Interpretation of Analysis of Variance and Covariance</em>. Ames, Iowa: Collegiate Press. <a href="https://doi.org/10.1037/13308-000">https://doi.org/10.1037/13308-000</a></p>
<p>Sobot, R. (2021). Exponential and Logarithmic Functions. In <em>Engineering Mathematics by Example</em>, 51–66. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-79545-0_4">https://doi.org/10.1007/978-3-030-79545-0_4</a></p>
<p>Solomon, S. L. (1982). Simstat a Simple Statistical Package to Support Simulation. In <em>Proceedings of the 14th Conference on Winter Simulation</em> - Volume 1, 307–11. WSC ’82. San Diego, California: Winter Simulation Conference. <a href="https://doi.org/10.5555/1035853.1035900">https://doi.org/10.5555/1035853.1035900</a></p>
<p>Somers, R. H. (1962). A New Asymmetric Measure of Association for Ordinal Variables. <em>American Sociological Review 27</em> (6): 799–811. <a href="http://www.jstor.org/stable/2090408">http://www.jstor.org/stable/2090408</a></p>
<p>Spearman, C. (1904). The Proof and Measurement of Association Between Two Things. <em>The American Journal of Psychology 15</em> (1): 72–101. <a href="http://www.jstor.org/stable/1412159">http://www.jstor.org/stable/1412159</a></p>
<p>Stigler, S. M. (1986). <em>The history of statistics: the measurement of uncertainty before 1900</em>. Cambridge, MA: Belknap Press of Harvard University Press. <a href="https://www.scirp.org/(S(351jmbntvnsjt1aadkposzje))/reference/ReferencesPapers.aspx?ReferenceID=1973131">https://www.scirp.org/(S(351jmbntvnsjt1aadkposzje))/reference/ReferencesPapers.aspx?ReferenceID=1973131</a></p>
<p>———. (2018). Richard Price, the First Bayesian. <em>Statistical Science 33</em> (1): 117–25. <a href="https://www.jstor.org/stable/26770983">https://www.jstor.org/stable/26770983</a></p>
<p>Suter, H. (1887). <em>Die Mathematik auf den Universitäten des Mittelalters</em>. Zürich: Druck von Zürcher und Furrer. <a href="https://doi.org/10.3931/e-rara-65095">https://doi.org/10.3931/e-rara-65095</a></p>
<p>Sylvester, J. J. (1904). <em>The Collected Mathematical Papers of James Joseph Sylvester</em>. Edited by Baker, H. F. Vol. 1. Cambridge: University Press. <a href="https://archive.org/details/collectedmathem01sylvrich/page/n7/mode/1up">https://archive.org/details/collectedmathem01sylvrich/page/n7/mode/1up</a></p>
<p>———. (1908). <em>The Collected Mathematical Papers of James Joseph Sylvester</em>. Edited by Baker, H. F. Vol. 2. Cambridge: University Press. <a href="https://archive.org/details/SylvesterCollected2/page/n3/mode/1up">https://archive.org/details/SylvesterCollected2/page/n3/mode/1up</a></p>
<p>———. (1909). <em>The Collected Mathematical Papers of James Joseph Sylvester</em>. Edited by Baker, H. F. Vol. 3. Cambridge: University Press. <a href="https://archive.org/details/TheCollectedMathematicalPapersOfJamesJosephSylvesterVolumeIii/page/n3/mode/1up">https://archive.org/details/TheCollectedMathematicalPapersOfJamesJosephSylvesterVolumeIii/page/n3/mode/1up</a></p>
<p>———. (1912). <em>The Collected Mathematical Papers of James Joseph Sylvester</em>. Edited by Baker, H. F. Vol. 4. Cambridge: University Press. <a href="https://archive.org/details/collectedmathema04sylvuoft/page/n8/mode/1up">https://archive.org/details/collectedmathema04sylvuoft/page/n8/mode/1up</a></p>
<p>Tate, R. F. (1955). The Theory of Correlation Between Two Continuous Variables When One Is Dichotomized. <em>Biometrika 42</em> (1/2): 205–16. <a href="http://www.jstor.org/stable/2333437">http://www.jstor.org/stable/2333437</a></p>
<p>Taylor, B. (1715). <em>Methodus incrementorum directa &amp; inversa</em>. Auctore Brook Taylor, LL. D. &amp; Regiae Societatis Secretario. Londini: Typis Pearsonianis: Prostant apud Gul. Innys ad Insignia Principis in Coemeterio Paulino MDCCXV. <a href="https://books.google.com/books?id=iXN1xgEACAAJ">https://books.google.com/books?id=iXN1xgEACAAJ</a></p>
<p>———. (1717). <em>Methodus incrementorum directa &amp; inversa</em>. Auctore Brook Taylor, LL. D. &amp; Regiae Societatis Secretario. Londini: Impensis Gulielmi Innys ad Insignia Principis in Coemetrio D. Pauli. MDCCXVII. <a href="https://books.google.com/books?id=r-Gq9YyZYXYC">https://books.google.com/books?id=r-Gq9YyZYXYC</a></p>
<p>Thurstone, L. L. (1931). Multiple Factor Analysis. <em>Psychological Review 38</em> (5): 406–27. <a href="https://doi.org/10.1037/h0069792">https://doi.org/10.1037/h0069792</a></p>
<p>———. (1934). The Vectors of Mind. <em>Psychological Review 41</em>: 1–32. <a href="https://doi.org/10.1037/h0075959">https://doi.org/10.1037/h0075959</a></p>
<p>———. (1935). <em>The Vectors of Mind. Multiple-Factor Analysis for the Isolation of Primary Traits</em>. Chicago, Illinois: University of Chicago Press. <a href="https://archive.org/details/vectorsofmindmul010122mbp/page/n7/mode/1up">https://archive.org/details/vectorsofmindmul010122mbp/page/n7/mode/1up</a></p>
<p>van Evra, J. (1997). Antoine Arnauld and Pierre Nicole, Logic or the Art of Thinking. <em>Philosophy in Review 17</em> (3): 153–55. <a href="https://philpapers.org/rec/VANAAA-13">https://philpapers.org/rec/VANAAA-13</a></p>
<p>Vince, J. (2021). The Complex Plane. In <em>Quaternions for Computer Graphics</em>, 55–70. London: Springer. <a href="https://doi.org/10.1007/978-1-4471-7509-4_4">https://doi.org/10.1007/978-1-4471-7509-4_4</a></p>
<p>Walter, W. (1982). Old and New Approaches to Euler’s Trigonometric Expansions. <em>The American Mathematical Monthly 89</em> (4): 225–30. <a href="http://www.jstor.org/stable/2320218">http://www.jstor.org/stable/2320218</a></p>
<p>Weierstraß, K. (1894). <em>Mathematische Werke</em>. Vol. 1. Berlin: Mayer &amp; Müller. <a href="https://quod.lib.umich.edu/u/umhistmath/AAN8481.0001.001">https://quod.lib.umich.edu/u/umhistmath/AAN8481.0001.001</a></p>
<p>Whish, C. M. (1834). XXXIII. On the Hindú Quadrature of the Circle, and the infinite Series of the proportion of the circumference to the diameter exhibited in the four S’ástras, the Tantra Sangraham, Yucti Bháshá, Carana Padhati, and Sadratnamála. <em>Transactions of the Royal Asiatic Society of Great Britain and Ireland 3</em> (3): 509–23. <a href="https://doi.org/10.1017/S0950473700001221">https://doi.org/10.1017/S0950473700001221</a></p>
<p>Wirtinger, W. (1927). Zur formalen Theorie der Funktionen von mehr komplexen Veränderlichen. <em>Mathematische Annalen 97</em>: 357–74. <a href="https://doi.org/10.1007/BF01447872">https://doi.org/10.1007/BF01447872</a></p>
<p>Wooff, D., &amp; Peladeau, N. (1994). Simstat: Simulation and Statistics for Ibm Personal Computers or Compatibles, Version 2.0. <em>Journal of the Royal Statistical Society Series C 43</em> (2): 417–22. <a href="https://doi.org/10.2307/2986032">https://doi.org/10.2307/2986032</a></p>
<p>Yates, F. (1934). Contingency Tables Involving Small Numbers and the $\chi^2$ Test. <em>Supplement to the Journal of the Royal Statistical Society 1</em> (2): 217–35. <a href="http://www.jstor.org/stable/2983604">http://www.jstor.org/stable/2983604</a></p>
<p>Yule, G. U. (1912). On the Methods of Measuring Association Between Two Attributes. <em>Journal of the Royal Statistical Society 75</em> (6): 579–652. <a href="http://www.jstor.org/stable/2340126">http://www.jstor.org/stable/2340126</a></p>

<script type="text/javascript" async
	src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	tex2jax: {inlineMath: [['\\[','\\]'], ['\\(','\\)']]}
	});
</script>


</body>
</html>